{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "\n",
    "        self.linear1 = nn.Linear(in_features, hidden_features)\n",
    "        self.linear2 = nn.Linear(hidden_features, out_features)\n",
    "        self.attn_linear1 = nn.Linear(hidden_features, d_h)\n",
    "        self.attn_linear2 = nn.Linear(hidden_features, d_h)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_prev, neighbors):\n",
    "        n_nodes = x.shape[0]\n",
    "\n",
    "        v_prev = self.linear1(v_prev)\n",
    "        v_prev = v_prev.unsqueeze(0).repeat(n_nodes, 1)\n",
    "\n",
    "        neighbors = self.linear1(neighbors)\n",
    "\n",
    "        attn_input = torch.cat([v_prev, neighbors], dim=-1)\n",
    "        attn_input = self.activation(attn_input)\n",
    "\n",
    "        attn1 = self.attn_linear1(attn_input)\n",
    "        attn2 = self.attn_linear2(attn_input)\n",
    "\n",
    "        attn_output = torch.matmul(attn1, attn2.transpose(0, 1)) / (self.d_h ** 0.5)\n",
    "        attn_output = self.activation(attn_output)\n",
    "\n",
    "        masked_attn_output = attn_output.masked_fill(neighbors == 0, float('-inf'))\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = x.unsqueeze(0).repeat(n_nodes, 1, 1)\n",
    "\n",
    "        output = torch.matmul(attn_weights.unsqueeze(1), x)\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        return output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # self.decoder = Decoder(self.n_hidden)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim = 1)\n",
    "        \n",
    "        # attention_coefficients = self.decoder(attn_res)\n",
    "        # return attention_coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, n_heads)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(out_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, v_prev, neighbors):\n",
    "        return self.decoder(x, v_prev, neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Feature Matrix:\n",
      "tensor([[-1.0030, -1.8632, -0.7330,  2.5421, -1.6586,  0.1317, -1.6019,  1.3239],\n",
      "        [-1.6821, -0.7835,  0.1511,  1.6416, -0.3283,  0.6239, -0.4302,  0.3748],\n",
      "        [-0.4827,  0.9448,  0.7304,  1.0704, -1.1878,  0.3692, -1.8309, -1.8636],\n",
      "        [ 1.6016,  0.6327, -0.9213,  2.3722,  0.1404,  0.3952, -0.0668,  1.0579]])\n",
      "\n",
      "Graph 1 - Adjacency Tensor:\n",
      "tensor([[[1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.],\n",
      "         [1., 1., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.],\n",
      "         [1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.],\n",
      "         [0., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.],\n",
      "         [0., 0., 1., 1.]]])\n",
      "\n",
      "\n",
      "Graph 2 - Feature Matrix:\n",
      "tensor([[-0.7394,  1.7005, -0.1197, -1.1318,  1.6592,  0.2626, -0.4151,  0.1770],\n",
      "        [ 0.2701,  0.3951, -0.2521,  0.3053, -2.1761,  0.5216, -0.2598,  0.4937],\n",
      "        [-1.1652, -0.2473, -0.6096, -0.2393, -0.7161, -0.8558, -0.7031, -0.1131],\n",
      "        [ 0.1021, -0.5274,  0.8939,  0.5427,  0.1613,  1.1347, -0.1973, -0.3296],\n",
      "        [-0.7180,  1.2812, -0.6736, -1.2675,  1.4460, -1.2569,  1.2987, -2.1056]])\n",
      "\n",
      "Graph 2 - Adjacency Tensor:\n",
      "tensor([[[1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.],\n",
      "         [1., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.],\n",
      "         [1., 1., 1., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.],\n",
      "         [0., 1., 1., 1., 0.]],\n",
      "\n",
      "        [[0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.],\n",
      "         [0., 0., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.],\n",
      "         [0., 0., 0., 1., 1.]]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create multiple dummy graphs with different node sizes\n",
    "graph_list = []\n",
    "\n",
    "# Graph 1\n",
    "G1 = nx.Graph()\n",
    "G1.add_nodes_from(range(4))  # Add nodes\n",
    "G1.add_edges_from([(0, 1), (1, 2), (2, 3)])  # Add edges\n",
    "\n",
    "adj_matrix1 = nx.adjacency_matrix(G1)\n",
    "adj_matrix1 = adj_matrix1 + sp.eye(adj_matrix1.shape[0])  # Add self-loop\n",
    "adj_tensor1 = torch.Tensor(adj_matrix1.todense())\n",
    "\n",
    "\n",
    "num_nodes1 = G1.number_of_nodes()\n",
    "in_features1 = 8\n",
    "x1 = torch.randn(num_nodes1, in_features1)\n",
    "\n",
    "# Resize adjacency tensor to match the input features size\n",
    "adj_tensor1 = adj_tensor1.unsqueeze(0)  # Add an extra dimension\n",
    "adj_tensor1 = adj_tensor1.repeat(num_nodes1, 1, 1)  # Repeat the adjacency tensor\n",
    "adj_tensor1 = adj_tensor1.transpose(0, 1)  # Transpose the dimensions\n",
    "\n",
    "# Generate labels for Graph 1\n",
    "labels1 = torch.randint(0, 2, (num_nodes1,)).to(device)\n",
    "\n",
    "graph_list.append((x1, adj_tensor1))\n",
    "\n",
    "# Graph 2\n",
    "G2 = nx.Graph()\n",
    "G2.add_nodes_from(range(5))  # Add nodes\n",
    "G2.add_edges_from([(0, 1), (1, 2), (2, 3), (3, 4)])  # Add edges\n",
    "\n",
    "adj_matrix2 = nx.adjacency_matrix(G2)\n",
    "adj_matrix2 = adj_matrix2 + sp.eye(adj_matrix2.shape[0])  # Add self-loop\n",
    "adj_tensor2 = torch.Tensor(adj_matrix2.todense())\n",
    "\n",
    "num_nodes2 = G2.number_of_nodes()\n",
    "in_features2 = 8\n",
    "x2 = torch.randn(num_nodes2, in_features2)\n",
    "\n",
    "# Resize adjacency tensor to match the input features size\n",
    "adj_tensor2 = adj_tensor2.unsqueeze(0)  # Add an extra dimension\n",
    "adj_tensor2 = adj_tensor2.repeat(num_nodes2, 1, 1)  # Repeat the adjacency tensor\n",
    "adj_tensor2 = adj_tensor2.transpose(0, 1)  # Transpose the dimensions\n",
    "\n",
    "# Generate labels for Graph 2\n",
    "labels2 = torch.randint(0, 2, (num_nodes2,)).to(device)\n",
    "\n",
    "graph_list.append((x2, adj_tensor2))\n",
    "\n",
    "# Access the graphs and their components from the graph list\n",
    "for i, (feature_matrix, adj_tensor) in enumerate(graph_list):\n",
    "    # # Expand the adj_tensor dimensions if using multiple attention heads\n",
    "    # if adj_tensor.dim() == 2:\n",
    "    #     adj_tensor = adj_tensor.unsqueeze(2).expand(-1, -1, self.n_heads)\n",
    "\n",
    "    graph_list[i] = (feature_matrix.cuda(), adj_tensor.cuda())\n",
    "\n",
    "\n",
    "    print(f\"Graph {i+1} - Feature Matrix:\")\n",
    "    print(feature_matrix)\n",
    "\n",
    "    print(f\"\\nGraph {i+1} - Adjacency Tensor:\")\n",
    "    print(adj_tensor)\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "tensor([[0.0198, 0.1400, 0.3839, 0.1161, 0.0379, 0.2337, 0.0343, 0.0343],\n",
      "        [0.0156, 0.0506, 0.2588, 0.0805, 0.0436, 0.0411, 0.4440, 0.0659],\n",
      "        [0.0170, 0.1283, 0.3172, 0.1139, 0.0301, 0.0301, 0.2601, 0.1031],\n",
      "        [0.0217, 0.1102, 0.1182, 0.0431, 0.0269, 0.5848, 0.0350, 0.0601]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 2 - Output:\n",
      "tensor([[0.0386, 0.3556, 0.1901, 0.0851, 0.0224, 0.0251, 0.1601, 0.0178, 0.0231,\n",
      "         0.0821],\n",
      "        [0.0435, 0.3346, 0.0120, 0.0337, 0.0818, 0.0468, 0.1661, 0.0247, 0.0344,\n",
      "         0.2224],\n",
      "        [0.0327, 0.1145, 0.3320, 0.1621, 0.0595, 0.0319, 0.0431, 0.0221, 0.1897,\n",
      "         0.0125],\n",
      "        [0.1681, 0.2533, 0.0167, 0.0466, 0.0199, 0.0144, 0.1773, 0.1206, 0.0915,\n",
      "         0.0915],\n",
      "        [0.0070, 0.5392, 0.0506, 0.0506, 0.0506, 0.0506, 0.0581, 0.1019, 0.0411,\n",
      "         0.0503]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create and initialize the GAT models for each graph\n",
    "gat_models = []\n",
    "for i, (feature_matrix, adj_tensor) in enumerate(graph_list):\n",
    "    in_features = feature_matrix.shape[1]\n",
    "    n_heads = adj_tensor.shape[2]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = 2 * n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "    gat_models.append(gat_model)\n",
    "    feature_matrix = feature_matrix.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(feature_matrix, adj_tensor)\n",
    "    print(f\"Graph {i+1} - Output:\")\n",
    "    print(output)\n",
    "    #output : 각 노드에 대한 클래스 라벨 예측 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Epoch: 001, Loss: -0.0426\n",
      "Graph 2: Epoch: 001, Loss: -0.0426\n",
      "Graph 1: Epoch: 002, Loss: -0.1730\n",
      "Graph 2: Epoch: 002, Loss: -0.1730\n",
      "Graph 1: Epoch: 003, Loss: -0.1797\n",
      "Graph 2: Epoch: 003, Loss: -0.1797\n",
      "Graph 1: Epoch: 004, Loss: -0.1878\n",
      "Graph 2: Epoch: 004, Loss: -0.1878\n",
      "Graph 1: Epoch: 005, Loss: -0.1145\n",
      "Graph 2: Epoch: 005, Loss: -0.1145\n",
      "Graph 1: Epoch: 006, Loss: -0.1843\n",
      "Graph 2: Epoch: 006, Loss: -0.1843\n",
      "Graph 1: Epoch: 007, Loss: -0.2047\n",
      "Graph 2: Epoch: 007, Loss: -0.2047\n",
      "Graph 1: Epoch: 008, Loss: -0.1988\n",
      "Graph 2: Epoch: 008, Loss: -0.1988\n",
      "Graph 1: Epoch: 009, Loss: -0.2134\n",
      "Graph 2: Epoch: 009, Loss: -0.2134\n",
      "Graph 1: Epoch: 010, Loss: -0.1352\n",
      "Graph 2: Epoch: 010, Loss: -0.1352\n",
      "Graph 1: Epoch: 011, Loss: -0.3044\n",
      "Graph 2: Epoch: 011, Loss: -0.3044\n",
      "Graph 1: Epoch: 012, Loss: -0.2079\n",
      "Graph 2: Epoch: 012, Loss: -0.2079\n",
      "Graph 1: Epoch: 013, Loss: -0.2585\n",
      "Graph 2: Epoch: 013, Loss: -0.2585\n",
      "Graph 1: Epoch: 014, Loss: -0.2304\n",
      "Graph 2: Epoch: 014, Loss: -0.2304\n",
      "Graph 1: Epoch: 015, Loss: -0.2538\n",
      "Graph 2: Epoch: 015, Loss: -0.2538\n",
      "Graph 1: Epoch: 016, Loss: -0.2752\n",
      "Graph 2: Epoch: 016, Loss: -0.2752\n",
      "Graph 1: Epoch: 017, Loss: -0.3264\n",
      "Graph 2: Epoch: 017, Loss: -0.3264\n",
      "Graph 1: Epoch: 018, Loss: -0.0815\n",
      "Graph 2: Epoch: 018, Loss: -0.0815\n",
      "Graph 1: Epoch: 019, Loss: -0.1764\n",
      "Graph 2: Epoch: 019, Loss: -0.1764\n",
      "Graph 1: Epoch: 020, Loss: -0.3304\n",
      "Graph 2: Epoch: 020, Loss: -0.3304\n",
      "Graph 1: Epoch: 021, Loss: -0.2715\n",
      "Graph 2: Epoch: 021, Loss: -0.2715\n",
      "Graph 1: Epoch: 022, Loss: -0.2732\n",
      "Graph 2: Epoch: 022, Loss: -0.2732\n",
      "Graph 1: Epoch: 023, Loss: -0.2234\n",
      "Graph 2: Epoch: 023, Loss: -0.2234\n",
      "Graph 1: Epoch: 024, Loss: -0.1802\n",
      "Graph 2: Epoch: 024, Loss: -0.1802\n",
      "Graph 1: Epoch: 025, Loss: -0.1913\n",
      "Graph 2: Epoch: 025, Loss: -0.1913\n",
      "Graph 1: Epoch: 026, Loss: -0.2298\n",
      "Graph 2: Epoch: 026, Loss: -0.2298\n",
      "Graph 1: Epoch: 027, Loss: -0.2252\n",
      "Graph 2: Epoch: 027, Loss: -0.2252\n",
      "Graph 1: Epoch: 028, Loss: -0.2901\n",
      "Graph 2: Epoch: 028, Loss: -0.2901\n",
      "Graph 1: Epoch: 029, Loss: -0.3236\n",
      "Graph 2: Epoch: 029, Loss: -0.3236\n",
      "Graph 1: Epoch: 030, Loss: -0.1561\n",
      "Graph 2: Epoch: 030, Loss: -0.1561\n",
      "Graph 1: Epoch: 031, Loss: -0.1912\n",
      "Graph 2: Epoch: 031, Loss: -0.1912\n",
      "Graph 1: Epoch: 032, Loss: -0.1975\n",
      "Graph 2: Epoch: 032, Loss: -0.1975\n",
      "Graph 1: Epoch: 033, Loss: -0.3560\n",
      "Graph 2: Epoch: 033, Loss: -0.3560\n",
      "Graph 1: Epoch: 034, Loss: -0.2281\n",
      "Graph 2: Epoch: 034, Loss: -0.2281\n",
      "Graph 1: Epoch: 035, Loss: -0.2576\n",
      "Graph 2: Epoch: 035, Loss: -0.2576\n",
      "Graph 1: Epoch: 036, Loss: -0.3811\n",
      "Graph 2: Epoch: 036, Loss: -0.3811\n",
      "Graph 1: Epoch: 037, Loss: -0.2579\n",
      "Graph 2: Epoch: 037, Loss: -0.2579\n",
      "Graph 1: Epoch: 038, Loss: -0.2639\n",
      "Graph 2: Epoch: 038, Loss: -0.2639\n",
      "Graph 1: Epoch: 039, Loss: -0.3420\n",
      "Graph 2: Epoch: 039, Loss: -0.3420\n",
      "Graph 1: Epoch: 040, Loss: -0.2136\n",
      "Graph 2: Epoch: 040, Loss: -0.2136\n",
      "Graph 1: Epoch: 041, Loss: -0.2224\n",
      "Graph 2: Epoch: 041, Loss: -0.2224\n",
      "Graph 1: Epoch: 042, Loss: -0.2455\n",
      "Graph 2: Epoch: 042, Loss: -0.2455\n",
      "Graph 1: Epoch: 043, Loss: -0.3142\n",
      "Graph 2: Epoch: 043, Loss: -0.3142\n",
      "Graph 1: Epoch: 044, Loss: -0.2708\n",
      "Graph 2: Epoch: 044, Loss: -0.2708\n",
      "Graph 1: Epoch: 045, Loss: -0.2803\n",
      "Graph 2: Epoch: 045, Loss: -0.2803\n",
      "Graph 1: Epoch: 046, Loss: -0.3581\n",
      "Graph 2: Epoch: 046, Loss: -0.3581\n",
      "Graph 1: Epoch: 047, Loss: -0.1733\n",
      "Graph 2: Epoch: 047, Loss: -0.1733\n",
      "Graph 1: Epoch: 048, Loss: -0.3540\n",
      "Graph 2: Epoch: 048, Loss: -0.3540\n",
      "Graph 1: Epoch: 049, Loss: -0.2428\n",
      "Graph 2: Epoch: 049, Loss: -0.2428\n",
      "Graph 1: Epoch: 050, Loss: -0.3092\n",
      "Graph 2: Epoch: 050, Loss: -0.3092\n",
      "Graph 1: Epoch: 051, Loss: -0.3136\n",
      "Graph 2: Epoch: 051, Loss: -0.3136\n",
      "Graph 1: Epoch: 052, Loss: -0.1942\n",
      "Graph 2: Epoch: 052, Loss: -0.1942\n",
      "Graph 1: Epoch: 053, Loss: -0.3115\n",
      "Graph 2: Epoch: 053, Loss: -0.3115\n",
      "Graph 1: Epoch: 054, Loss: -0.2018\n",
      "Graph 2: Epoch: 054, Loss: -0.2018\n",
      "Graph 1: Epoch: 055, Loss: -0.2553\n",
      "Graph 2: Epoch: 055, Loss: -0.2553\n",
      "Graph 1: Epoch: 056, Loss: -0.2458\n",
      "Graph 2: Epoch: 056, Loss: -0.2458\n",
      "Graph 1: Epoch: 057, Loss: -0.2045\n",
      "Graph 2: Epoch: 057, Loss: -0.2045\n",
      "Graph 1: Epoch: 058, Loss: -0.4172\n",
      "Graph 2: Epoch: 058, Loss: -0.4172\n",
      "Graph 1: Epoch: 059, Loss: -0.1814\n",
      "Graph 2: Epoch: 059, Loss: -0.1814\n",
      "Graph 1: Epoch: 060, Loss: -0.2193\n",
      "Graph 2: Epoch: 060, Loss: -0.2193\n",
      "Graph 1: Epoch: 061, Loss: -0.4285\n",
      "Graph 2: Epoch: 061, Loss: -0.4285\n",
      "Graph 1: Epoch: 062, Loss: -0.3690\n",
      "Graph 2: Epoch: 062, Loss: -0.3690\n",
      "Graph 1: Epoch: 063, Loss: -0.3276\n",
      "Graph 2: Epoch: 063, Loss: -0.3276\n",
      "Graph 1: Epoch: 064, Loss: -0.2326\n",
      "Graph 2: Epoch: 064, Loss: -0.2326\n",
      "Graph 1: Epoch: 065, Loss: -0.4249\n",
      "Graph 2: Epoch: 065, Loss: -0.4249\n",
      "Graph 1: Epoch: 066, Loss: -0.1455\n",
      "Graph 2: Epoch: 066, Loss: -0.1455\n",
      "Graph 1: Epoch: 067, Loss: -0.4011\n",
      "Graph 2: Epoch: 067, Loss: -0.4011\n",
      "Graph 1: Epoch: 068, Loss: -0.3343\n",
      "Graph 2: Epoch: 068, Loss: -0.3343\n",
      "Graph 1: Epoch: 069, Loss: -0.1560\n",
      "Graph 2: Epoch: 069, Loss: -0.1560\n",
      "Graph 1: Epoch: 070, Loss: -0.3240\n",
      "Graph 2: Epoch: 070, Loss: -0.3240\n",
      "Graph 1: Epoch: 071, Loss: -0.3440\n",
      "Graph 2: Epoch: 071, Loss: -0.3440\n",
      "Graph 1: Epoch: 072, Loss: -0.3907\n",
      "Graph 2: Epoch: 072, Loss: -0.3907\n",
      "Graph 1: Epoch: 073, Loss: -0.3062\n",
      "Graph 2: Epoch: 073, Loss: -0.3062\n",
      "Graph 1: Epoch: 074, Loss: -0.2623\n",
      "Graph 2: Epoch: 074, Loss: -0.2623\n",
      "Graph 1: Epoch: 075, Loss: -0.1476\n",
      "Graph 2: Epoch: 075, Loss: -0.1476\n",
      "Graph 1: Epoch: 076, Loss: -0.3558\n",
      "Graph 2: Epoch: 076, Loss: -0.3558\n",
      "Graph 1: Epoch: 077, Loss: -0.3304\n",
      "Graph 2: Epoch: 077, Loss: -0.3304\n",
      "Graph 1: Epoch: 078, Loss: -0.2071\n",
      "Graph 2: Epoch: 078, Loss: -0.2071\n",
      "Graph 1: Epoch: 079, Loss: -0.2431\n",
      "Graph 2: Epoch: 079, Loss: -0.2431\n",
      "Graph 1: Epoch: 080, Loss: -0.2802\n",
      "Graph 2: Epoch: 080, Loss: -0.2802\n",
      "Graph 1: Epoch: 081, Loss: -0.5077\n",
      "Graph 2: Epoch: 081, Loss: -0.5077\n",
      "Graph 1: Epoch: 082, Loss: -0.1943\n",
      "Graph 2: Epoch: 082, Loss: -0.1943\n",
      "Graph 1: Epoch: 083, Loss: -0.1902\n",
      "Graph 2: Epoch: 083, Loss: -0.1902\n",
      "Graph 1: Epoch: 084, Loss: -0.1997\n",
      "Graph 2: Epoch: 084, Loss: -0.1997\n",
      "Graph 1: Epoch: 085, Loss: -0.2785\n",
      "Graph 2: Epoch: 085, Loss: -0.2785\n",
      "Graph 1: Epoch: 086, Loss: -0.3014\n",
      "Graph 2: Epoch: 086, Loss: -0.3014\n",
      "Graph 1: Epoch: 087, Loss: -0.2971\n",
      "Graph 2: Epoch: 087, Loss: -0.2971\n",
      "Graph 1: Epoch: 088, Loss: -0.2457\n",
      "Graph 2: Epoch: 088, Loss: -0.2457\n",
      "Graph 1: Epoch: 089, Loss: -0.3687\n",
      "Graph 2: Epoch: 089, Loss: -0.3687\n",
      "Graph 1: Epoch: 090, Loss: -0.1663\n",
      "Graph 2: Epoch: 090, Loss: -0.1663\n",
      "Graph 1: Epoch: 091, Loss: -0.1235\n",
      "Graph 2: Epoch: 091, Loss: -0.1235\n",
      "Graph 1: Epoch: 092, Loss: -0.2339\n",
      "Graph 2: Epoch: 092, Loss: -0.2339\n",
      "Graph 1: Epoch: 093, Loss: -0.3947\n",
      "Graph 2: Epoch: 093, Loss: -0.3947\n",
      "Graph 1: Epoch: 094, Loss: -0.3426\n",
      "Graph 2: Epoch: 094, Loss: -0.3426\n",
      "Graph 1: Epoch: 095, Loss: -0.4299\n",
      "Graph 2: Epoch: 095, Loss: -0.4299\n",
      "Graph 1: Epoch: 096, Loss: -0.4184\n",
      "Graph 2: Epoch: 096, Loss: -0.4184\n",
      "Graph 1: Epoch: 097, Loss: -0.3836\n",
      "Graph 2: Epoch: 097, Loss: -0.3836\n",
      "Graph 1: Epoch: 098, Loss: -0.5010\n",
      "Graph 2: Epoch: 098, Loss: -0.5010\n",
      "Graph 1: Epoch: 099, Loss: -0.4125\n",
      "Graph 2: Epoch: 099, Loss: -0.4125\n",
      "Graph 1: Epoch: 100, Loss: -0.2253\n",
      "Graph 2: Epoch: 100, Loss: -0.2253\n"
     ]
    }
   ],
   "source": [
    "# Set the optimizer and loss function\n",
    "optimizer = optim.Adam(gat_model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "# Move the model and loss function to the GPU\n",
    "gat_model = gat_model.cuda()\n",
    "criterion = criterion.cuda()\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    for graph_idx, (feature_matrix, adj_tensor) in enumerate(graph_list):\n",
    "        feature_matrix = feature_matrix.to(device)\n",
    "        adj_tensor = adj_tensor.to(device)\n",
    "         # Generate random labels for the current graph\n",
    "        num_nodes = feature_matrix.shape[0]\n",
    "        labels = torch.tensor([random.randint(0, 1) for _ in range(num_nodes)]).to(device)\n",
    "                \n",
    "        # Zero the gradients\n",
    "        gat_model.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = gat_models[graph_idx](feature_matrix, adj_tensor)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(output.squeeze(0), labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Calculate the average loss for the epoch\n",
    "    average_loss = total_loss / len(graph_list)\n",
    "    \n",
    "    for graph_idx in range(len(graph_list)):\n",
    "        print(\"Graph {}: Epoch: {:03d}, Loss: {:.4f}\".format(graph_idx+1, epoch+1, average_loss))\n",
    "     \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
