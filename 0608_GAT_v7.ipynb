{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "from graphviz import Graph\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.7614])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_1 = torch.randn(3,1)\n",
    "a_2 = torch.randn(3,1)\n",
    "c= a_1*a_2.T\n",
    "d= torch.matmul(a_1.T,a_2)\n",
    "\n",
    "d=d.squeeze(0)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "\n",
    "        self.phi = torch.nn.Linear(hidden_features, d_h * hidden_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.C = torch.nn.Parameter(torch.randn(1)) # constant C\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_i, v_j):\n",
    "        phi1 = torch.randn(d_h,1).cuda()\n",
    "        phi2 = torch.randn(d_h,1).cuda()\n",
    "\n",
    "        phi1_v_i = phi1 * v_i # phi1_v_prev 의 사이즈 (d* d_h)\n",
    "        phi2_v_j = phi2 * v_j #.squeeze(0) # phi2_neighbors 의 사이즈 (n,d* d_h)\n",
    "\n",
    "        attn_input = torch.matmul(phi1_v_i.transpose(0,1), phi2_v_j) / (self.d_h ** 0.5) # (1,n) 의 크기를 갖는 attn_input\n",
    "        # attn_input = attn_input.squeeze(0)  # Remove the extra dimension\n",
    "\n",
    "        attn_output = self.C * self.activation(attn_input)\n",
    "\n",
    "        masked_attn_output = attn_output.masked_fill(v_j == 0, float('-inf'))\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "        # attn_weights = self.softmax(attn_output)\n",
    "\n",
    "        output = attn_weights.squeeze() * x\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        return x, output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, n_heads)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(out_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, v_i, v_j):\n",
    "        return self.decoder(x, v_i, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weighted_graph(num_nodes, num_edges, max_weight=10):\n",
    "    # 방향 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 노드 추가\n",
    "    nodes = range(num_nodes)\n",
    "    graph.add_nodes_from(nodes)\n",
    "    \n",
    "    # 간선 추가\n",
    "    edges = []\n",
    "    for i in range(num_edges):\n",
    "        # 임의의 출발 노드와 도착 노드 선택\n",
    "        source = random.choice(nodes)\n",
    "        target = random.choice(nodes)\n",
    "        \n",
    "        # 출발 노드와 도착 노드가 같은 경우 건너뜀\n",
    "        if source == target:\n",
    "            continue\n",
    "        \n",
    "        # 가중치 랜덤 생성\n",
    "        weight = random.randint(1, max_weight)\n",
    "        \n",
    "        # 간선 추가\n",
    "        edges.append((source, target, weight))\n",
    "\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    adj_matrix = adj_matrix + sp.eye(adj_matrix.shape[0]) # Add self-loop\n",
    "    adj_tensor = torch.Tensor(adj_matrix.todense())\n",
    "\n",
    "    in_features =  1\n",
    "    x = torch.randn(num_nodes, in_features)\n",
    "\n",
    "    adj_tensor = adj_tensor.unsqueeze(0)\n",
    "    adj_tensor = adj_tensor.repeat(num_nodes, 1, 1)\n",
    "    adj_tensor = adj_tensor.transpose(0,1)\n",
    "        \n",
    "    graph.add_weighted_edges_from(edges)\n",
    "    \n",
    "    return graph, x, adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 100\n",
    "output_file = 'random_undirected_graphs.pkl'\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for _ in range(num_graphs):\n",
    "    num_nodes, num_edges, max_weight = np.random.randint(1,20), np.random.randint(1,30), np.random.randint(1,30)\n",
    "    graph, x, adj_tensor = generate_random_weighted_graph(num_nodes, num_edges, max_weight)\n",
    "    graphs.append((x, adj_tensor))\n",
    "\n",
    "\n",
    "# 그래프를 pickle 파일로 저장\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일에서 그래프 데이터 로드\n",
    "with open('random_undirected_graphs.pkl', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0128, 0.3112, 0.0174, 0.0471, 0.0988, 0.0335, 0.0245, 0.0602, 0.0088,\n",
      "         0.0746, 0.1281, 0.0120, 0.0783, 0.0139, 0.0789],\n",
      "        [0.0183, 0.1441, 0.0158, 0.0406, 0.0817, 0.0297, 0.0119, 0.0664, 0.0073,\n",
      "         0.1091, 0.1418, 0.0315, 0.0459, 0.0167, 0.2395],\n",
      "        [0.0136, 0.0895, 0.0163, 0.0500, 0.0971, 0.0423, 0.0241, 0.1055, 0.0051,\n",
      "         0.1098, 0.2296, 0.0244, 0.0438, 0.0196, 0.1290],\n",
      "        [0.0346, 0.1748, 0.0234, 0.0527, 0.0966, 0.0391, 0.0090, 0.0525, 0.0140,\n",
      "         0.1089, 0.1527, 0.0196, 0.0671, 0.0070, 0.1482],\n",
      "        [0.0268, 0.0996, 0.0125, 0.0511, 0.0631, 0.0362, 0.0563, 0.1255, 0.0061,\n",
      "         0.1168, 0.2097, 0.0296, 0.0800, 0.0077, 0.0789],\n",
      "        [0.0295, 0.0952, 0.0150, 0.0503, 0.0612, 0.0323, 0.0144, 0.0895, 0.0101,\n",
      "         0.0426, 0.2300, 0.0297, 0.1556, 0.0074, 0.1372],\n",
      "        [0.0228, 0.0543, 0.0299, 0.0421, 0.0615, 0.0373, 0.0099, 0.1168, 0.0057,\n",
      "         0.0809, 0.2637, 0.0209, 0.1253, 0.0183, 0.1108],\n",
      "        [0.0363, 0.1232, 0.0262, 0.0316, 0.0641, 0.0312, 0.0877, 0.1870, 0.0046,\n",
      "         0.0735, 0.1190, 0.0173, 0.1259, 0.0100, 0.0623],\n",
      "        [0.0093, 0.0451, 0.0278, 0.0831, 0.0731, 0.0348, 0.0160, 0.1000, 0.0115,\n",
      "         0.1168, 0.2412, 0.0267, 0.0503, 0.0091, 0.1551],\n",
      "        [0.0290, 0.0642, 0.0233, 0.0490, 0.0687, 0.0471, 0.0078, 0.3106, 0.0229,\n",
      "         0.1576, 0.0613, 0.0152, 0.0829, 0.0066, 0.0536],\n",
      "        [0.0316, 0.2618, 0.0158, 0.0485, 0.0506, 0.0455, 0.0123, 0.0851, 0.0173,\n",
      "         0.0419, 0.2414, 0.0125, 0.0421, 0.0081, 0.0858],\n",
      "        [0.0122, 0.0382, 0.0189, 0.0249, 0.0269, 0.0261, 0.0268, 0.1651, 0.0074,\n",
      "         0.0432, 0.4912, 0.0144, 0.0322, 0.0186, 0.0540],\n",
      "        [0.0140, 0.1078, 0.0111, 0.0812, 0.0524, 0.0316, 0.0505, 0.0729, 0.0107,\n",
      "         0.1940, 0.2188, 0.0079, 0.0600, 0.0262, 0.0609],\n",
      "        [0.0100, 0.0868, 0.0236, 0.0752, 0.0522, 0.0222, 0.0083, 0.1289, 0.0165,\n",
      "         0.0365, 0.3744, 0.0235, 0.0751, 0.0149, 0.0519],\n",
      "        [0.0413, 0.0538, 0.0124, 0.0604, 0.0452, 0.0379, 0.0249, 0.1206, 0.0087,\n",
      "         0.1293, 0.2836, 0.0104, 0.0391, 0.0126, 0.1199]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 2 - Output:\n",
      "torch.Size([11, 11])\n",
      "1\n",
      "tensor([[0.1568, 0.0494, 0.1148, 0.0860, 0.0048, 0.0593, 0.0315, 0.1555, 0.2025,\n",
      "         0.0426, 0.0968],\n",
      "        [0.1033, 0.0398, 0.0134, 0.0415, 0.0110, 0.0594, 0.0290, 0.0466, 0.5507,\n",
      "         0.0334, 0.0720],\n",
      "        [0.1544, 0.0262, 0.1191, 0.1212, 0.0057, 0.0560, 0.0282, 0.0936, 0.2568,\n",
      "         0.0677, 0.0712],\n",
      "        [0.1342, 0.0287, 0.0191, 0.2006, 0.0116, 0.0282, 0.0291, 0.1168, 0.3187,\n",
      "         0.0648, 0.0481],\n",
      "        [0.4530, 0.0197, 0.1585, 0.0266, 0.0149, 0.0797, 0.0216, 0.0607, 0.0545,\n",
      "         0.0211, 0.0897],\n",
      "        [0.2654, 0.1119, 0.0336, 0.0620, 0.0152, 0.0355, 0.0167, 0.1320, 0.2567,\n",
      "         0.0176, 0.0532],\n",
      "        [0.1991, 0.0342, 0.0543, 0.1329, 0.0043, 0.0582, 0.0671, 0.1058, 0.1606,\n",
      "         0.0698, 0.1138],\n",
      "        [0.4544, 0.0239, 0.0069, 0.0324, 0.0309, 0.0460, 0.0751, 0.0663, 0.0764,\n",
      "         0.0676, 0.1200],\n",
      "        [0.1912, 0.0613, 0.0593, 0.0569, 0.0053, 0.0482, 0.0316, 0.2942, 0.1116,\n",
      "         0.0433, 0.0970],\n",
      "        [0.2253, 0.0156, 0.0599, 0.1381, 0.0149, 0.0352, 0.0337, 0.2413, 0.1714,\n",
      "         0.0216, 0.0430],\n",
      "        [0.4006, 0.0585, 0.0514, 0.1133, 0.0072, 0.0223, 0.0220, 0.0773, 0.0667,\n",
      "         0.0638, 0.1168]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 3 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0338, 0.0582, 0.1051, 0.0611, 0.0126, 0.0225, 0.0559, 0.0489, 0.0360,\n",
      "         0.0181, 0.0045, 0.0795, 0.3891, 0.0170, 0.0575],\n",
      "        [0.0546, 0.0612, 0.0919, 0.0417, 0.0055, 0.0350, 0.0600, 0.0450, 0.0878,\n",
      "         0.0134, 0.0102, 0.0272, 0.3369, 0.0194, 0.1100],\n",
      "        [0.0425, 0.0177, 0.0228, 0.0458, 0.0340, 0.0244, 0.0579, 0.0243, 0.0449,\n",
      "         0.0130, 0.0066, 0.0202, 0.6043, 0.0091, 0.0325],\n",
      "        [0.0806, 0.0959, 0.0549, 0.0682, 0.0199, 0.0331, 0.0494, 0.0268, 0.0456,\n",
      "         0.0100, 0.0050, 0.0197, 0.3627, 0.0270, 0.1012],\n",
      "        [0.0720, 0.0132, 0.0420, 0.0403, 0.0200, 0.0379, 0.0665, 0.0314, 0.0280,\n",
      "         0.0101, 0.0163, 0.0120, 0.5580, 0.0091, 0.0433],\n",
      "        [0.0281, 0.0448, 0.0285, 0.0374, 0.0182, 0.0490, 0.0612, 0.0478, 0.0259,\n",
      "         0.0054, 0.0073, 0.0351, 0.4958, 0.0334, 0.0821],\n",
      "        [0.0856, 0.0548, 0.1692, 0.0386, 0.0076, 0.0168, 0.0546, 0.0551, 0.0497,\n",
      "         0.0213, 0.0088, 0.0612, 0.3268, 0.0138, 0.0361],\n",
      "        [0.1024, 0.0279, 0.0542, 0.0441, 0.0146, 0.0189, 0.0713, 0.0241, 0.0901,\n",
      "         0.0175, 0.0060, 0.0139, 0.3998, 0.0432, 0.0720],\n",
      "        [0.0828, 0.0454, 0.0529, 0.0751, 0.0496, 0.0523, 0.1065, 0.0760, 0.1054,\n",
      "         0.0078, 0.0085, 0.0181, 0.1395, 0.0084, 0.1715],\n",
      "        [0.0372, 0.0247, 0.0914, 0.0701, 0.0435, 0.1442, 0.0573, 0.0461, 0.0811,\n",
      "         0.0090, 0.0035, 0.0350, 0.1448, 0.0529, 0.1591],\n",
      "        [0.1138, 0.0448, 0.0261, 0.0615, 0.0069, 0.0283, 0.0872, 0.0377, 0.0406,\n",
      "         0.0185, 0.0123, 0.0120, 0.4172, 0.0197, 0.0734],\n",
      "        [0.0657, 0.0222, 0.0429, 0.0679, 0.0425, 0.0402, 0.2586, 0.0609, 0.0300,\n",
      "         0.0221, 0.0031, 0.0197, 0.1034, 0.0522, 0.1684],\n",
      "        [0.0681, 0.0432, 0.0164, 0.1144, 0.0149, 0.0798, 0.0570, 0.0597, 0.0697,\n",
      "         0.0133, 0.0064, 0.0406, 0.2576, 0.0140, 0.1447],\n",
      "        [0.0846, 0.0228, 0.0785, 0.0307, 0.0137, 0.0493, 0.0543, 0.0401, 0.0747,\n",
      "         0.0121, 0.0052, 0.0580, 0.3491, 0.0195, 0.1074],\n",
      "        [0.0294, 0.0103, 0.0252, 0.0707, 0.0099, 0.0193, 0.0935, 0.0365, 0.0391,\n",
      "         0.0251, 0.0082, 0.0482, 0.4797, 0.0265, 0.0785]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 4 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.1611, 0.0284, 0.1055, 0.0596, 0.0187, 0.0214, 0.0158, 0.0364, 0.1318,\n",
      "         0.0998, 0.0108, 0.0031, 0.0258, 0.0692, 0.1285, 0.0368, 0.0284, 0.0188],\n",
      "        [0.2973, 0.0717, 0.0730, 0.0452, 0.0316, 0.0315, 0.0075, 0.0447, 0.0302,\n",
      "         0.0375, 0.0467, 0.0212, 0.0037, 0.1381, 0.0082, 0.0467, 0.0438, 0.0213],\n",
      "        [0.0960, 0.0402, 0.0304, 0.1141, 0.0247, 0.2246, 0.0191, 0.0319, 0.1342,\n",
      "         0.0321, 0.0114, 0.0263, 0.1343, 0.0307, 0.0089, 0.0224, 0.0053, 0.0134],\n",
      "        [0.0927, 0.2031, 0.1232, 0.0867, 0.0197, 0.1574, 0.0385, 0.0407, 0.0621,\n",
      "         0.0177, 0.0159, 0.0057, 0.0143, 0.0255, 0.0099, 0.0333, 0.0100, 0.0435],\n",
      "        [0.0698, 0.0659, 0.2300, 0.2311, 0.0202, 0.0373, 0.0509, 0.0284, 0.0555,\n",
      "         0.0234, 0.0114, 0.0118, 0.0169, 0.0595, 0.0035, 0.0328, 0.0174, 0.0343],\n",
      "        [0.0383, 0.0692, 0.0132, 0.3057, 0.0401, 0.0712, 0.0060, 0.0356, 0.0545,\n",
      "         0.1557, 0.0164, 0.0127, 0.0094, 0.0477, 0.0082, 0.0526, 0.0312, 0.0324],\n",
      "        [0.0553, 0.0219, 0.0587, 0.0580, 0.0156, 0.0602, 0.0655, 0.0360, 0.0464,\n",
      "         0.4708, 0.0206, 0.0073, 0.0214, 0.0194, 0.0084, 0.0076, 0.0135, 0.0133],\n",
      "        [0.1032, 0.1730, 0.0699, 0.0608, 0.0344, 0.0057, 0.0300, 0.0332, 0.0605,\n",
      "         0.1175, 0.0152, 0.0182, 0.0060, 0.0263, 0.1360, 0.0141, 0.0160, 0.0799],\n",
      "        [0.2332, 0.0703, 0.1853, 0.0611, 0.0922, 0.0223, 0.0312, 0.0383, 0.0942,\n",
      "         0.0291, 0.0244, 0.0081, 0.0132, 0.0130, 0.0047, 0.0210, 0.0296, 0.0290],\n",
      "        [0.1511, 0.0949, 0.0512, 0.1182, 0.0460, 0.0982, 0.0103, 0.0428, 0.0852,\n",
      "         0.0799, 0.0136, 0.0268, 0.0082, 0.0791, 0.0037, 0.0368, 0.0185, 0.0354],\n",
      "        [0.1627, 0.0668, 0.2415, 0.1257, 0.0162, 0.0137, 0.0066, 0.0249, 0.0381,\n",
      "         0.0334, 0.0127, 0.0324, 0.0624, 0.0441, 0.0079, 0.0132, 0.0216, 0.0760],\n",
      "        [0.0747, 0.0677, 0.0591, 0.0503, 0.0142, 0.2552, 0.0355, 0.0323, 0.0738,\n",
      "         0.0739, 0.0471, 0.0021, 0.0429, 0.0410, 0.0091, 0.0688, 0.0210, 0.0312],\n",
      "        [0.0952, 0.0532, 0.1083, 0.0589, 0.0148, 0.1105, 0.0205, 0.0211, 0.0852,\n",
      "         0.1318, 0.0141, 0.0109, 0.0088, 0.1787, 0.0087, 0.0132, 0.0198, 0.0464],\n",
      "        [0.2105, 0.0984, 0.1532, 0.1922, 0.0139, 0.0206, 0.0413, 0.0342, 0.0338,\n",
      "         0.0458, 0.0215, 0.0162, 0.0225, 0.0068, 0.0170, 0.0165, 0.0073, 0.0485],\n",
      "        [0.0682, 0.1721, 0.0367, 0.1165, 0.0105, 0.0266, 0.0485, 0.0958, 0.0349,\n",
      "         0.1754, 0.0100, 0.0065, 0.0234, 0.0522, 0.0057, 0.0428, 0.0272, 0.0472],\n",
      "        [0.1952, 0.0314, 0.1173, 0.0222, 0.0503, 0.1199, 0.0095, 0.0583, 0.0161,\n",
      "         0.0148, 0.0323, 0.0637, 0.0168, 0.0823, 0.0042, 0.0134, 0.0553, 0.0967],\n",
      "        [0.0834, 0.0868, 0.0421, 0.0703, 0.0610, 0.1789, 0.0103, 0.0507, 0.1333,\n",
      "         0.0501, 0.0169, 0.0050, 0.0149, 0.0569, 0.0056, 0.0789, 0.0300, 0.0249],\n",
      "        [0.0553, 0.0753, 0.0687, 0.2778, 0.0331, 0.0471, 0.0224, 0.0481, 0.0246,\n",
      "         0.0410, 0.0151, 0.0037, 0.0064, 0.0438, 0.0182, 0.0599, 0.0180, 0.1416]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 5 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.1838, 0.1239, 0.1409, 0.0092, 0.1147, 0.0224, 0.1904, 0.0256, 0.0362,\n",
      "         0.0285, 0.0105, 0.0626, 0.0514],\n",
      "        [0.0808, 0.1455, 0.1726, 0.0121, 0.0777, 0.0552, 0.2094, 0.0319, 0.1055,\n",
      "         0.0196, 0.0067, 0.0431, 0.0399],\n",
      "        [0.1604, 0.0956, 0.1668, 0.0330, 0.0402, 0.0100, 0.1775, 0.0827, 0.0472,\n",
      "         0.0150, 0.0085, 0.0875, 0.0755],\n",
      "        [0.0320, 0.0325, 0.3986, 0.1770, 0.0581, 0.0341, 0.0735, 0.0111, 0.0174,\n",
      "         0.0600, 0.0090, 0.0629, 0.0339],\n",
      "        [0.0440, 0.1052, 0.0679, 0.1118, 0.1435, 0.0104, 0.3041, 0.0510, 0.0107,\n",
      "         0.0324, 0.0116, 0.0669, 0.0405],\n",
      "        [0.1397, 0.1558, 0.0394, 0.1057, 0.1693, 0.0129, 0.1481, 0.0659, 0.0153,\n",
      "         0.0161, 0.0159, 0.0968, 0.0192],\n",
      "        [0.0663, 0.1377, 0.1258, 0.0209, 0.0670, 0.0155, 0.0899, 0.1099, 0.0205,\n",
      "         0.0101, 0.0161, 0.2835, 0.0367],\n",
      "        [0.1063, 0.1292, 0.0874, 0.1589, 0.0619, 0.0068, 0.1688, 0.0263, 0.0668,\n",
      "         0.0834, 0.0093, 0.0722, 0.0227],\n",
      "        [0.0924, 0.1686, 0.1576, 0.0073, 0.0620, 0.0874, 0.0625, 0.0589, 0.1176,\n",
      "         0.0465, 0.0055, 0.0803, 0.0532],\n",
      "        [0.1630, 0.0691, 0.2107, 0.0123, 0.0508, 0.0436, 0.0811, 0.0087, 0.0965,\n",
      "         0.0652, 0.0169, 0.1634, 0.0187],\n",
      "        [0.1401, 0.0373, 0.1850, 0.1126, 0.0377, 0.0195, 0.1084, 0.0133, 0.0191,\n",
      "         0.0988, 0.0103, 0.1917, 0.0263],\n",
      "        [0.1123, 0.3135, 0.1133, 0.0251, 0.0416, 0.0677, 0.0690, 0.0267, 0.0558,\n",
      "         0.0518, 0.0043, 0.0972, 0.0216],\n",
      "        [0.1002, 0.0796, 0.0644, 0.0399, 0.0776, 0.0372, 0.4222, 0.0162, 0.0145,\n",
      "         0.0864, 0.0075, 0.0276, 0.0268]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 6 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.8805, 0.1195]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 7 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.8807, 0.1193],\n",
      "        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 8 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0313, 0.0229, 0.3627, 0.0259, 0.0413, 0.1038, 0.0561, 0.0051, 0.0133,\n",
      "         0.0484, 0.0386, 0.1506, 0.0310, 0.0690],\n",
      "        [0.0743, 0.0401, 0.1878, 0.0162, 0.0319, 0.0479, 0.1841, 0.0033, 0.0598,\n",
      "         0.1252, 0.0934, 0.0561, 0.0410, 0.0389],\n",
      "        [0.0985, 0.0576, 0.1172, 0.0230, 0.0806, 0.1554, 0.1164, 0.0049, 0.0176,\n",
      "         0.0561, 0.1769, 0.0379, 0.0131, 0.0446],\n",
      "        [0.0573, 0.0225, 0.4377, 0.0181, 0.0316, 0.1123, 0.0328, 0.0081, 0.0116,\n",
      "         0.0308, 0.1262, 0.0404, 0.0198, 0.0507],\n",
      "        [0.0305, 0.0442, 0.2826, 0.0218, 0.0414, 0.0900, 0.0844, 0.0054, 0.0240,\n",
      "         0.0809, 0.0897, 0.1386, 0.0107, 0.0558],\n",
      "        [0.0823, 0.0214, 0.3610, 0.0150, 0.0816, 0.0249, 0.0577, 0.0071, 0.0273,\n",
      "         0.0974, 0.0779, 0.0543, 0.0115, 0.0805],\n",
      "        [0.0700, 0.0364, 0.4728, 0.0119, 0.0264, 0.0788, 0.0493, 0.0076, 0.0107,\n",
      "         0.0759, 0.0498, 0.0340, 0.0216, 0.0547],\n",
      "        [0.0560, 0.0333, 0.3364, 0.0242, 0.0376, 0.0858, 0.1393, 0.0035, 0.0301,\n",
      "         0.0775, 0.0475, 0.0717, 0.0318, 0.0254],\n",
      "        [0.0451, 0.0236, 0.4414, 0.0185, 0.0590, 0.0971, 0.0539, 0.0050, 0.0168,\n",
      "         0.0572, 0.0496, 0.0503, 0.0164, 0.0661],\n",
      "        [0.0441, 0.0358, 0.0504, 0.0095, 0.0619, 0.1432, 0.2489, 0.0055, 0.0520,\n",
      "         0.0812, 0.0776, 0.0765, 0.0150, 0.0985],\n",
      "        [0.0480, 0.0211, 0.4027, 0.0165, 0.0252, 0.0507, 0.0941, 0.0068, 0.0250,\n",
      "         0.0344, 0.1730, 0.0539, 0.0168, 0.0319],\n",
      "        [0.0730, 0.0410, 0.0787, 0.0232, 0.2683, 0.0785, 0.0639, 0.0056, 0.0102,\n",
      "         0.0809, 0.1265, 0.0678, 0.0155, 0.0670],\n",
      "        [0.1654, 0.0282, 0.2036, 0.0170, 0.0418, 0.1679, 0.0516, 0.0059, 0.0144,\n",
      "         0.0712, 0.0915, 0.0843, 0.0226, 0.0345],\n",
      "        [0.0611, 0.0277, 0.3082, 0.0132, 0.0539, 0.1289, 0.0776, 0.0075, 0.0180,\n",
      "         0.0451, 0.1602, 0.0582, 0.0200, 0.0202]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 9 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0327, 0.0073, 0.0184, 0.0598, 0.0344, 0.0141, 0.1254, 0.0340, 0.0205,\n",
      "         0.0314, 0.0305, 0.0393, 0.0164, 0.5359],\n",
      "        [0.0193, 0.0108, 0.0147, 0.1349, 0.0269, 0.0220, 0.3291, 0.0230, 0.0276,\n",
      "         0.0522, 0.0373, 0.0449, 0.0185, 0.2387],\n",
      "        [0.0259, 0.0126, 0.0156, 0.0723, 0.0208, 0.0175, 0.1680, 0.0176, 0.0279,\n",
      "         0.0232, 0.0228, 0.0242, 0.0237, 0.5280],\n",
      "        [0.0292, 0.0079, 0.0321, 0.1097, 0.0316, 0.0116, 0.2195, 0.0277, 0.0187,\n",
      "         0.0415, 0.0291, 0.0405, 0.0361, 0.3648],\n",
      "        [0.0234, 0.0078, 0.0185, 0.0896, 0.0318, 0.0180, 0.1011, 0.0191, 0.0166,\n",
      "         0.0678, 0.0406, 0.0534, 0.0221, 0.4903],\n",
      "        [0.0230, 0.0127, 0.0118, 0.0944, 0.0264, 0.0247, 0.1014, 0.0170, 0.0188,\n",
      "         0.0255, 0.0226, 0.0345, 0.0185, 0.5687],\n",
      "        [0.0437, 0.0066, 0.0187, 0.0939, 0.0436, 0.0142, 0.2454, 0.0195, 0.0271,\n",
      "         0.0440, 0.0417, 0.0423, 0.0367, 0.3226],\n",
      "        [0.0273, 0.0100, 0.0135, 0.0806, 0.0233, 0.0203, 0.1774, 0.0282, 0.0159,\n",
      "         0.0539, 0.0321, 0.0285, 0.0259, 0.4630],\n",
      "        [0.0252, 0.0129, 0.0107, 0.0685, 0.0240, 0.0235, 0.1497, 0.0226, 0.0136,\n",
      "         0.0430, 0.0248, 0.0310, 0.0268, 0.5235],\n",
      "        [0.0310, 0.0103, 0.0307, 0.1281, 0.0278, 0.0141, 0.2269, 0.0167, 0.0138,\n",
      "         0.0704, 0.0390, 0.0917, 0.0344, 0.2649],\n",
      "        [0.0208, 0.0155, 0.0151, 0.0829, 0.0281, 0.0204, 0.4358, 0.0144, 0.0363,\n",
      "         0.0210, 0.0194, 0.0410, 0.0267, 0.2226],\n",
      "        [0.0362, 0.0100, 0.0085, 0.1193, 0.0482, 0.0337, 0.2798, 0.0205, 0.0265,\n",
      "         0.0577, 0.0298, 0.0464, 0.0239, 0.2594],\n",
      "        [0.0313, 0.0118, 0.0133, 0.1249, 0.0202, 0.0181, 0.3857, 0.0295, 0.0228,\n",
      "         0.0321, 0.0395, 0.0520, 0.0185, 0.2005],\n",
      "        [0.0177, 0.0105, 0.0189, 0.0794, 0.0190, 0.0242, 0.1557, 0.0187, 0.0182,\n",
      "         0.0278, 0.0333, 0.0393, 0.0225, 0.5147]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 10 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 11 - Output:\n",
      "torch.Size([11, 11])\n",
      "1\n",
      "tensor([[0.0140, 0.0437, 0.2862, 0.1977, 0.0258, 0.0166, 0.0941, 0.0373, 0.0229,\n",
      "         0.1192, 0.1426],\n",
      "        [0.0121, 0.1957, 0.1580, 0.0832, 0.0256, 0.0344, 0.0990, 0.0117, 0.0540,\n",
      "         0.0997, 0.2265],\n",
      "        [0.0076, 0.0662, 0.1353, 0.3192, 0.0313, 0.0263, 0.0637, 0.0385, 0.0367,\n",
      "         0.0530, 0.2223],\n",
      "        [0.0095, 0.0761, 0.2182, 0.2184, 0.0504, 0.0198, 0.1175, 0.0199, 0.0462,\n",
      "         0.0482, 0.1759],\n",
      "        [0.0133, 0.0934, 0.1261, 0.0972, 0.0311, 0.0331, 0.1868, 0.0095, 0.1119,\n",
      "         0.0619, 0.2358],\n",
      "        [0.0167, 0.0696, 0.1862, 0.0498, 0.0209, 0.0195, 0.2478, 0.0212, 0.0306,\n",
      "         0.1253, 0.2123],\n",
      "        [0.0279, 0.0264, 0.6113, 0.0550, 0.0503, 0.0152, 0.0660, 0.0159, 0.0174,\n",
      "         0.0372, 0.0775],\n",
      "        [0.0110, 0.1159, 0.0811, 0.2265, 0.0592, 0.0220, 0.1464, 0.0150, 0.0324,\n",
      "         0.0683, 0.2221],\n",
      "        [0.0216, 0.0845, 0.3207, 0.1904, 0.0224, 0.0173, 0.1249, 0.0178, 0.0236,\n",
      "         0.0750, 0.1018],\n",
      "        [0.0093, 0.0952, 0.3143, 0.0888, 0.0857, 0.0169, 0.1868, 0.0199, 0.0433,\n",
      "         0.0752, 0.0646],\n",
      "        [0.0177, 0.0835, 0.1077, 0.3042, 0.0854, 0.0106, 0.0966, 0.0233, 0.0211,\n",
      "         0.1334, 0.1165]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 12 - Output:\n",
      "torch.Size([16, 16])\n",
      "1\n",
      "tensor([[0.0215, 0.0674, 0.1903, 0.0219, 0.0095, 0.0251, 0.0914, 0.1602, 0.0399,\n",
      "         0.1196, 0.0096, 0.0083, 0.0367, 0.0532, 0.1259, 0.0196],\n",
      "        [0.0367, 0.0431, 0.0603, 0.0031, 0.0170, 0.0208, 0.0889, 0.1452, 0.1871,\n",
      "         0.0487, 0.0345, 0.0125, 0.0349, 0.0862, 0.1317, 0.0495],\n",
      "        [0.0164, 0.0236, 0.0447, 0.0157, 0.0149, 0.0161, 0.1390, 0.1117, 0.0538,\n",
      "         0.0583, 0.0216, 0.0062, 0.0226, 0.1238, 0.2961, 0.0356],\n",
      "        [0.0241, 0.0507, 0.1000, 0.0126, 0.0037, 0.0323, 0.2131, 0.1518, 0.0640,\n",
      "         0.0496, 0.0170, 0.0151, 0.0481, 0.1013, 0.0774, 0.0392],\n",
      "        [0.0151, 0.0308, 0.0540, 0.0128, 0.0081, 0.0353, 0.3188, 0.1418, 0.0966,\n",
      "         0.0393, 0.0097, 0.0144, 0.0482, 0.0613, 0.0931, 0.0205],\n",
      "        [0.0259, 0.0431, 0.1115, 0.0116, 0.0057, 0.0191, 0.0906, 0.1442, 0.1225,\n",
      "         0.0612, 0.0260, 0.0107, 0.0407, 0.1695, 0.0961, 0.0215],\n",
      "        [0.0988, 0.0459, 0.1059, 0.0186, 0.0077, 0.0143, 0.0967, 0.0851, 0.0897,\n",
      "         0.0853, 0.0301, 0.0047, 0.0325, 0.1677, 0.0841, 0.0328],\n",
      "        [0.0126, 0.0419, 0.1258, 0.0258, 0.0126, 0.0150, 0.1556, 0.1306, 0.2504,\n",
      "         0.0536, 0.0097, 0.0099, 0.0293, 0.0460, 0.0506, 0.0305],\n",
      "        [0.0334, 0.0305, 0.1775, 0.0097, 0.0108, 0.0277, 0.1519, 0.0916, 0.0752,\n",
      "         0.0554, 0.0189, 0.0056, 0.0269, 0.1059, 0.0884, 0.0906],\n",
      "        [0.0260, 0.0289, 0.1696, 0.0104, 0.0061, 0.0306, 0.1408, 0.1305, 0.1031,\n",
      "         0.0642, 0.0158, 0.0110, 0.0462, 0.1364, 0.0457, 0.0347],\n",
      "        [0.0196, 0.0185, 0.0619, 0.0093, 0.0163, 0.0341, 0.2485, 0.1161, 0.0476,\n",
      "         0.0554, 0.0104, 0.0087, 0.0334, 0.0568, 0.2154, 0.0480],\n",
      "        [0.0264, 0.0730, 0.1165, 0.0128, 0.0075, 0.0283, 0.0860, 0.1849, 0.1116,\n",
      "         0.0397, 0.0077, 0.0241, 0.0278, 0.1113, 0.1247, 0.0177],\n",
      "        [0.0274, 0.0570, 0.1410, 0.0078, 0.0055, 0.0570, 0.1317, 0.1630, 0.0696,\n",
      "         0.0504, 0.0392, 0.0074, 0.0412, 0.0870, 0.0553, 0.0594],\n",
      "        [0.0133, 0.0301, 0.0940, 0.0180, 0.0144, 0.0273, 0.1847, 0.1768, 0.1131,\n",
      "         0.0366, 0.0163, 0.0083, 0.0321, 0.0346, 0.1846, 0.0159],\n",
      "        [0.0394, 0.0504, 0.0971, 0.0102, 0.0082, 0.0264, 0.1800, 0.2530, 0.0398,\n",
      "         0.0454, 0.0373, 0.0054, 0.0311, 0.0841, 0.0625, 0.0295],\n",
      "        [0.0280, 0.0446, 0.0639, 0.0142, 0.0079, 0.0361, 0.2523, 0.1212, 0.1456,\n",
      "         0.0578, 0.0227, 0.0058, 0.0386, 0.0672, 0.0771, 0.0169]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 13 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 14 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0380, 0.0305, 0.3367, 0.0374, 0.0710, 0.0747, 0.0428, 0.0688, 0.0034,\n",
      "         0.0097, 0.0689, 0.0923, 0.0276, 0.0579, 0.0403],\n",
      "        [0.0286, 0.0481, 0.2822, 0.0167, 0.0181, 0.1341, 0.0529, 0.0306, 0.0034,\n",
      "         0.0305, 0.1010, 0.0848, 0.0448, 0.0733, 0.0508],\n",
      "        [0.0170, 0.0559, 0.1630, 0.0534, 0.0737, 0.0738, 0.0358, 0.0527, 0.0030,\n",
      "         0.0192, 0.0779, 0.2637, 0.0447, 0.0262, 0.0399],\n",
      "        [0.0241, 0.0335, 0.3080, 0.0311, 0.0170, 0.0729, 0.0245, 0.1175, 0.0041,\n",
      "         0.0250, 0.1098, 0.0808, 0.0470, 0.0198, 0.0850],\n",
      "        [0.0380, 0.0665, 0.1706, 0.0316, 0.0406, 0.0510, 0.0434, 0.0758, 0.0022,\n",
      "         0.0189, 0.1799, 0.0863, 0.0635, 0.0509, 0.0807],\n",
      "        [0.0335, 0.0324, 0.1193, 0.0250, 0.0420, 0.0355, 0.0336, 0.0738, 0.0094,\n",
      "         0.0044, 0.0246, 0.3462, 0.0598, 0.0973, 0.0633],\n",
      "        [0.0381, 0.0539, 0.2796, 0.0447, 0.0452, 0.0622, 0.0480, 0.1037, 0.0021,\n",
      "         0.0224, 0.0496, 0.1034, 0.0542, 0.0379, 0.0551],\n",
      "        [0.0315, 0.0517, 0.0959, 0.0217, 0.0625, 0.0570, 0.0512, 0.0164, 0.0039,\n",
      "         0.0122, 0.0943, 0.2869, 0.1072, 0.0464, 0.0611],\n",
      "        [0.0641, 0.0820, 0.1317, 0.0217, 0.0286, 0.0904, 0.0369, 0.0510, 0.0042,\n",
      "         0.0070, 0.0930, 0.1583, 0.0648, 0.0796, 0.0867],\n",
      "        [0.0280, 0.0715, 0.1695, 0.0354, 0.0139, 0.1236, 0.0303, 0.0606, 0.0035,\n",
      "         0.0201, 0.0375, 0.1654, 0.1193, 0.0526, 0.0688],\n",
      "        [0.0328, 0.0550, 0.2656, 0.0337, 0.0314, 0.1208, 0.0203, 0.0296, 0.0026,\n",
      "         0.0383, 0.0660, 0.1173, 0.0513, 0.0487, 0.0865],\n",
      "        [0.0366, 0.0436, 0.3012, 0.0334, 0.0837, 0.0512, 0.0242, 0.1261, 0.0026,\n",
      "         0.0219, 0.0464, 0.0858, 0.0591, 0.0318, 0.0524],\n",
      "        [0.0350, 0.0593, 0.1274, 0.0202, 0.1049, 0.0477, 0.0440, 0.0883, 0.0041,\n",
      "         0.0074, 0.1114, 0.1496, 0.0477, 0.0521, 0.1010],\n",
      "        [0.0232, 0.0424, 0.2773, 0.0144, 0.0165, 0.0850, 0.0693, 0.0322, 0.0053,\n",
      "         0.0146, 0.0415, 0.1925, 0.0728, 0.0513, 0.0618],\n",
      "        [0.0438, 0.0391, 0.2662, 0.0335, 0.0283, 0.1425, 0.0530, 0.0264, 0.0032,\n",
      "         0.0143, 0.0527, 0.1088, 0.1035, 0.0368, 0.0478]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 15 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.5491, 0.2730, 0.0390, 0.0546, 0.0842],\n",
      "        [0.6516, 0.0494, 0.1803, 0.0456, 0.0730],\n",
      "        [0.6005, 0.0854, 0.0768, 0.0315, 0.2058],\n",
      "        [0.7162, 0.1016, 0.0830, 0.0590, 0.0402],\n",
      "        [0.6192, 0.1076, 0.0267, 0.1385, 0.1080]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 16 - Output:\n",
      "torch.Size([10, 10])\n",
      "1\n",
      "tensor([[0.0849, 0.0461, 0.0585, 0.0454, 0.0918, 0.1507, 0.0064, 0.2917, 0.1811,\n",
      "         0.0434],\n",
      "        [0.0212, 0.0533, 0.2937, 0.0315, 0.1931, 0.0864, 0.0303, 0.1844, 0.0941,\n",
      "         0.0121],\n",
      "        [0.1482, 0.0180, 0.2482, 0.0217, 0.1513, 0.0153, 0.0333, 0.1845, 0.1142,\n",
      "         0.0654],\n",
      "        [0.0961, 0.0325, 0.0492, 0.0269, 0.5372, 0.0136, 0.0183, 0.1125, 0.0606,\n",
      "         0.0531],\n",
      "        [0.2731, 0.0461, 0.1054, 0.0363, 0.2509, 0.0405, 0.0113, 0.0747, 0.1423,\n",
      "         0.0194],\n",
      "        [0.1402, 0.0249, 0.1568, 0.0161, 0.2646, 0.0389, 0.0334, 0.0964, 0.2100,\n",
      "         0.0187],\n",
      "        [0.1792, 0.0677, 0.0586, 0.0244, 0.1552, 0.0933, 0.0073, 0.1525, 0.2176,\n",
      "         0.0442],\n",
      "        [0.0432, 0.0612, 0.0253, 0.0101, 0.5068, 0.0778, 0.0248, 0.1068, 0.0953,\n",
      "         0.0486],\n",
      "        [0.1641, 0.0673, 0.2317, 0.0234, 0.1537, 0.0499, 0.0068, 0.1099, 0.0842,\n",
      "         0.1092],\n",
      "        [0.0925, 0.0271, 0.1088, 0.0490, 0.3661, 0.0269, 0.0087, 0.1073, 0.1503,\n",
      "         0.0634]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 17 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.8776, 0.1224],\n",
      "        [0.1196, 0.8804]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 18 - Output:\n",
      "torch.Size([11, 11])\n",
      "1\n",
      "tensor([[0.0813, 0.0951, 0.0580, 0.0747, 0.0039, 0.1609, 0.0471, 0.0558, 0.1231,\n",
      "         0.0754, 0.2246],\n",
      "        [0.1016, 0.0733, 0.0252, 0.0527, 0.0062, 0.0872, 0.0529, 0.0403, 0.0460,\n",
      "         0.0374, 0.4772],\n",
      "        [0.0841, 0.1029, 0.0370, 0.1172, 0.0041, 0.1420, 0.0477, 0.0641, 0.1628,\n",
      "         0.0704, 0.1677],\n",
      "        [0.0585, 0.1241, 0.0341, 0.0479, 0.0207, 0.2686, 0.0066, 0.0636, 0.1192,\n",
      "         0.0700, 0.1868],\n",
      "        [0.1426, 0.0560, 0.2292, 0.0519, 0.0068, 0.1259, 0.0324, 0.0347, 0.1139,\n",
      "         0.0247, 0.1819],\n",
      "        [0.1688, 0.0512, 0.0208, 0.0444, 0.0071, 0.1400, 0.0360, 0.0445, 0.1123,\n",
      "         0.0706, 0.3045],\n",
      "        [0.0624, 0.0871, 0.0126, 0.1219, 0.0310, 0.1162, 0.0097, 0.2117, 0.1077,\n",
      "         0.0355, 0.2040],\n",
      "        [0.1150, 0.0739, 0.0951, 0.0439, 0.0046, 0.0995, 0.0676, 0.0358, 0.0812,\n",
      "         0.0481, 0.3354],\n",
      "        [0.0796, 0.0839, 0.2360, 0.0257, 0.0071, 0.1460, 0.0226, 0.1094, 0.1342,\n",
      "         0.0318, 0.1237],\n",
      "        [0.1007, 0.0512, 0.0348, 0.0620, 0.0045, 0.1163, 0.0553, 0.0776, 0.1048,\n",
      "         0.0701, 0.3228],\n",
      "        [0.0751, 0.1265, 0.0127, 0.0391, 0.0087, 0.0725, 0.0385, 0.0563, 0.0816,\n",
      "         0.0756, 0.4133]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 19 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.2154, 0.5717, 0.0754, 0.0463, 0.0614, 0.0297],\n",
      "        [0.5024, 0.2474, 0.1286, 0.0389, 0.0363, 0.0465],\n",
      "        [0.6509, 0.0677, 0.1291, 0.0732, 0.0532, 0.0259],\n",
      "        [0.6032, 0.1919, 0.0781, 0.0390, 0.0380, 0.0497],\n",
      "        [0.0832, 0.3235, 0.4230, 0.0705, 0.0782, 0.0217],\n",
      "        [0.2727, 0.3258, 0.2348, 0.1004, 0.0442, 0.0220]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 20 - Output:\n",
      "torch.Size([16, 16])\n",
      "1\n",
      "tensor([[0.0114, 0.1103, 0.0192, 0.0348, 0.0414, 0.0206, 0.0443, 0.0179, 0.0040,\n",
      "         0.2820, 0.1426, 0.0838, 0.0295, 0.0589, 0.0575, 0.0419],\n",
      "        [0.0278, 0.0699, 0.0306, 0.0249, 0.0328, 0.0095, 0.0559, 0.0177, 0.0060,\n",
      "         0.1854, 0.1735, 0.1713, 0.0620, 0.0941, 0.0228, 0.0156],\n",
      "        [0.0136, 0.0706, 0.1110, 0.0092, 0.0904, 0.0084, 0.0407, 0.0297, 0.0088,\n",
      "         0.0983, 0.1577, 0.1272, 0.0746, 0.0847, 0.0577, 0.0174],\n",
      "        [0.0074, 0.3224, 0.0778, 0.0385, 0.0231, 0.0190, 0.0268, 0.0135, 0.0071,\n",
      "         0.1307, 0.0368, 0.0916, 0.0387, 0.0432, 0.0992, 0.0242],\n",
      "        [0.0056, 0.0797, 0.0547, 0.0341, 0.0195, 0.0069, 0.0434, 0.0419, 0.0128,\n",
      "         0.1795, 0.1084, 0.1152, 0.0881, 0.0951, 0.0900, 0.0250],\n",
      "        [0.0216, 0.1525, 0.0647, 0.0201, 0.0121, 0.0052, 0.0324, 0.0136, 0.0168,\n",
      "         0.1293, 0.0726, 0.2185, 0.0379, 0.0647, 0.0609, 0.0769],\n",
      "        [0.0161, 0.0934, 0.0248, 0.0400, 0.0436, 0.0112, 0.0462, 0.0200, 0.0033,\n",
      "         0.2249, 0.0796, 0.0818, 0.0812, 0.0667, 0.0708, 0.0964],\n",
      "        [0.0054, 0.1339, 0.0257, 0.0120, 0.0318, 0.0109, 0.0213, 0.0261, 0.0325,\n",
      "         0.3230, 0.0967, 0.0970, 0.0642, 0.0597, 0.0339, 0.0257],\n",
      "        [0.0079, 0.1242, 0.0327, 0.0059, 0.1437, 0.1011, 0.0309, 0.0414, 0.0091,\n",
      "         0.0625, 0.0483, 0.0631, 0.0300, 0.0321, 0.2267, 0.0404],\n",
      "        [0.0171, 0.0703, 0.0244, 0.0142, 0.0402, 0.0074, 0.0374, 0.0361, 0.0054,\n",
      "         0.1028, 0.2125, 0.1150, 0.0940, 0.0764, 0.0552, 0.0918],\n",
      "        [0.0104, 0.0925, 0.0763, 0.0602, 0.1173, 0.0166, 0.0823, 0.0083, 0.0056,\n",
      "         0.1064, 0.0541, 0.1588, 0.0296, 0.0259, 0.0794, 0.0764],\n",
      "        [0.0084, 0.0322, 0.0439, 0.0134, 0.0126, 0.0206, 0.0277, 0.0252, 0.0065,\n",
      "         0.4655, 0.0361, 0.0975, 0.0510, 0.0315, 0.0680, 0.0600],\n",
      "        [0.0094, 0.0644, 0.0151, 0.0268, 0.0353, 0.0379, 0.0533, 0.0211, 0.0039,\n",
      "         0.0876, 0.0650, 0.2149, 0.0467, 0.0717, 0.1587, 0.0883],\n",
      "        [0.0327, 0.1421, 0.1014, 0.0110, 0.0464, 0.0117, 0.0492, 0.0168, 0.0044,\n",
      "         0.1199, 0.1307, 0.0824, 0.0512, 0.0266, 0.1228, 0.0509],\n",
      "        [0.0100, 0.1048, 0.0167, 0.0103, 0.1133, 0.0094, 0.0395, 0.0202, 0.0129,\n",
      "         0.1736, 0.0474, 0.1491, 0.0904, 0.0948, 0.0650, 0.0427],\n",
      "        [0.0125, 0.2045, 0.0484, 0.0611, 0.0586, 0.0230, 0.0430, 0.0237, 0.0025,\n",
      "         0.0966, 0.0456, 0.1383, 0.0435, 0.0793, 0.0817, 0.0377]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 21 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.5014, 0.0257, 0.0923, 0.1861, 0.1572, 0.0374],\n",
      "        [0.3183, 0.0159, 0.1579, 0.2562, 0.1728, 0.0788],\n",
      "        [0.2144, 0.0505, 0.1677, 0.3643, 0.1840, 0.0190],\n",
      "        [0.4170, 0.0969, 0.0942, 0.2289, 0.1464, 0.0166],\n",
      "        [0.2526, 0.0323, 0.1733, 0.4140, 0.0994, 0.0284],\n",
      "        [0.4433, 0.1090, 0.0445, 0.2636, 0.1167, 0.0228]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 22 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.0484, 0.5291, 0.0500, 0.0189, 0.1811, 0.0584, 0.1141],\n",
      "        [0.0645, 0.3939, 0.0133, 0.1188, 0.2412, 0.0848, 0.0835],\n",
      "        [0.0413, 0.5137, 0.0199, 0.0401, 0.1569, 0.0999, 0.1281],\n",
      "        [0.0271, 0.2134, 0.0904, 0.0185, 0.2412, 0.1490, 0.2606],\n",
      "        [0.0757, 0.1513, 0.0351, 0.0192, 0.4938, 0.1655, 0.0595],\n",
      "        [0.0115, 0.2545, 0.0669, 0.2228, 0.2150, 0.1036, 0.1256],\n",
      "        [0.1040, 0.3948, 0.0231, 0.0226, 0.1388, 0.2259, 0.0908]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 23 - Output:\n",
      "torch.Size([16, 16])\n",
      "1\n",
      "tensor([[0.0186, 0.0447, 0.0285, 0.0130, 0.0337, 0.0119, 0.0391, 0.3348, 0.0925,\n",
      "         0.0246, 0.0504, 0.2058, 0.0115, 0.0655, 0.0111, 0.0143],\n",
      "        [0.0210, 0.0138, 0.0363, 0.0459, 0.0124, 0.0183, 0.0050, 0.0215, 0.0595,\n",
      "         0.0476, 0.0625, 0.5411, 0.0174, 0.0256, 0.0202, 0.0518],\n",
      "        [0.0273, 0.0514, 0.0383, 0.0367, 0.0082, 0.0092, 0.0390, 0.0351, 0.0827,\n",
      "         0.0848, 0.0530, 0.1461, 0.0124, 0.2952, 0.0720, 0.0085],\n",
      "        [0.0309, 0.0516, 0.1923, 0.0657, 0.0459, 0.0442, 0.0864, 0.0167, 0.1114,\n",
      "         0.0118, 0.0868, 0.0104, 0.0990, 0.0098, 0.1289, 0.0083],\n",
      "        [0.0098, 0.0869, 0.0412, 0.0312, 0.0058, 0.0192, 0.1401, 0.0315, 0.0772,\n",
      "         0.0539, 0.1600, 0.1857, 0.0181, 0.0308, 0.0936, 0.0148],\n",
      "        [0.0263, 0.0244, 0.0669, 0.1150, 0.0522, 0.0137, 0.0171, 0.1997, 0.0453,\n",
      "         0.0345, 0.0197, 0.0144, 0.0388, 0.2888, 0.0058, 0.0375],\n",
      "        [0.0207, 0.0275, 0.0197, 0.0208, 0.0069, 0.0135, 0.0629, 0.1777, 0.0265,\n",
      "         0.1028, 0.0891, 0.1698, 0.0293, 0.1614, 0.0115, 0.0600],\n",
      "        [0.1145, 0.0535, 0.0138, 0.0136, 0.0119, 0.0271, 0.0434, 0.0375, 0.0235,\n",
      "         0.1065, 0.0181, 0.1360, 0.0106, 0.2996, 0.0148, 0.0756],\n",
      "        [0.0412, 0.0323, 0.0661, 0.0796, 0.0820, 0.0123, 0.0389, 0.0107, 0.0467,\n",
      "         0.0062, 0.0238, 0.3453, 0.0262, 0.0119, 0.1293, 0.0475],\n",
      "        [0.0422, 0.0232, 0.1551, 0.0052, 0.0508, 0.0667, 0.1571, 0.0923, 0.1870,\n",
      "         0.0110, 0.0344, 0.0290, 0.0259, 0.0857, 0.0157, 0.0187],\n",
      "        [0.0161, 0.0599, 0.0269, 0.0390, 0.0560, 0.0080, 0.0667, 0.3693, 0.0538,\n",
      "         0.0137, 0.0536, 0.0100, 0.0142, 0.0239, 0.1657, 0.0231],\n",
      "        [0.0575, 0.0240, 0.0656, 0.0314, 0.0341, 0.0033, 0.0162, 0.4273, 0.0274,\n",
      "         0.0293, 0.0376, 0.0245, 0.0180, 0.0296, 0.1332, 0.0410],\n",
      "        [0.0336, 0.0447, 0.0826, 0.0135, 0.0645, 0.0078, 0.0448, 0.0844, 0.0807,\n",
      "         0.0190, 0.0603, 0.0485, 0.0070, 0.1501, 0.2456, 0.0128],\n",
      "        [0.0474, 0.1098, 0.0483, 0.0502, 0.0809, 0.0509, 0.0196, 0.0260, 0.0699,\n",
      "         0.1806, 0.0467, 0.0031, 0.0138, 0.2029, 0.0253, 0.0245],\n",
      "        [0.0139, 0.0284, 0.0448, 0.1582, 0.0355, 0.0100, 0.0309, 0.0536, 0.0450,\n",
      "         0.0260, 0.0472, 0.4258, 0.0061, 0.0357, 0.0117, 0.0270],\n",
      "        [0.0120, 0.0745, 0.2059, 0.0743, 0.0260, 0.0719, 0.0259, 0.0168, 0.0239,\n",
      "         0.0781, 0.0672, 0.1093, 0.0091, 0.0320, 0.1657, 0.0075]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 24 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.0243, 0.0732, 0.0819, 0.0903, 0.0214, 0.0354, 0.0423, 0.0826, 0.1198,\n",
      "         0.3764, 0.0154, 0.0305, 0.0065],\n",
      "        [0.0531, 0.0054, 0.0403, 0.0344, 0.0476, 0.0702, 0.0330, 0.0727, 0.0287,\n",
      "         0.4796, 0.0242, 0.0158, 0.0951],\n",
      "        [0.0171, 0.0273, 0.0914, 0.0624, 0.0391, 0.1267, 0.1590, 0.1400, 0.0270,\n",
      "         0.2567, 0.0263, 0.0082, 0.0188],\n",
      "        [0.0209, 0.0047, 0.0606, 0.0418, 0.0277, 0.0423, 0.0858, 0.1579, 0.0221,\n",
      "         0.3194, 0.0775, 0.0843, 0.0549],\n",
      "        [0.0444, 0.0204, 0.0796, 0.1126, 0.0533, 0.0135, 0.0150, 0.1501, 0.0170,\n",
      "         0.2645, 0.1194, 0.0134, 0.0969],\n",
      "        [0.0689, 0.0159, 0.0784, 0.0233, 0.0707, 0.0169, 0.0066, 0.1680, 0.0424,\n",
      "         0.2793, 0.0470, 0.0516, 0.1309],\n",
      "        [0.0775, 0.0179, 0.0531, 0.0373, 0.0400, 0.0174, 0.0060, 0.0782, 0.0526,\n",
      "         0.5021, 0.0538, 0.0200, 0.0440],\n",
      "        [0.0131, 0.0203, 0.0206, 0.0295, 0.0295, 0.1415, 0.0425, 0.2929, 0.0681,\n",
      "         0.2321, 0.0748, 0.0205, 0.0146],\n",
      "        [0.0272, 0.0119, 0.0316, 0.0460, 0.0355, 0.1165, 0.0340, 0.1253, 0.1183,\n",
      "         0.3676, 0.0476, 0.0074, 0.0311],\n",
      "        [0.0439, 0.0114, 0.0375, 0.0389, 0.0544, 0.1363, 0.0136, 0.1553, 0.0925,\n",
      "         0.2463, 0.0423, 0.0092, 0.1184],\n",
      "        [0.0165, 0.0191, 0.0714, 0.0162, 0.0342, 0.0386, 0.0204, 0.2940, 0.0409,\n",
      "         0.3002, 0.1063, 0.0263, 0.0158],\n",
      "        [0.0490, 0.0183, 0.0658, 0.0764, 0.0651, 0.0469, 0.0269, 0.2888, 0.0466,\n",
      "         0.1581, 0.1334, 0.0054, 0.0194],\n",
      "        [0.0600, 0.0153, 0.1183, 0.0591, 0.0415, 0.0339, 0.0175, 0.1142, 0.0852,\n",
      "         0.1965, 0.0875, 0.0054, 0.1656]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 25 - Output:\n",
      "torch.Size([11, 11])\n",
      "1\n",
      "tensor([[0.0153, 0.0577, 0.0098, 0.1330, 0.0303, 0.0428, 0.0698, 0.3372, 0.0890,\n",
      "         0.1788, 0.0364],\n",
      "        [0.0461, 0.0332, 0.0155, 0.1177, 0.0407, 0.1161, 0.0143, 0.1935, 0.1167,\n",
      "         0.2877, 0.0186],\n",
      "        [0.0306, 0.0068, 0.0466, 0.1345, 0.0646, 0.0513, 0.0351, 0.1727, 0.2935,\n",
      "         0.1339, 0.0304],\n",
      "        [0.1215, 0.0483, 0.0382, 0.1074, 0.0165, 0.0793, 0.0561, 0.0165, 0.4436,\n",
      "         0.0604, 0.0121],\n",
      "        [0.1800, 0.0158, 0.0198, 0.0747, 0.2405, 0.0185, 0.0440, 0.1877, 0.0810,\n",
      "         0.1194, 0.0184],\n",
      "        [0.0375, 0.0254, 0.0291, 0.1055, 0.0365, 0.0153, 0.0185, 0.0460, 0.3383,\n",
      "         0.3091, 0.0387],\n",
      "        [0.0362, 0.0215, 0.0603, 0.1768, 0.1707, 0.0368, 0.0070, 0.1015, 0.2686,\n",
      "         0.0606, 0.0600],\n",
      "        [0.0729, 0.0169, 0.0344, 0.0297, 0.4763, 0.0227, 0.0177, 0.1212, 0.0733,\n",
      "         0.1148, 0.0202],\n",
      "        [0.2761, 0.0274, 0.0091, 0.0388, 0.1175, 0.0156, 0.0758, 0.0841, 0.0482,\n",
      "         0.1540, 0.1534],\n",
      "        [0.1773, 0.0298, 0.0267, 0.0391, 0.1265, 0.0252, 0.0211, 0.0843, 0.3890,\n",
      "         0.0689, 0.0120],\n",
      "        [0.0702, 0.0165, 0.0838, 0.1182, 0.2071, 0.0777, 0.0134, 0.0537, 0.2673,\n",
      "         0.0798, 0.0123]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 26 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0077, 0.0294, 0.1180, 0.0216, 0.0084, 0.0266, 0.0137, 0.1293, 0.0261,\n",
      "         0.0159, 0.0199, 0.0523, 0.0227, 0.0395, 0.0228, 0.1380, 0.3083],\n",
      "        [0.0040, 0.0474, 0.1151, 0.0167, 0.0286, 0.0400, 0.0506, 0.1005, 0.0651,\n",
      "         0.0504, 0.0250, 0.0720, 0.0726, 0.0047, 0.0510, 0.0448, 0.2116],\n",
      "        [0.0301, 0.0052, 0.1568, 0.0146, 0.0298, 0.0219, 0.0232, 0.0674, 0.0410,\n",
      "         0.0208, 0.0071, 0.4084, 0.0204, 0.0306, 0.0327, 0.0286, 0.0614],\n",
      "        [0.0251, 0.0058, 0.2369, 0.0109, 0.0282, 0.0218, 0.0173, 0.0907, 0.0765,\n",
      "         0.0171, 0.0302, 0.0776, 0.0335, 0.0474, 0.0101, 0.0772, 0.1935],\n",
      "        [0.0156, 0.1189, 0.2132, 0.0117, 0.0044, 0.0600, 0.0671, 0.0399, 0.0468,\n",
      "         0.0288, 0.0116, 0.0905, 0.0309, 0.0226, 0.0197, 0.0445, 0.1738],\n",
      "        [0.0342, 0.0451, 0.0717, 0.0164, 0.0357, 0.0124, 0.0122, 0.1168, 0.0785,\n",
      "         0.0052, 0.0146, 0.1983, 0.0149, 0.0733, 0.1106, 0.0356, 0.1244],\n",
      "        [0.0366, 0.0143, 0.0800, 0.0095, 0.0113, 0.0427, 0.0344, 0.3033, 0.1169,\n",
      "         0.0108, 0.0169, 0.0500, 0.0465, 0.0073, 0.0904, 0.0855, 0.0436],\n",
      "        [0.0235, 0.0189, 0.1946, 0.0181, 0.0122, 0.0178, 0.0600, 0.1687, 0.0482,\n",
      "         0.0070, 0.0075, 0.0789, 0.0589, 0.0219, 0.0773, 0.0814, 0.1049],\n",
      "        [0.0311, 0.0859, 0.1732, 0.0186, 0.0163, 0.0476, 0.0998, 0.0598, 0.0363,\n",
      "         0.0052, 0.0197, 0.1160, 0.0219, 0.0106, 0.0126, 0.0513, 0.1941],\n",
      "        [0.0409, 0.0403, 0.0904, 0.0297, 0.0225, 0.0271, 0.0171, 0.0653, 0.0325,\n",
      "         0.0032, 0.0153, 0.3691, 0.0454, 0.0167, 0.0156, 0.0909, 0.0781],\n",
      "        [0.0091, 0.1118, 0.0883, 0.0300, 0.0128, 0.0264, 0.0176, 0.0266, 0.0596,\n",
      "         0.0154, 0.0053, 0.2470, 0.0568, 0.0271, 0.0850, 0.0782, 0.1029],\n",
      "        [0.0075, 0.0329, 0.0884, 0.0202, 0.0583, 0.0200, 0.0058, 0.0700, 0.0453,\n",
      "         0.0183, 0.0400, 0.1011, 0.0121, 0.0344, 0.0259, 0.0189, 0.4010],\n",
      "        [0.0142, 0.0147, 0.0169, 0.0476, 0.0041, 0.0882, 0.0531, 0.2545, 0.0518,\n",
      "         0.0137, 0.0431, 0.0892, 0.0162, 0.0391, 0.0371, 0.0377, 0.1788],\n",
      "        [0.0195, 0.0121, 0.1190, 0.0487, 0.0297, 0.0219, 0.0613, 0.1189, 0.0295,\n",
      "         0.0206, 0.0252, 0.1892, 0.0904, 0.0029, 0.0399, 0.0567, 0.1145],\n",
      "        [0.0054, 0.0231, 0.0996, 0.0151, 0.0145, 0.0600, 0.0193, 0.1898, 0.0844,\n",
      "         0.0169, 0.0155, 0.1018, 0.0491, 0.0133, 0.0618, 0.1006, 0.1296],\n",
      "        [0.0159, 0.0211, 0.0812, 0.0115, 0.0174, 0.0219, 0.0091, 0.0403, 0.0641,\n",
      "         0.0047, 0.0317, 0.0719, 0.0302, 0.0380, 0.0273, 0.0312, 0.4825],\n",
      "        [0.0094, 0.1487, 0.0987, 0.0148, 0.0358, 0.0354, 0.0574, 0.1016, 0.0147,\n",
      "         0.0103, 0.0616, 0.0837, 0.0327, 0.0062, 0.0361, 0.0253, 0.2276]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 27 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0731, 0.0278, 0.0218, 0.0760, 0.0841, 0.0293, 0.0111, 0.2694, 0.0199,\n",
      "         0.0075, 0.0278, 0.0989, 0.0538, 0.1997],\n",
      "        [0.0902, 0.0244, 0.0237, 0.1884, 0.0765, 0.0442, 0.0067, 0.1749, 0.0325,\n",
      "         0.0159, 0.0133, 0.1394, 0.0515, 0.1185],\n",
      "        [0.0975, 0.0479, 0.0262, 0.0899, 0.0603, 0.0540, 0.0105, 0.2077, 0.0412,\n",
      "         0.0047, 0.0235, 0.0714, 0.1182, 0.1472],\n",
      "        [0.1071, 0.0521, 0.0243, 0.1498, 0.0767, 0.0337, 0.0075, 0.1256, 0.0235,\n",
      "         0.0085, 0.0213, 0.1679, 0.1039, 0.0982],\n",
      "        [0.1619, 0.0286, 0.0338, 0.0705, 0.1110, 0.0478, 0.0097, 0.1136, 0.0250,\n",
      "         0.0064, 0.0198, 0.1138, 0.0847, 0.1734],\n",
      "        [0.1012, 0.0174, 0.0162, 0.0796, 0.2186, 0.0580, 0.0086, 0.0668, 0.0198,\n",
      "         0.0228, 0.0143, 0.1682, 0.1166, 0.0921],\n",
      "        [0.0455, 0.0360, 0.0208, 0.1149, 0.0780, 0.0292, 0.0062, 0.2342, 0.0366,\n",
      "         0.0181, 0.0137, 0.1584, 0.0618, 0.1466],\n",
      "        [0.0631, 0.0259, 0.0209, 0.1130, 0.1235, 0.0155, 0.0146, 0.3392, 0.0132,\n",
      "         0.0105, 0.0330, 0.0690, 0.1093, 0.0491],\n",
      "        [0.0706, 0.0226, 0.0348, 0.1117, 0.0887, 0.0344, 0.0076, 0.1175, 0.0306,\n",
      "         0.0069, 0.0340, 0.1253, 0.0931, 0.2221],\n",
      "        [0.0540, 0.0185, 0.0374, 0.1100, 0.1162, 0.0544, 0.0062, 0.2624, 0.0299,\n",
      "         0.0109, 0.0210, 0.1286, 0.0761, 0.0744],\n",
      "        [0.0753, 0.0358, 0.0324, 0.1068, 0.1064, 0.0200, 0.0215, 0.1325, 0.0272,\n",
      "         0.0045, 0.0231, 0.2141, 0.0845, 0.1159],\n",
      "        [0.1002, 0.0208, 0.0371, 0.0934, 0.1747, 0.0422, 0.0062, 0.1400, 0.0409,\n",
      "         0.0126, 0.0142, 0.1097, 0.1344, 0.0737],\n",
      "        [0.0801, 0.0450, 0.0165, 0.1215, 0.0891, 0.0356, 0.0055, 0.0934, 0.0251,\n",
      "         0.0180, 0.0192, 0.1415, 0.1303, 0.1792],\n",
      "        [0.0615, 0.0434, 0.0136, 0.0583, 0.1446, 0.0517, 0.0139, 0.0696, 0.0211,\n",
      "         0.0077, 0.0207, 0.1417, 0.0368, 0.3154]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 28 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.8807, 0.1193]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 29 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.5000, 0.5000],\n",
      "        [0.8808, 0.1192]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 30 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 31 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 32 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.0226, 0.1740, 0.0230, 0.0854, 0.1202, 0.1205, 0.4543],\n",
      "        [0.0371, 0.0807, 0.0198, 0.0585, 0.0678, 0.0990, 0.6370],\n",
      "        [0.0768, 0.0816, 0.0144, 0.4812, 0.0536, 0.1151, 0.1773],\n",
      "        [0.0234, 0.0474, 0.0334, 0.4909, 0.1603, 0.1800, 0.0645],\n",
      "        [0.0214, 0.0683, 0.0436, 0.0797, 0.0430, 0.0714, 0.6727],\n",
      "        [0.0351, 0.1722, 0.0232, 0.4397, 0.0358, 0.1392, 0.1548],\n",
      "        [0.0392, 0.0646, 0.0171, 0.1718, 0.1104, 0.0893, 0.5075]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 33 - Output:\n",
      "torch.Size([12, 12])\n",
      "1\n",
      "tensor([[0.0282, 0.1125, 0.1344, 0.0307, 0.0284, 0.0262, 0.0340, 0.0221, 0.0083,\n",
      "         0.0965, 0.3817, 0.0970],\n",
      "        [0.0461, 0.0816, 0.1005, 0.0139, 0.0221, 0.0273, 0.0229, 0.0428, 0.0132,\n",
      "         0.2168, 0.1842, 0.2287],\n",
      "        [0.0481, 0.0892, 0.0520, 0.0323, 0.0183, 0.0139, 0.0205, 0.0221, 0.0177,\n",
      "         0.1342, 0.4453, 0.1063],\n",
      "        [0.0492, 0.1331, 0.0873, 0.0069, 0.0209, 0.0311, 0.0771, 0.0195, 0.0363,\n",
      "         0.3235, 0.1171, 0.0979],\n",
      "        [0.0311, 0.0994, 0.0482, 0.0201, 0.0493, 0.0236, 0.0115, 0.0399, 0.0145,\n",
      "         0.4085, 0.1642, 0.0897],\n",
      "        [0.0379, 0.1404, 0.1137, 0.0247, 0.0268, 0.0299, 0.0607, 0.0149, 0.0098,\n",
      "         0.1368, 0.2331, 0.1715],\n",
      "        [0.0224, 0.1346, 0.1416, 0.0124, 0.0260, 0.0265, 0.0320, 0.0293, 0.0190,\n",
      "         0.2591, 0.1597, 0.1375],\n",
      "        [0.1444, 0.0476, 0.1507, 0.0105, 0.0155, 0.0705, 0.0374, 0.0517, 0.0126,\n",
      "         0.2218, 0.0760, 0.1612],\n",
      "        [0.0206, 0.0340, 0.0930, 0.0460, 0.0203, 0.0274, 0.0133, 0.0198, 0.0191,\n",
      "         0.1063, 0.5260, 0.0743],\n",
      "        [0.0563, 0.1571, 0.0348, 0.0148, 0.0134, 0.0272, 0.0197, 0.0193, 0.0581,\n",
      "         0.0654, 0.4405, 0.0934],\n",
      "        [0.0270, 0.0904, 0.1824, 0.0503, 0.0365, 0.0662, 0.0160, 0.0083, 0.0229,\n",
      "         0.1223, 0.1707, 0.2071],\n",
      "        [0.0223, 0.1716, 0.1370, 0.0162, 0.0156, 0.0210, 0.0635, 0.0226, 0.0247,\n",
      "         0.3316, 0.0861, 0.0879]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 34 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.0384, 0.0072, 0.0201, 0.0120, 0.5142, 0.0231, 0.0580, 0.0118, 0.0133,\n",
      "         0.0131, 0.0927, 0.0213, 0.0236, 0.0072, 0.0179, 0.0238, 0.0444, 0.0580],\n",
      "        [0.0679, 0.0058, 0.0251, 0.0097, 0.1994, 0.0382, 0.0665, 0.0205, 0.0186,\n",
      "         0.0133, 0.2280, 0.0294, 0.0344, 0.0082, 0.0256, 0.0319, 0.0550, 0.1227],\n",
      "        [0.1796, 0.0042, 0.0186, 0.0202, 0.2253, 0.0217, 0.0767, 0.0186, 0.0148,\n",
      "         0.0163, 0.1597, 0.0303, 0.0371, 0.0134, 0.0190, 0.0310, 0.0421, 0.0713],\n",
      "        [0.0998, 0.0065, 0.0177, 0.0046, 0.2022, 0.0497, 0.0508, 0.0242, 0.0160,\n",
      "         0.0165, 0.1415, 0.0340, 0.0492, 0.0170, 0.0276, 0.0443, 0.0713, 0.1269],\n",
      "        [0.1189, 0.0077, 0.0215, 0.0071, 0.0817, 0.0280, 0.0954, 0.0218, 0.0295,\n",
      "         0.0113, 0.2548, 0.0359, 0.0459, 0.0071, 0.0312, 0.0350, 0.0536, 0.1137],\n",
      "        [0.0543, 0.0039, 0.0218, 0.0106, 0.2060, 0.0304, 0.0695, 0.0311, 0.0296,\n",
      "         0.0102, 0.2235, 0.0243, 0.0330, 0.0131, 0.0358, 0.0360, 0.0586, 0.1085],\n",
      "        [0.1409, 0.0055, 0.0156, 0.0155, 0.1261, 0.0375, 0.0649, 0.0210, 0.0183,\n",
      "         0.0089, 0.2114, 0.0232, 0.0329, 0.0128, 0.0309, 0.0302, 0.0700, 0.1344],\n",
      "        [0.0707, 0.0107, 0.0162, 0.0150, 0.2535, 0.0366, 0.0385, 0.0169, 0.0184,\n",
      "         0.0069, 0.1922, 0.0278, 0.0404, 0.0088, 0.0179, 0.0404, 0.0468, 0.1425],\n",
      "        [0.0703, 0.0060, 0.0149, 0.0072, 0.1900, 0.0315, 0.0582, 0.0166, 0.0191,\n",
      "         0.0200, 0.3192, 0.0292, 0.0295, 0.0126, 0.0221, 0.0266, 0.0478, 0.0792],\n",
      "        [0.0634, 0.0039, 0.0137, 0.0136, 0.3001, 0.0292, 0.0380, 0.0202, 0.0137,\n",
      "         0.0137, 0.2513, 0.0276, 0.0371, 0.0248, 0.0219, 0.0271, 0.0424, 0.0582],\n",
      "        [0.0610, 0.0080, 0.0103, 0.0108, 0.2390, 0.0291, 0.0500, 0.0188, 0.0175,\n",
      "         0.0096, 0.3025, 0.0217, 0.0324, 0.0155, 0.0205, 0.0302, 0.0397, 0.0834],\n",
      "        [0.0903, 0.0039, 0.0236, 0.0143, 0.0662, 0.0329, 0.0869, 0.0215, 0.0227,\n",
      "         0.0112, 0.2510, 0.0325, 0.0453, 0.0096, 0.0268, 0.0437, 0.0513, 0.1664],\n",
      "        [0.0545, 0.0076, 0.0140, 0.0173, 0.3207, 0.0242, 0.0518, 0.0181, 0.0132,\n",
      "         0.0100, 0.1787, 0.0249, 0.0359, 0.0100, 0.0187, 0.0334, 0.0533, 0.1136],\n",
      "        [0.0740, 0.0097, 0.0138, 0.0216, 0.2960, 0.0386, 0.0561, 0.0125, 0.0108,\n",
      "         0.0210, 0.1969, 0.0218, 0.0311, 0.0061, 0.0211, 0.0362, 0.0544, 0.0785],\n",
      "        [0.1395, 0.0089, 0.0128, 0.0073, 0.1146, 0.0272, 0.0541, 0.0172, 0.0186,\n",
      "         0.0115, 0.2041, 0.0265, 0.0364, 0.0165, 0.0262, 0.0310, 0.0712, 0.1764],\n",
      "        [0.0954, 0.0098, 0.0201, 0.0081, 0.2950, 0.0229, 0.0578, 0.0190, 0.0177,\n",
      "         0.0121, 0.1700, 0.0177, 0.0365, 0.0111, 0.0209, 0.0228, 0.0576, 0.1057],\n",
      "        [0.1062, 0.0146, 0.0231, 0.0086, 0.2064, 0.0240, 0.0310, 0.0193, 0.0136,\n",
      "         0.0104, 0.2703, 0.0267, 0.0325, 0.0103, 0.0138, 0.0310, 0.0511, 0.1076],\n",
      "        [0.0829, 0.0059, 0.0132, 0.0114, 0.3316, 0.0278, 0.0525, 0.0186, 0.0125,\n",
      "         0.0155, 0.1547, 0.0217, 0.0372, 0.0152, 0.0180, 0.0328, 0.0613, 0.0872]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 35 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.0365, 0.0434, 0.0330, 0.1172, 0.0311, 0.0360, 0.0583, 0.0921, 0.0067,\n",
      "         0.0590, 0.0515, 0.4255, 0.0095],\n",
      "        [0.1051, 0.0381, 0.0210, 0.1594, 0.1741, 0.0179, 0.0265, 0.2229, 0.0486,\n",
      "         0.0866, 0.0237, 0.0691, 0.0068],\n",
      "        [0.0601, 0.1065, 0.0330, 0.1345, 0.0567, 0.0514, 0.0258, 0.3915, 0.0083,\n",
      "         0.0093, 0.0297, 0.0627, 0.0304],\n",
      "        [0.0704, 0.0671, 0.0503, 0.0395, 0.0333, 0.0441, 0.0412, 0.1574, 0.0107,\n",
      "         0.0069, 0.0265, 0.4196, 0.0331],\n",
      "        [0.0641, 0.0590, 0.0299, 0.1058, 0.0680, 0.0362, 0.0319, 0.2780, 0.0064,\n",
      "         0.0111, 0.0592, 0.2133, 0.0370],\n",
      "        [0.0364, 0.0415, 0.0200, 0.1875, 0.0347, 0.0379, 0.0467, 0.1806, 0.0225,\n",
      "         0.0245, 0.0213, 0.3377, 0.0086],\n",
      "        [0.0180, 0.0467, 0.0228, 0.0223, 0.0149, 0.0281, 0.0480, 0.2069, 0.0113,\n",
      "         0.0411, 0.0271, 0.4705, 0.0424],\n",
      "        [0.0701, 0.0300, 0.0193, 0.0721, 0.0354, 0.0194, 0.0131, 0.1294, 0.0137,\n",
      "         0.0158, 0.0317, 0.4795, 0.0703],\n",
      "        [0.0371, 0.0485, 0.0534, 0.0394, 0.0854, 0.0311, 0.0287, 0.0581, 0.0126,\n",
      "         0.0060, 0.0567, 0.5204, 0.0226],\n",
      "        [0.1770, 0.0545, 0.0093, 0.0849, 0.0262, 0.0298, 0.1015, 0.0700, 0.0064,\n",
      "         0.0516, 0.1621, 0.0917, 0.1348],\n",
      "        [0.0593, 0.0548, 0.0472, 0.0490, 0.0198, 0.0270, 0.1116, 0.2065, 0.0086,\n",
      "         0.0097, 0.0609, 0.0432, 0.3023],\n",
      "        [0.0452, 0.0707, 0.0241, 0.1044, 0.0335, 0.0198, 0.0129, 0.0881, 0.0119,\n",
      "         0.0216, 0.0211, 0.5201, 0.0265],\n",
      "        [0.0903, 0.0606, 0.0630, 0.1324, 0.0204, 0.0329, 0.0455, 0.2789, 0.0045,\n",
      "         0.0225, 0.0382, 0.1180, 0.0928]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 36 - Output:\n",
      "torch.Size([16, 16])\n",
      "1\n",
      "tensor([[0.2240, 0.0409, 0.0169, 0.0727, 0.0486, 0.0766, 0.0423, 0.0240, 0.0761,\n",
      "         0.0148, 0.0748, 0.0466, 0.0267, 0.1549, 0.0573, 0.0027],\n",
      "        [0.2247, 0.0274, 0.0133, 0.0690, 0.0325, 0.0904, 0.0389, 0.0203, 0.0365,\n",
      "         0.0182, 0.1543, 0.0508, 0.0141, 0.1176, 0.0875, 0.0045],\n",
      "        [0.3485, 0.0320, 0.0181, 0.0792, 0.0272, 0.0778, 0.0426, 0.0215, 0.0420,\n",
      "         0.0139, 0.0602, 0.0613, 0.0138, 0.0639, 0.0944, 0.0037],\n",
      "        [0.0896, 0.0701, 0.0219, 0.0458, 0.0387, 0.1683, 0.0505, 0.0122, 0.0724,\n",
      "         0.0288, 0.0509, 0.1124, 0.0104, 0.0951, 0.1294, 0.0037],\n",
      "        [0.1170, 0.0254, 0.0202, 0.0409, 0.0334, 0.0795, 0.0439, 0.0181, 0.0524,\n",
      "         0.0182, 0.0480, 0.0546, 0.0225, 0.3778, 0.0450, 0.0029],\n",
      "        [0.2991, 0.0342, 0.0168, 0.0343, 0.0322, 0.0682, 0.0453, 0.0125, 0.0497,\n",
      "         0.0118, 0.0594, 0.0605, 0.0206, 0.2071, 0.0437, 0.0048],\n",
      "        [0.2953, 0.0398, 0.0229, 0.0441, 0.0231, 0.0422, 0.0323, 0.0165, 0.0757,\n",
      "         0.0228, 0.0429, 0.0891, 0.0310, 0.1642, 0.0554, 0.0028],\n",
      "        [0.2464, 0.0502, 0.0168, 0.0443, 0.0147, 0.0783, 0.0248, 0.0199, 0.0611,\n",
      "         0.0073, 0.0842, 0.0699, 0.0153, 0.2182, 0.0388, 0.0097],\n",
      "        [0.2135, 0.0523, 0.0179, 0.0494, 0.0347, 0.0874, 0.0396, 0.0280, 0.0568,\n",
      "         0.0320, 0.1112, 0.0745, 0.0138, 0.0843, 0.1021, 0.0025],\n",
      "        [0.3189, 0.0572, 0.0237, 0.0430, 0.0380, 0.0679, 0.0426, 0.0154, 0.0494,\n",
      "         0.0170, 0.0336, 0.0685, 0.0151, 0.1265, 0.0800, 0.0033],\n",
      "        [0.3620, 0.0472, 0.0159, 0.0222, 0.0292, 0.0580, 0.0323, 0.0134, 0.0218,\n",
      "         0.0196, 0.0992, 0.0469, 0.0106, 0.1813, 0.0332, 0.0071],\n",
      "        [0.3004, 0.0382, 0.0184, 0.0342, 0.0259, 0.1028, 0.0275, 0.0164, 0.0288,\n",
      "         0.0156, 0.1202, 0.0452, 0.0160, 0.1700, 0.0353, 0.0051],\n",
      "        [0.3803, 0.0277, 0.0098, 0.0620, 0.0231, 0.0765, 0.0201, 0.0163, 0.0356,\n",
      "         0.0192, 0.1242, 0.0334, 0.0109, 0.0971, 0.0552, 0.0083],\n",
      "        [0.1129, 0.0367, 0.0186, 0.0578, 0.0336, 0.0914, 0.0553, 0.0270, 0.0645,\n",
      "         0.0224, 0.0689, 0.0876, 0.0190, 0.2284, 0.0735, 0.0024],\n",
      "        [0.1527, 0.0500, 0.0176, 0.0646, 0.0294, 0.0647, 0.0472, 0.0202, 0.0517,\n",
      "         0.0204, 0.1777, 0.0543, 0.0183, 0.1900, 0.0380, 0.0031],\n",
      "        [0.2473, 0.0281, 0.0193, 0.0321, 0.0241, 0.0660, 0.0298, 0.0200, 0.0355,\n",
      "         0.0149, 0.0730, 0.0540, 0.0198, 0.2893, 0.0425, 0.0041]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 37 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.1124, 0.1788, 0.0778, 0.6021, 0.0289],\n",
      "        [0.0783, 0.0942, 0.0688, 0.7174, 0.0413],\n",
      "        [0.3109, 0.1107, 0.0225, 0.2333, 0.3226],\n",
      "        [0.0524, 0.0420, 0.0793, 0.2273, 0.5990],\n",
      "        [0.2813, 0.2330, 0.0300, 0.3978, 0.0580]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 38 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.0264, 0.0213, 0.2351, 0.2458, 0.2529, 0.0624, 0.1563],\n",
      "        [0.0497, 0.0146, 0.2269, 0.3166, 0.0984, 0.0680, 0.2257],\n",
      "        [0.0326, 0.0336, 0.3002, 0.1417, 0.1762, 0.0262, 0.2893],\n",
      "        [0.0582, 0.0184, 0.0838, 0.2067, 0.5178, 0.0444, 0.0708],\n",
      "        [0.0196, 0.0305, 0.2155, 0.0943, 0.4656, 0.0757, 0.0987],\n",
      "        [0.0602, 0.0113, 0.1658, 0.1413, 0.2601, 0.1550, 0.2063],\n",
      "        [0.0260, 0.0379, 0.0569, 0.3493, 0.2101, 0.0404, 0.2794]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 39 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0994, 0.0334, 0.0096, 0.1057, 0.0167, 0.1432, 0.0199, 0.0095, 0.0282,\n",
      "         0.0496, 0.0144, 0.1505, 0.1985, 0.0903, 0.0310],\n",
      "        [0.1303, 0.0243, 0.0198, 0.2071, 0.0292, 0.1545, 0.0206, 0.0042, 0.0345,\n",
      "         0.0234, 0.0246, 0.1014, 0.1133, 0.0721, 0.0407],\n",
      "        [0.0970, 0.0122, 0.0210, 0.0757, 0.0206, 0.1666, 0.0232, 0.0075, 0.0158,\n",
      "         0.0316, 0.0315, 0.1028, 0.2303, 0.1324, 0.0319],\n",
      "        [0.1330, 0.0160, 0.0132, 0.1384, 0.0280, 0.2144, 0.0241, 0.0061, 0.0199,\n",
      "         0.0481, 0.0317, 0.1392, 0.0572, 0.1025, 0.0281],\n",
      "        [0.1669, 0.0158, 0.0151, 0.0959, 0.0301, 0.1256, 0.0183, 0.0056, 0.0351,\n",
      "         0.0326, 0.0271, 0.2621, 0.0420, 0.0935, 0.0342],\n",
      "        [0.1241, 0.0225, 0.0100, 0.0713, 0.0307, 0.2589, 0.0206, 0.0094, 0.0304,\n",
      "         0.0163, 0.0232, 0.1843, 0.1120, 0.0624, 0.0237],\n",
      "        [0.1534, 0.0287, 0.0123, 0.0800, 0.0329, 0.1028, 0.0297, 0.0051, 0.0229,\n",
      "         0.0451, 0.0173, 0.2029, 0.1605, 0.0744, 0.0318],\n",
      "        [0.0848, 0.0149, 0.0084, 0.1147, 0.0278, 0.2044, 0.0250, 0.0103, 0.0223,\n",
      "         0.0356, 0.0203, 0.1563, 0.1642, 0.0739, 0.0373],\n",
      "        [0.1858, 0.0256, 0.0171, 0.0388, 0.0257, 0.0807, 0.0346, 0.0042, 0.0202,\n",
      "         0.0409, 0.0293, 0.2406, 0.1384, 0.0861, 0.0319],\n",
      "        [0.1110, 0.0202, 0.0207, 0.0734, 0.0322, 0.1615, 0.0283, 0.0031, 0.0526,\n",
      "         0.0381, 0.0307, 0.0798, 0.2440, 0.0671, 0.0373],\n",
      "        [0.2229, 0.0210, 0.0078, 0.0458, 0.0308, 0.1047, 0.0215, 0.0170, 0.0167,\n",
      "         0.0178, 0.0193, 0.0964, 0.2619, 0.0835, 0.0327],\n",
      "        [0.4072, 0.0102, 0.0147, 0.1093, 0.0215, 0.1297, 0.0136, 0.0149, 0.0150,\n",
      "         0.0436, 0.0219, 0.0388, 0.0465, 0.0826, 0.0305],\n",
      "        [0.4591, 0.0114, 0.0156, 0.0547, 0.0237, 0.1227, 0.0184, 0.0129, 0.0197,\n",
      "         0.0174, 0.0157, 0.0878, 0.0824, 0.0270, 0.0314],\n",
      "        [0.2078, 0.0192, 0.0152, 0.0638, 0.0221, 0.1333, 0.0251, 0.0062, 0.0235,\n",
      "         0.0259, 0.0232, 0.1033, 0.0849, 0.2142, 0.0324],\n",
      "        [0.3159, 0.0131, 0.0114, 0.0485, 0.0233, 0.1166, 0.0462, 0.0074, 0.0176,\n",
      "         0.0379, 0.0239, 0.1300, 0.0712, 0.1027, 0.0342]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 40 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.1548, 0.0151, 0.0208, 0.1635, 0.2776, 0.0879, 0.0734, 0.0467, 0.0474,\n",
      "         0.0141, 0.0661, 0.0108, 0.0216],\n",
      "        [0.1448, 0.0116, 0.0422, 0.4129, 0.0975, 0.0655, 0.0395, 0.0447, 0.0399,\n",
      "         0.0111, 0.0578, 0.0172, 0.0154],\n",
      "        [0.1359, 0.0382, 0.0502, 0.3524, 0.0841, 0.1213, 0.0325, 0.0374, 0.0089,\n",
      "         0.0222, 0.0790, 0.0098, 0.0279],\n",
      "        [0.2118, 0.0084, 0.0680, 0.2290, 0.1480, 0.0679, 0.0486, 0.0481, 0.0284,\n",
      "         0.0137, 0.0875, 0.0165, 0.0241],\n",
      "        [0.1649, 0.0108, 0.0417, 0.2687, 0.1310, 0.0778, 0.0502, 0.1021, 0.0159,\n",
      "         0.0222, 0.0580, 0.0095, 0.0471],\n",
      "        [0.1299, 0.0107, 0.0608, 0.3333, 0.1347, 0.0854, 0.0433, 0.0558, 0.0344,\n",
      "         0.0095, 0.0639, 0.0190, 0.0194],\n",
      "        [0.1492, 0.0176, 0.0097, 0.3992, 0.0788, 0.0985, 0.0175, 0.0738, 0.0225,\n",
      "         0.0281, 0.0564, 0.0236, 0.0251],\n",
      "        [0.1787, 0.0106, 0.0445, 0.1383, 0.1590, 0.1217, 0.0358, 0.1109, 0.0199,\n",
      "         0.0066, 0.0623, 0.0518, 0.0599],\n",
      "        [0.1524, 0.0150, 0.0235, 0.2855, 0.1267, 0.0647, 0.0378, 0.0955, 0.0382,\n",
      "         0.0098, 0.0866, 0.0112, 0.0531],\n",
      "        [0.1571, 0.0269, 0.0431, 0.2798, 0.1386, 0.1350, 0.0371, 0.0573, 0.0180,\n",
      "         0.0087, 0.0563, 0.0131, 0.0290],\n",
      "        [0.1469, 0.0149, 0.0397, 0.3197, 0.2048, 0.0423, 0.0384, 0.0597, 0.0433,\n",
      "         0.0279, 0.0350, 0.0077, 0.0196],\n",
      "        [0.3108, 0.0099, 0.0238, 0.1550, 0.1408, 0.0598, 0.1065, 0.0448, 0.0368,\n",
      "         0.0217, 0.0534, 0.0259, 0.0109],\n",
      "        [0.0824, 0.0098, 0.0362, 0.2505, 0.1981, 0.1326, 0.0544, 0.0654, 0.0440,\n",
      "         0.0241, 0.0702, 0.0240, 0.0082]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 41 - Output:\n",
      "torch.Size([12, 12])\n",
      "1\n",
      "tensor([[0.1997, 0.1197, 0.0992, 0.0103, 0.0711, 0.0172, 0.1023, 0.1093, 0.0121,\n",
      "         0.0264, 0.0503, 0.1825],\n",
      "        [0.1452, 0.3127, 0.1432, 0.0090, 0.0512, 0.0558, 0.0608, 0.0329, 0.0329,\n",
      "         0.0124, 0.0272, 0.1169],\n",
      "        [0.1126, 0.0604, 0.5322, 0.0160, 0.0321, 0.0310, 0.0545, 0.0293, 0.0264,\n",
      "         0.0078, 0.0403, 0.0575],\n",
      "        [0.0650, 0.2401, 0.3826, 0.0281, 0.0734, 0.0146, 0.0567, 0.0440, 0.0152,\n",
      "         0.0150, 0.0266, 0.0388],\n",
      "        [0.1531, 0.1387, 0.1698, 0.0076, 0.0968, 0.0344, 0.1898, 0.0470, 0.0240,\n",
      "         0.0139, 0.0421, 0.0828],\n",
      "        [0.1615, 0.3307, 0.0526, 0.0289, 0.0441, 0.0381, 0.0593, 0.1582, 0.0399,\n",
      "         0.0051, 0.0396, 0.0420],\n",
      "        [0.1065, 0.1075, 0.3875, 0.0128, 0.0645, 0.0365, 0.0375, 0.0551, 0.0078,\n",
      "         0.0247, 0.0535, 0.1060],\n",
      "        [0.1205, 0.0735, 0.4161, 0.0112, 0.0368, 0.0420, 0.1024, 0.0731, 0.0137,\n",
      "         0.0149, 0.0286, 0.0671],\n",
      "        [0.1474, 0.1814, 0.2352, 0.0054, 0.0566, 0.0473, 0.1121, 0.0684, 0.0353,\n",
      "         0.0224, 0.0277, 0.0607],\n",
      "        [0.1321, 0.0940, 0.3353, 0.0111, 0.0427, 0.0347, 0.1953, 0.0500, 0.0157,\n",
      "         0.0169, 0.0399, 0.0323],\n",
      "        [0.0685, 0.0419, 0.5624, 0.0158, 0.0396, 0.0242, 0.0650, 0.0333, 0.0311,\n",
      "         0.0071, 0.0408, 0.0705],\n",
      "        [0.0704, 0.0907, 0.4261, 0.0245, 0.0667, 0.0279, 0.0972, 0.0769, 0.0203,\n",
      "         0.0068, 0.0245, 0.0680]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 42 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.1192, 0.8808],\n",
      "        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 43 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.0109, 0.0494, 0.0208, 0.0029, 0.0493, 0.0428, 0.1354, 0.0213, 0.0947,\n",
      "         0.1179, 0.0142, 0.0481, 0.0162, 0.0515, 0.0989, 0.0210, 0.0382, 0.1666],\n",
      "        [0.0094, 0.0492, 0.0173, 0.0037, 0.0098, 0.0776, 0.1339, 0.0264, 0.1160,\n",
      "         0.0841, 0.0414, 0.0394, 0.0117, 0.0863, 0.0884, 0.0393, 0.0914, 0.0748],\n",
      "        [0.0063, 0.0855, 0.0131, 0.0176, 0.0111, 0.0513, 0.1730, 0.0188, 0.1122,\n",
      "         0.0283, 0.0128, 0.1473, 0.0121, 0.0466, 0.0825, 0.0173, 0.0556, 0.1084],\n",
      "        [0.0092, 0.0240, 0.0284, 0.0036, 0.0479, 0.0212, 0.3406, 0.0145, 0.0496,\n",
      "         0.1105, 0.0262, 0.0595, 0.0200, 0.0423, 0.0916, 0.0117, 0.0362, 0.0632],\n",
      "        [0.0071, 0.0528, 0.0125, 0.0088, 0.0145, 0.0302, 0.2387, 0.0163, 0.1153,\n",
      "         0.0375, 0.0173, 0.0938, 0.0124, 0.0489, 0.1158, 0.0216, 0.1199, 0.0367],\n",
      "        [0.0049, 0.0614, 0.0203, 0.0075, 0.0238, 0.0429, 0.2524, 0.0291, 0.1167,\n",
      "         0.0769, 0.0118, 0.1265, 0.0121, 0.0441, 0.0493, 0.0180, 0.0609, 0.0414],\n",
      "        [0.0127, 0.0171, 0.0131, 0.0048, 0.0231, 0.1187, 0.2582, 0.0150, 0.0774,\n",
      "         0.0424, 0.0172, 0.0927, 0.0116, 0.0426, 0.0432, 0.0351, 0.0618, 0.1133],\n",
      "        [0.0094, 0.0266, 0.0141, 0.0111, 0.0192, 0.0556, 0.2548, 0.0154, 0.2161,\n",
      "         0.0592, 0.0214, 0.0515, 0.0052, 0.0241, 0.0416, 0.0294, 0.0533, 0.0920],\n",
      "        [0.0211, 0.0512, 0.0249, 0.0025, 0.0103, 0.0966, 0.1148, 0.0220, 0.1011,\n",
      "         0.0864, 0.0217, 0.0900, 0.0182, 0.0306, 0.0321, 0.0410, 0.1792, 0.0562],\n",
      "        [0.0088, 0.0362, 0.0338, 0.0096, 0.0246, 0.0373, 0.2734, 0.0110, 0.1439,\n",
      "         0.1110, 0.0160, 0.0518, 0.0058, 0.0253, 0.0419, 0.0256, 0.1026, 0.0413],\n",
      "        [0.0362, 0.0459, 0.0219, 0.0035, 0.0299, 0.0371, 0.1530, 0.0402, 0.1316,\n",
      "         0.0610, 0.0167, 0.0614, 0.0056, 0.0303, 0.1115, 0.0181, 0.0999, 0.0962],\n",
      "        [0.0114, 0.0539, 0.0120, 0.0120, 0.0133, 0.0329, 0.2161, 0.0115, 0.0747,\n",
      "         0.0685, 0.0088, 0.0711, 0.0130, 0.0819, 0.1021, 0.0198, 0.0553, 0.1416],\n",
      "        [0.0098, 0.0190, 0.0169, 0.0101, 0.0140, 0.0484, 0.3389, 0.0104, 0.0860,\n",
      "         0.0665, 0.0169, 0.0525, 0.0076, 0.0815, 0.0337, 0.0261, 0.0732, 0.0884],\n",
      "        [0.0221, 0.0415, 0.0274, 0.0029, 0.0232, 0.0306, 0.2360, 0.0277, 0.1105,\n",
      "         0.0806, 0.0103, 0.0669, 0.0089, 0.0489, 0.0859, 0.0397, 0.0679, 0.0692],\n",
      "        [0.0235, 0.0344, 0.0113, 0.0062, 0.0231, 0.0253, 0.2145, 0.0141, 0.1421,\n",
      "         0.1503, 0.0160, 0.0376, 0.0078, 0.0422, 0.0476, 0.0212, 0.1160, 0.0669],\n",
      "        [0.0124, 0.0400, 0.0384, 0.0065, 0.0161, 0.0245, 0.1488, 0.0122, 0.1354,\n",
      "         0.0813, 0.0356, 0.0461, 0.0054, 0.0257, 0.0542, 0.0487, 0.1043, 0.1646],\n",
      "        [0.0152, 0.0349, 0.0223, 0.0061, 0.0260, 0.0269, 0.1108, 0.0209, 0.2367,\n",
      "         0.1105, 0.0068, 0.0326, 0.0101, 0.0424, 0.0446, 0.0227, 0.0614, 0.1692],\n",
      "        [0.0105, 0.0339, 0.0356, 0.0054, 0.0056, 0.0577, 0.1420, 0.0190, 0.1536,\n",
      "         0.0678, 0.0424, 0.1076, 0.0237, 0.0368, 0.0541, 0.0136, 0.1105, 0.0800]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 44 - Output:\n",
      "torch.Size([16, 16])\n",
      "1\n",
      "tensor([[0.0274, 0.0311, 0.0264, 0.0480, 0.0174, 0.4145, 0.0442, 0.0487, 0.0689,\n",
      "         0.0360, 0.0385, 0.0213, 0.0027, 0.0435, 0.1115, 0.0198],\n",
      "        [0.0166, 0.0312, 0.0256, 0.0799, 0.0269, 0.4016, 0.0250, 0.0266, 0.1294,\n",
      "         0.0285, 0.0582, 0.0183, 0.0035, 0.0429, 0.0646, 0.0213],\n",
      "        [0.0202, 0.0359, 0.0254, 0.1071, 0.0168, 0.4026, 0.0299, 0.0473, 0.0846,\n",
      "         0.0275, 0.0633, 0.0117, 0.0047, 0.0382, 0.0704, 0.0144],\n",
      "        [0.0150, 0.0216, 0.0172, 0.0742, 0.0171, 0.4062, 0.1026, 0.0256, 0.0908,\n",
      "         0.0245, 0.0648, 0.0231, 0.0062, 0.0357, 0.0638, 0.0115],\n",
      "        [0.0173, 0.0287, 0.0323, 0.0728, 0.0320, 0.3441, 0.0560, 0.0317, 0.1527,\n",
      "         0.0325, 0.0386, 0.0217, 0.0030, 0.0385, 0.0798, 0.0183],\n",
      "        [0.0135, 0.0221, 0.0141, 0.0303, 0.0122, 0.3677, 0.0532, 0.0286, 0.2141,\n",
      "         0.0253, 0.0364, 0.0236, 0.0081, 0.0352, 0.0980, 0.0175],\n",
      "        [0.0177, 0.0239, 0.0210, 0.0997, 0.0278, 0.3137, 0.0531, 0.0372, 0.1379,\n",
      "         0.0282, 0.0487, 0.0186, 0.0045, 0.0465, 0.1087, 0.0126],\n",
      "        [0.0172, 0.0279, 0.0222, 0.0513, 0.0157, 0.2992, 0.0255, 0.0352, 0.1557,\n",
      "         0.0246, 0.0889, 0.0178, 0.0086, 0.0373, 0.1635, 0.0095],\n",
      "        [0.0305, 0.0404, 0.0221, 0.0600, 0.0235, 0.3060, 0.0300, 0.0658, 0.1325,\n",
      "         0.0425, 0.0911, 0.0170, 0.0030, 0.0583, 0.0600, 0.0174],\n",
      "        [0.0307, 0.0414, 0.0397, 0.2534, 0.0242, 0.0726, 0.0480, 0.0341, 0.1827,\n",
      "         0.0270, 0.1054, 0.0113, 0.0034, 0.0479, 0.0588, 0.0195],\n",
      "        [0.0111, 0.0225, 0.0209, 0.0661, 0.0227, 0.3819, 0.0655, 0.0482, 0.1046,\n",
      "         0.0226, 0.0603, 0.0192, 0.0047, 0.0323, 0.0974, 0.0201],\n",
      "        [0.0242, 0.0194, 0.0183, 0.0705, 0.0108, 0.3730, 0.0282, 0.0373, 0.1626,\n",
      "         0.0248, 0.0809, 0.0199, 0.0057, 0.0510, 0.0551, 0.0183],\n",
      "        [0.0146, 0.0266, 0.0145, 0.0637, 0.0217, 0.3082, 0.0425, 0.0397, 0.2592,\n",
      "         0.0276, 0.0483, 0.0197, 0.0053, 0.0330, 0.0596, 0.0157],\n",
      "        [0.0235, 0.0355, 0.0330, 0.0810, 0.0183, 0.2154, 0.0479, 0.0554, 0.1178,\n",
      "         0.0287, 0.0890, 0.0295, 0.0027, 0.0555, 0.1480, 0.0188],\n",
      "        [0.0156, 0.0390, 0.0241, 0.0716, 0.0179, 0.4227, 0.0473, 0.0358, 0.0571,\n",
      "         0.0313, 0.0591, 0.0152, 0.0035, 0.0399, 0.0994, 0.0205],\n",
      "        [0.0176, 0.0363, 0.0417, 0.1318, 0.0282, 0.2140, 0.0586, 0.0781, 0.0645,\n",
      "         0.0404, 0.1128, 0.0151, 0.0025, 0.0510, 0.0740, 0.0334]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 45 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 46 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0274, 0.2896, 0.0088, 0.0322, 0.0419, 0.0736, 0.0478, 0.2915, 0.0390,\n",
      "         0.0363, 0.0268, 0.0102, 0.0605, 0.0145],\n",
      "        [0.0152, 0.5413, 0.0178, 0.0327, 0.0287, 0.0805, 0.0378, 0.1195, 0.0299,\n",
      "         0.0180, 0.0162, 0.0171, 0.0352, 0.0101],\n",
      "        [0.0234, 0.5778, 0.0107, 0.0317, 0.0264, 0.0743, 0.0352, 0.0870, 0.0284,\n",
      "         0.0229, 0.0188, 0.0149, 0.0393, 0.0092],\n",
      "        [0.0189, 0.2817, 0.0108, 0.0242, 0.0329, 0.1509, 0.0512, 0.2502, 0.0359,\n",
      "         0.0390, 0.0302, 0.0158, 0.0462, 0.0119],\n",
      "        [0.0241, 0.4384, 0.0188, 0.0304, 0.0300, 0.0740, 0.0487, 0.1507, 0.0438,\n",
      "         0.0361, 0.0296, 0.0086, 0.0587, 0.0082],\n",
      "        [0.0170, 0.4178, 0.0115, 0.0280, 0.0248, 0.1101, 0.0651, 0.1720, 0.0454,\n",
      "         0.0258, 0.0175, 0.0157, 0.0316, 0.0178],\n",
      "        [0.0274, 0.4656, 0.0114, 0.0311, 0.0367, 0.1005, 0.0436, 0.1298, 0.0307,\n",
      "         0.0261, 0.0167, 0.0108, 0.0560, 0.0136],\n",
      "        [0.0155, 0.5966, 0.0141, 0.0278, 0.0224, 0.0637, 0.0221, 0.0933, 0.0351,\n",
      "         0.0209, 0.0218, 0.0147, 0.0421, 0.0102],\n",
      "        [0.0229, 0.5544, 0.0197, 0.0356, 0.0242, 0.0809, 0.0320, 0.0983, 0.0392,\n",
      "         0.0191, 0.0185, 0.0099, 0.0352, 0.0100],\n",
      "        [0.0422, 0.2350, 0.0169, 0.0344, 0.0288, 0.1642, 0.0459, 0.2087, 0.0454,\n",
      "         0.0315, 0.0147, 0.0113, 0.1098, 0.0112],\n",
      "        [0.0108, 0.4365, 0.0097, 0.0266, 0.0360, 0.1299, 0.0351, 0.1165, 0.0355,\n",
      "         0.0361, 0.0316, 0.0284, 0.0557, 0.0115],\n",
      "        [0.0258, 0.4942, 0.0114, 0.0363, 0.0304, 0.0914, 0.0289, 0.1130, 0.0476,\n",
      "         0.0239, 0.0129, 0.0231, 0.0518, 0.0093],\n",
      "        [0.0234, 0.4099, 0.0063, 0.0298, 0.0233, 0.1926, 0.0447, 0.0691, 0.0340,\n",
      "         0.0252, 0.0302, 0.0178, 0.0724, 0.0211],\n",
      "        [0.0185, 0.3487, 0.0114, 0.0447, 0.0440, 0.0929, 0.0451, 0.1592, 0.0689,\n",
      "         0.0356, 0.0523, 0.0092, 0.0593, 0.0103]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 47 - Output:\n",
      "torch.Size([8, 8])\n",
      "1\n",
      "tensor([[0.0860, 0.0287, 0.2141, 0.0725, 0.4536, 0.0608, 0.0147, 0.0697],\n",
      "        [0.0183, 0.5015, 0.1362, 0.0579, 0.1556, 0.0640, 0.0278, 0.0387],\n",
      "        [0.1048, 0.0703, 0.1168, 0.0390, 0.5260, 0.0365, 0.0136, 0.0929],\n",
      "        [0.1865, 0.0272, 0.3873, 0.0332, 0.2228, 0.0710, 0.0228, 0.0493],\n",
      "        [0.5659, 0.0408, 0.0742, 0.0368, 0.1379, 0.0913, 0.0174, 0.0357],\n",
      "        [0.0516, 0.0166, 0.5652, 0.0345, 0.1555, 0.0742, 0.0401, 0.0624],\n",
      "        [0.5149, 0.0856, 0.0869, 0.0504, 0.1517, 0.0373, 0.0130, 0.0600],\n",
      "        [0.2619, 0.2684, 0.1344, 0.0343, 0.1909, 0.0457, 0.0148, 0.0496]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 48 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0094, 0.0395, 0.0305, 0.0366, 0.0212, 0.0363, 0.0408, 0.0161, 0.0076,\n",
      "         0.0477, 0.1798, 0.0585, 0.0220, 0.2860, 0.1682],\n",
      "        [0.0147, 0.0218, 0.0117, 0.0238, 0.0399, 0.0207, 0.0166, 0.0089, 0.0843,\n",
      "         0.0854, 0.2192, 0.0827, 0.0570, 0.0937, 0.2198],\n",
      "        [0.0139, 0.0219, 0.0287, 0.0459, 0.0231, 0.0286, 0.0866, 0.0047, 0.0574,\n",
      "         0.0233, 0.2388, 0.0324, 0.0680, 0.0538, 0.2728],\n",
      "        [0.0275, 0.0076, 0.0514, 0.0192, 0.0254, 0.0298, 0.0156, 0.0108, 0.1554,\n",
      "         0.0318, 0.1317, 0.0723, 0.0319, 0.2769, 0.1126],\n",
      "        [0.0106, 0.0815, 0.0379, 0.0131, 0.0238, 0.0345, 0.0851, 0.0060, 0.0398,\n",
      "         0.0227, 0.0931, 0.0476, 0.1142, 0.2726, 0.1177],\n",
      "        [0.0161, 0.0472, 0.0497, 0.0133, 0.0218, 0.0116, 0.0197, 0.0106, 0.0419,\n",
      "         0.0634, 0.2281, 0.0289, 0.0316, 0.1361, 0.2800],\n",
      "        [0.0176, 0.0643, 0.0133, 0.0093, 0.0246, 0.0363, 0.0344, 0.0250, 0.0457,\n",
      "         0.0107, 0.1916, 0.0374, 0.0347, 0.1999, 0.2552],\n",
      "        [0.0050, 0.0233, 0.0481, 0.0306, 0.0283, 0.0322, 0.0226, 0.0143, 0.0244,\n",
      "         0.0299, 0.3783, 0.0412, 0.0294, 0.1926, 0.1000],\n",
      "        [0.0051, 0.0395, 0.0488, 0.0227, 0.0314, 0.0329, 0.0488, 0.0109, 0.0127,\n",
      "         0.0330, 0.0918, 0.0280, 0.0688, 0.0565, 0.4691],\n",
      "        [0.0158, 0.0360, 0.0753, 0.0225, 0.0121, 0.0196, 0.0621, 0.0165, 0.0083,\n",
      "         0.0297, 0.2711, 0.0364, 0.0359, 0.0752, 0.2837],\n",
      "        [0.0228, 0.0549, 0.0229, 0.0576, 0.0280, 0.0142, 0.0190, 0.0061, 0.0193,\n",
      "         0.0232, 0.2590, 0.0431, 0.0396, 0.0482, 0.3421],\n",
      "        [0.0075, 0.0369, 0.0139, 0.0403, 0.0333, 0.0349, 0.0206, 0.0089, 0.0275,\n",
      "         0.0770, 0.3110, 0.0384, 0.0513, 0.1030, 0.1955],\n",
      "        [0.0075, 0.0236, 0.0218, 0.0420, 0.0136, 0.0456, 0.0591, 0.0185, 0.0233,\n",
      "         0.0160, 0.1752, 0.0408, 0.0326, 0.1690, 0.3112],\n",
      "        [0.0235, 0.0284, 0.0700, 0.0918, 0.0227, 0.0320, 0.0760, 0.0044, 0.0214,\n",
      "         0.0176, 0.0855, 0.0294, 0.0339, 0.2761, 0.1874],\n",
      "        [0.0120, 0.0135, 0.0386, 0.0261, 0.0154, 0.0296, 0.0078, 0.0410, 0.0186,\n",
      "         0.0394, 0.4540, 0.0377, 0.0371, 0.1856, 0.0437]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 49 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.0902, 0.0268, 0.0432, 0.1740, 0.5871, 0.0787],\n",
      "        [0.3288, 0.0900, 0.0153, 0.1720, 0.2220, 0.1719],\n",
      "        [0.0527, 0.5929, 0.0287, 0.1957, 0.0695, 0.0606],\n",
      "        [0.1593, 0.1866, 0.0140, 0.2586, 0.1417, 0.2399],\n",
      "        [0.3717, 0.0166, 0.0805, 0.1657, 0.1188, 0.2467],\n",
      "        [0.2462, 0.4844, 0.0371, 0.0837, 0.0278, 0.1207]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 50 - Output:\n",
      "torch.Size([4, 4])\n",
      "1\n",
      "tensor([[0.0597, 0.0597, 0.5778, 0.3028],\n",
      "        [0.0324, 0.3058, 0.2628, 0.3991],\n",
      "        [0.0776, 0.6925, 0.0503, 0.1796],\n",
      "        [0.0359, 0.1438, 0.3798, 0.4405]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 51 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.0604, 0.0525, 0.0491, 0.1769, 0.6611],\n",
      "        [0.1785, 0.0232, 0.1092, 0.2664, 0.4227],\n",
      "        [0.1152, 0.1040, 0.0256, 0.2118, 0.5434],\n",
      "        [0.1399, 0.0227, 0.1367, 0.3320, 0.3687],\n",
      "        [0.1799, 0.0216, 0.3868, 0.2577, 0.1541]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 52 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.0133, 0.0553, 0.0131, 0.0777, 0.0056, 0.0062, 0.0236, 0.2234, 0.0602,\n",
      "         0.0158, 0.0998, 0.0687, 0.0272, 0.1086, 0.0322, 0.0958, 0.0210, 0.0525],\n",
      "        [0.0139, 0.0610, 0.0187, 0.1117, 0.0023, 0.0182, 0.0256, 0.1536, 0.0733,\n",
      "         0.0146, 0.1263, 0.0949, 0.0444, 0.0501, 0.0386, 0.0871, 0.0240, 0.0417],\n",
      "        [0.0069, 0.0387, 0.0189, 0.1088, 0.0050, 0.0217, 0.0230, 0.3085, 0.0529,\n",
      "         0.0125, 0.0405, 0.0807, 0.0208, 0.0604, 0.0341, 0.1237, 0.0161, 0.0267],\n",
      "        [0.0123, 0.0332, 0.0116, 0.1175, 0.0074, 0.0111, 0.0125, 0.2161, 0.0748,\n",
      "         0.0154, 0.0443, 0.0817, 0.0227, 0.0532, 0.0258, 0.1971, 0.0160, 0.0471],\n",
      "        [0.0076, 0.0353, 0.0255, 0.1444, 0.0033, 0.0254, 0.0362, 0.1520, 0.0502,\n",
      "         0.0095, 0.0635, 0.1263, 0.0337, 0.0697, 0.0315, 0.1179, 0.0327, 0.0353],\n",
      "        [0.0113, 0.0692, 0.0141, 0.1693, 0.0056, 0.0164, 0.0228, 0.1262, 0.0604,\n",
      "         0.0078, 0.0752, 0.0980, 0.0292, 0.0732, 0.0338, 0.1246, 0.0154, 0.0475],\n",
      "        [0.0196, 0.0520, 0.0140, 0.0763, 0.0031, 0.0138, 0.0159, 0.1051, 0.0594,\n",
      "         0.0173, 0.0985, 0.1057, 0.0374, 0.0897, 0.0313, 0.1908, 0.0166, 0.0535],\n",
      "        [0.0086, 0.0611, 0.0330, 0.1569, 0.0071, 0.0178, 0.0233, 0.0800, 0.0655,\n",
      "         0.0104, 0.0694, 0.0615, 0.0188, 0.0656, 0.0250, 0.2546, 0.0081, 0.0332],\n",
      "        [0.0212, 0.0601, 0.0207, 0.1087, 0.0020, 0.0194, 0.0201, 0.1877, 0.0645,\n",
      "         0.0166, 0.0989, 0.1243, 0.0342, 0.0607, 0.0329, 0.0509, 0.0246, 0.0523],\n",
      "        [0.0177, 0.0260, 0.0133, 0.0336, 0.0052, 0.0156, 0.0139, 0.2343, 0.0599,\n",
      "         0.0141, 0.0733, 0.1440, 0.0248, 0.0609, 0.0226, 0.1879, 0.0151, 0.0378],\n",
      "        [0.0186, 0.0471, 0.0112, 0.0732, 0.0022, 0.0181, 0.0402, 0.1539, 0.0868,\n",
      "         0.0236, 0.0678, 0.1051, 0.0422, 0.0603, 0.0375, 0.1537, 0.0182, 0.0402],\n",
      "        [0.0129, 0.0545, 0.0216, 0.1094, 0.0038, 0.0177, 0.0140, 0.2864, 0.0705,\n",
      "         0.0108, 0.0928, 0.0845, 0.0290, 0.0674, 0.0329, 0.0480, 0.0197, 0.0242],\n",
      "        [0.0117, 0.0381, 0.0163, 0.1851, 0.0044, 0.0130, 0.0334, 0.1075, 0.0497,\n",
      "         0.0096, 0.1214, 0.1207, 0.0286, 0.0458, 0.0301, 0.1247, 0.0212, 0.0386],\n",
      "        [0.0172, 0.0499, 0.0104, 0.0891, 0.0060, 0.0077, 0.0200, 0.1379, 0.0600,\n",
      "         0.0176, 0.0780, 0.1717, 0.0215, 0.1003, 0.0267, 0.1160, 0.0179, 0.0521],\n",
      "        [0.0214, 0.0542, 0.0151, 0.0720, 0.0069, 0.0094, 0.0133, 0.2471, 0.0381,\n",
      "         0.0095, 0.1249, 0.0959, 0.0173, 0.0388, 0.0274, 0.1480, 0.0209, 0.0396],\n",
      "        [0.0172, 0.0496, 0.0089, 0.0808, 0.0037, 0.0082, 0.0268, 0.1209, 0.0507,\n",
      "         0.0287, 0.1567, 0.1183, 0.0295, 0.1032, 0.0318, 0.0834, 0.0280, 0.0538],\n",
      "        [0.0097, 0.0803, 0.0091, 0.1270, 0.0052, 0.0218, 0.0215, 0.1073, 0.0418,\n",
      "         0.0191, 0.1104, 0.1324, 0.0194, 0.0790, 0.0332, 0.1215, 0.0154, 0.0460],\n",
      "        [0.0075, 0.0647, 0.0111, 0.1453, 0.0112, 0.0171, 0.0150, 0.0759, 0.0505,\n",
      "         0.0115, 0.1982, 0.1001, 0.0225, 0.0653, 0.0257, 0.1267, 0.0135, 0.0383]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 53 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0203, 0.0221, 0.0732, 0.2438, 0.0233, 0.1032, 0.0262, 0.0247, 0.0149,\n",
      "         0.0808, 0.0345, 0.0216, 0.0111, 0.1421, 0.1347, 0.0181, 0.0056],\n",
      "        [0.0699, 0.0086, 0.0550, 0.1516, 0.0414, 0.1697, 0.0223, 0.0230, 0.0039,\n",
      "         0.0469, 0.0299, 0.0366, 0.0262, 0.2327, 0.0390, 0.0269, 0.0162],\n",
      "        [0.0523, 0.0314, 0.0422, 0.1818, 0.0158, 0.3406, 0.0246, 0.0103, 0.0051,\n",
      "         0.0218, 0.0217, 0.0193, 0.0213, 0.0783, 0.0769, 0.0411, 0.0156],\n",
      "        [0.0226, 0.0350, 0.0541, 0.1299, 0.0103, 0.1196, 0.0473, 0.0235, 0.0042,\n",
      "         0.0944, 0.0212, 0.0262, 0.0110, 0.2193, 0.0874, 0.0611, 0.0326],\n",
      "        [0.0315, 0.0204, 0.0766, 0.1553, 0.0214, 0.1092, 0.0170, 0.0376, 0.0035,\n",
      "         0.0559, 0.0468, 0.0617, 0.0250, 0.2288, 0.0726, 0.0272, 0.0095],\n",
      "        [0.0358, 0.0337, 0.0506, 0.1185, 0.0165, 0.1489, 0.0187, 0.0409, 0.0047,\n",
      "         0.0513, 0.0331, 0.0267, 0.0236, 0.3224, 0.0372, 0.0311, 0.0063],\n",
      "        [0.0430, 0.0114, 0.0598, 0.1117, 0.0196, 0.0767, 0.0170, 0.0340, 0.0053,\n",
      "         0.0264, 0.0268, 0.0495, 0.0293, 0.3547, 0.0763, 0.0515, 0.0071],\n",
      "        [0.0256, 0.0116, 0.0560, 0.1953, 0.0103, 0.3435, 0.0226, 0.0210, 0.0054,\n",
      "         0.0249, 0.0282, 0.0232, 0.0452, 0.0460, 0.0955, 0.0267, 0.0191],\n",
      "        [0.0538, 0.0157, 0.0807, 0.1182, 0.0413, 0.1207, 0.0269, 0.0336, 0.0066,\n",
      "         0.0865, 0.0320, 0.0079, 0.0087, 0.2128, 0.0722, 0.0653, 0.0170],\n",
      "        [0.0745, 0.0182, 0.0517, 0.0544, 0.0153, 0.3708, 0.0083, 0.0282, 0.0063,\n",
      "         0.0489, 0.0463, 0.0693, 0.0319, 0.0757, 0.0542, 0.0392, 0.0065],\n",
      "        [0.0269, 0.0129, 0.0588, 0.3350, 0.0250, 0.0480, 0.0277, 0.0763, 0.0070,\n",
      "         0.0279, 0.0233, 0.0197, 0.0089, 0.1898, 0.0683, 0.0336, 0.0108],\n",
      "        [0.1102, 0.0095, 0.0271, 0.1951, 0.0234, 0.2585, 0.0257, 0.0533, 0.0068,\n",
      "         0.0646, 0.0257, 0.0245, 0.0202, 0.0279, 0.0877, 0.0313, 0.0085],\n",
      "        [0.0477, 0.0147, 0.1695, 0.0794, 0.0218, 0.1639, 0.0148, 0.0246, 0.0055,\n",
      "         0.0664, 0.0244, 0.0753, 0.0269, 0.1847, 0.0460, 0.0259, 0.0086],\n",
      "        [0.0205, 0.0167, 0.0315, 0.0919, 0.0210, 0.4750, 0.0132, 0.0263, 0.0073,\n",
      "         0.0445, 0.0208, 0.0101, 0.0153, 0.1097, 0.0529, 0.0317, 0.0119],\n",
      "        [0.0313, 0.0329, 0.0282, 0.1804, 0.0136, 0.1472, 0.0493, 0.1404, 0.0157,\n",
      "         0.0338, 0.0218, 0.0793, 0.0094, 0.0821, 0.0745, 0.0559, 0.0041],\n",
      "        [0.0277, 0.0127, 0.0232, 0.0982, 0.0154, 0.4220, 0.0262, 0.0221, 0.0031,\n",
      "         0.0556, 0.0370, 0.0504, 0.0338, 0.0602, 0.0469, 0.0511, 0.0145],\n",
      "        [0.0486, 0.0172, 0.0430, 0.2352, 0.0273, 0.1326, 0.0166, 0.0561, 0.0051,\n",
      "         0.0317, 0.0274, 0.0455, 0.0201, 0.1942, 0.0584, 0.0344, 0.0067]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 54 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0688, 0.0031, 0.1182, 0.0294, 0.0271, 0.0413, 0.0631, 0.0346, 0.0350,\n",
      "         0.0125, 0.1550, 0.2284, 0.0624, 0.0548, 0.0661],\n",
      "        [0.0888, 0.0093, 0.1986, 0.0306, 0.0316, 0.0240, 0.0343, 0.0120, 0.0432,\n",
      "         0.0069, 0.0751, 0.3003, 0.0653, 0.0382, 0.0417],\n",
      "        [0.1463, 0.0073, 0.1164, 0.0202, 0.0318, 0.0460, 0.0359, 0.0108, 0.0225,\n",
      "         0.0118, 0.0802, 0.3437, 0.0403, 0.0340, 0.0528],\n",
      "        [0.0843, 0.0061, 0.2453, 0.0158, 0.0419, 0.0219, 0.0806, 0.0096, 0.0433,\n",
      "         0.0164, 0.0924, 0.1438, 0.0832, 0.0597, 0.0559],\n",
      "        [0.0598, 0.0097, 0.3916, 0.0154, 0.0197, 0.0488, 0.0653, 0.0077, 0.0283,\n",
      "         0.0125, 0.0546, 0.1305, 0.0407, 0.0642, 0.0510],\n",
      "        [0.1263, 0.0105, 0.1644, 0.0112, 0.0383, 0.0415, 0.0489, 0.0213, 0.0216,\n",
      "         0.0092, 0.1248, 0.2788, 0.0250, 0.0376, 0.0406],\n",
      "        [0.0505, 0.0032, 0.3355, 0.0200, 0.0137, 0.0386, 0.0706, 0.0345, 0.0712,\n",
      "         0.0354, 0.0709, 0.0874, 0.0889, 0.0287, 0.0510],\n",
      "        [0.0954, 0.0044, 0.1560, 0.0262, 0.0294, 0.0404, 0.0386, 0.0124, 0.0242,\n",
      "         0.0302, 0.2217, 0.1129, 0.0710, 0.0246, 0.1126],\n",
      "        [0.0631, 0.0109, 0.1235, 0.0166, 0.0104, 0.0290, 0.0322, 0.0103, 0.0528,\n",
      "         0.0175, 0.0875, 0.3743, 0.0913, 0.0278, 0.0529],\n",
      "        [0.0757, 0.0039, 0.2673, 0.0273, 0.0237, 0.0400, 0.0455, 0.0105, 0.0507,\n",
      "         0.0281, 0.1428, 0.0945, 0.0405, 0.0469, 0.1025],\n",
      "        [0.1016, 0.0058, 0.1629, 0.0115, 0.0163, 0.0612, 0.0186, 0.0304, 0.0466,\n",
      "         0.0293, 0.1011, 0.2672, 0.0345, 0.0293, 0.0837],\n",
      "        [0.1284, 0.0049, 0.1984, 0.0490, 0.0252, 0.0132, 0.0415, 0.0123, 0.0450,\n",
      "         0.0311, 0.1320, 0.1616, 0.0439, 0.0356, 0.0779],\n",
      "        [0.1380, 0.0034, 0.1338, 0.0819, 0.0349, 0.0703, 0.0835, 0.0083, 0.0523,\n",
      "         0.0317, 0.0948, 0.1412, 0.0438, 0.0369, 0.0453],\n",
      "        [0.2021, 0.0104, 0.0849, 0.0133, 0.0234, 0.0136, 0.0217, 0.0153, 0.1165,\n",
      "         0.0215, 0.2362, 0.0931, 0.0269, 0.0412, 0.0798],\n",
      "        [0.1257, 0.0068, 0.0791, 0.0287, 0.0689, 0.0511, 0.0405, 0.0143, 0.0136,\n",
      "         0.0114, 0.1943, 0.2413, 0.0408, 0.0324, 0.0512]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 55 - Output:\n",
      "torch.Size([3, 3])\n",
      "1\n",
      "tensor([[0.0606, 0.6957, 0.2437],\n",
      "        [0.0708, 0.7778, 0.1513],\n",
      "        [0.0715, 0.7799, 0.1486]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 56 - Output:\n",
      "torch.Size([9, 9])\n",
      "1\n",
      "tensor([[0.1285, 0.0575, 0.3700, 0.0181, 0.1022, 0.1753, 0.1008, 0.0321, 0.0156],\n",
      "        [0.1252, 0.0798, 0.0643, 0.0179, 0.1558, 0.2954, 0.1641, 0.0864, 0.0112],\n",
      "        [0.1587, 0.1165, 0.0513, 0.0280, 0.1018, 0.3442, 0.1638, 0.0173, 0.0184],\n",
      "        [0.0916, 0.0882, 0.1562, 0.0525, 0.0813, 0.3610, 0.1003, 0.0619, 0.0069],\n",
      "        [0.2030, 0.0791, 0.1800, 0.0605, 0.1828, 0.1192, 0.1107, 0.0582, 0.0064],\n",
      "        [0.2488, 0.0856, 0.1486, 0.0685, 0.0765, 0.1443, 0.1142, 0.1075, 0.0060],\n",
      "        [0.1216, 0.0825, 0.1438, 0.0413, 0.0942, 0.2640, 0.1714, 0.0744, 0.0069],\n",
      "        [0.1637, 0.0913, 0.1021, 0.0575, 0.1456, 0.2803, 0.1306, 0.0178, 0.0112],\n",
      "        [0.1683, 0.1052, 0.1001, 0.0758, 0.1282, 0.2796, 0.1045, 0.0306, 0.0076]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 57 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0623, 0.0067, 0.0282, 0.0641, 0.0840, 0.2079, 0.1551, 0.0124, 0.0397,\n",
      "         0.0445, 0.0153, 0.0521, 0.0052, 0.1054, 0.0219, 0.0654, 0.0299],\n",
      "        [0.0375, 0.0061, 0.0195, 0.0727, 0.0853, 0.2271, 0.2538, 0.0175, 0.0270,\n",
      "         0.0404, 0.0076, 0.0274, 0.0170, 0.0798, 0.0273, 0.0379, 0.0161],\n",
      "        [0.0399, 0.0055, 0.0220, 0.0602, 0.1743, 0.0833, 0.3002, 0.0145, 0.0192,\n",
      "         0.0429, 0.0155, 0.0366, 0.0089, 0.0808, 0.0248, 0.0531, 0.0183],\n",
      "        [0.0362, 0.0038, 0.0248, 0.0828, 0.0948, 0.2128, 0.1654, 0.0108, 0.0219,\n",
      "         0.0544, 0.0167, 0.0515, 0.0163, 0.0717, 0.0299, 0.0838, 0.0224],\n",
      "        [0.0245, 0.0081, 0.0203, 0.0875, 0.0711, 0.1815, 0.2829, 0.0128, 0.0210,\n",
      "         0.0492, 0.0102, 0.0236, 0.0099, 0.0635, 0.0299, 0.0823, 0.0218],\n",
      "        [0.0381, 0.0039, 0.0265, 0.0501, 0.1165, 0.2656, 0.1337, 0.0113, 0.0238,\n",
      "         0.0732, 0.0110, 0.0493, 0.0181, 0.0772, 0.0309, 0.0446, 0.0261],\n",
      "        [0.0377, 0.0110, 0.0190, 0.0756, 0.0819, 0.1899, 0.2824, 0.0056, 0.0226,\n",
      "         0.0418, 0.0114, 0.0412, 0.0114, 0.0559, 0.0407, 0.0508, 0.0211],\n",
      "        [0.0321, 0.0026, 0.0261, 0.0526, 0.1345, 0.1094, 0.3236, 0.0228, 0.0247,\n",
      "         0.0488, 0.0206, 0.0225, 0.0150, 0.0628, 0.0293, 0.0402, 0.0323],\n",
      "        [0.0334, 0.0043, 0.0177, 0.0899, 0.0638, 0.1801, 0.2561, 0.0141, 0.0216,\n",
      "         0.0695, 0.0154, 0.0205, 0.0196, 0.0959, 0.0212, 0.0478, 0.0291],\n",
      "        [0.0285, 0.0085, 0.0191, 0.0554, 0.1077, 0.0966, 0.3819, 0.0081, 0.0194,\n",
      "         0.0495, 0.0130, 0.0412, 0.0116, 0.0492, 0.0163, 0.0745, 0.0195],\n",
      "        [0.0570, 0.0035, 0.0221, 0.1097, 0.0773, 0.1283, 0.2038, 0.0135, 0.0218,\n",
      "         0.0465, 0.0125, 0.0278, 0.0193, 0.1161, 0.0344, 0.0755, 0.0310],\n",
      "        [0.0452, 0.0038, 0.0233, 0.0806, 0.1605, 0.1877, 0.1344, 0.0172, 0.0286,\n",
      "         0.0396, 0.0120, 0.0583, 0.0106, 0.0809, 0.0263, 0.0508, 0.0402],\n",
      "        [0.0379, 0.0096, 0.0230, 0.0568, 0.0912, 0.0848, 0.3585, 0.0060, 0.0265,\n",
      "         0.0478, 0.0168, 0.0264, 0.0101, 0.0824, 0.0167, 0.0852, 0.0204],\n",
      "        [0.0459, 0.0059, 0.0221, 0.1476, 0.0733, 0.0721, 0.2101, 0.0135, 0.0226,\n",
      "         0.0732, 0.0109, 0.0628, 0.0108, 0.0750, 0.0294, 0.1092, 0.0158],\n",
      "        [0.0282, 0.0036, 0.0336, 0.1084, 0.1013, 0.1894, 0.1841, 0.0088, 0.0281,\n",
      "         0.0537, 0.0206, 0.0385, 0.0146, 0.0588, 0.0311, 0.0662, 0.0311],\n",
      "        [0.0382, 0.0062, 0.0215, 0.0526, 0.0627, 0.1767, 0.3247, 0.0118, 0.0254,\n",
      "         0.0672, 0.0125, 0.0553, 0.0086, 0.0560, 0.0185, 0.0373, 0.0248],\n",
      "        [0.0296, 0.0084, 0.0264, 0.0706, 0.0689, 0.0582, 0.4832, 0.0101, 0.0230,\n",
      "         0.0268, 0.0091, 0.0347, 0.0064, 0.0629, 0.0217, 0.0333, 0.0267]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 58 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.4461, 0.0673, 0.0283, 0.1658, 0.2925],\n",
      "        [0.5069, 0.0648, 0.0410, 0.0636, 0.3238],\n",
      "        [0.6971, 0.0698, 0.0360, 0.0777, 0.1194],\n",
      "        [0.5499, 0.1224, 0.0251, 0.1071, 0.1955],\n",
      "        [0.6405, 0.0418, 0.0998, 0.0461, 0.1718]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 59 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0223, 0.0257, 0.0736, 0.0690, 0.0530, 0.0384, 0.0259, 0.0071, 0.0430,\n",
      "         0.0304, 0.0113, 0.0168, 0.3218, 0.0143, 0.2203, 0.0164, 0.0109],\n",
      "        [0.0202, 0.0373, 0.0309, 0.0672, 0.1890, 0.0115, 0.0118, 0.0107, 0.0542,\n",
      "         0.0650, 0.0180, 0.0458, 0.0376, 0.0102, 0.3535, 0.0274, 0.0097],\n",
      "        [0.0533, 0.0629, 0.1001, 0.0289, 0.2312, 0.0201, 0.0137, 0.0119, 0.0736,\n",
      "         0.0959, 0.0309, 0.0137, 0.0962, 0.0239, 0.1244, 0.0136, 0.0057],\n",
      "        [0.0322, 0.0322, 0.0937, 0.0419, 0.0799, 0.0272, 0.0070, 0.0111, 0.0390,\n",
      "         0.0977, 0.0178, 0.0090, 0.1692, 0.0178, 0.2750, 0.0342, 0.0152],\n",
      "        [0.0259, 0.0698, 0.0393, 0.0610, 0.1037, 0.0126, 0.0052, 0.0150, 0.0584,\n",
      "         0.1597, 0.0193, 0.0230, 0.0913, 0.0140, 0.2452, 0.0425, 0.0141],\n",
      "        [0.0207, 0.0754, 0.1397, 0.0224, 0.0965, 0.0166, 0.0045, 0.0229, 0.0425,\n",
      "         0.1390, 0.0225, 0.0200, 0.1576, 0.0185, 0.1343, 0.0542, 0.0128],\n",
      "        [0.1165, 0.0208, 0.0436, 0.0237, 0.1268, 0.0195, 0.0117, 0.0162, 0.0415,\n",
      "         0.0443, 0.0261, 0.0391, 0.0779, 0.0188, 0.3565, 0.0117, 0.0054],\n",
      "        [0.0338, 0.0391, 0.0911, 0.0267, 0.2135, 0.0169, 0.0037, 0.0149, 0.0482,\n",
      "         0.1326, 0.0479, 0.0330, 0.0780, 0.0114, 0.1542, 0.0366, 0.0184],\n",
      "        [0.0300, 0.0369, 0.0810, 0.0218, 0.0793, 0.0181, 0.0100, 0.0111, 0.0288,\n",
      "         0.0616, 0.0198, 0.0146, 0.2434, 0.0221, 0.2967, 0.0132, 0.0116],\n",
      "        [0.0875, 0.0327, 0.1288, 0.0675, 0.1202, 0.0122, 0.0057, 0.0097, 0.0475,\n",
      "         0.0888, 0.0097, 0.0289, 0.2079, 0.0496, 0.0548, 0.0289, 0.0196],\n",
      "        [0.0436, 0.0908, 0.0830, 0.0706, 0.0484, 0.0186, 0.0059, 0.0180, 0.0555,\n",
      "         0.1593, 0.0192, 0.0404, 0.1002, 0.0095, 0.1986, 0.0300, 0.0085],\n",
      "        [0.0623, 0.0188, 0.0591, 0.0498, 0.1063, 0.0300, 0.0083, 0.0074, 0.0597,\n",
      "         0.0450, 0.0475, 0.0271, 0.1647, 0.0184, 0.2661, 0.0222, 0.0073],\n",
      "        [0.0253, 0.0587, 0.0569, 0.0361, 0.1404, 0.0112, 0.0100, 0.0126, 0.0490,\n",
      "         0.0796, 0.0270, 0.0311, 0.2316, 0.0109, 0.1769, 0.0347, 0.0082],\n",
      "        [0.0440, 0.0272, 0.1883, 0.0863, 0.0846, 0.0070, 0.0093, 0.0177, 0.0673,\n",
      "         0.0933, 0.0242, 0.0174, 0.0985, 0.0120, 0.1775, 0.0308, 0.0147],\n",
      "        [0.0369, 0.0422, 0.0696, 0.0308, 0.0694, 0.0306, 0.0065, 0.0100, 0.0510,\n",
      "         0.1593, 0.0236, 0.0345, 0.1857, 0.0101, 0.1961, 0.0342, 0.0097],\n",
      "        [0.0186, 0.0405, 0.0626, 0.0316, 0.2008, 0.0257, 0.0130, 0.0127, 0.0468,\n",
      "         0.1151, 0.0253, 0.0237, 0.1296, 0.0120, 0.2152, 0.0200, 0.0065],\n",
      "        [0.0233, 0.0306, 0.1065, 0.0250, 0.1103, 0.0163, 0.0180, 0.0149, 0.0355,\n",
      "         0.1086, 0.0130, 0.0136, 0.2400, 0.0246, 0.1610, 0.0525, 0.0064]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 60 - Output:\n",
      "torch.Size([11, 11])\n",
      "1\n",
      "tensor([[0.0431, 0.0425, 0.1835, 0.2453, 0.1042, 0.0077, 0.0750, 0.0212, 0.0788,\n",
      "         0.0258, 0.1727],\n",
      "        [0.1586, 0.0610, 0.2905, 0.1164, 0.0561, 0.0055, 0.0659, 0.0625, 0.0708,\n",
      "         0.0199, 0.0927],\n",
      "        [0.0569, 0.0384, 0.1575, 0.3818, 0.0254, 0.0082, 0.0534, 0.1022, 0.0948,\n",
      "         0.0192, 0.0621],\n",
      "        [0.0685, 0.0419, 0.1172, 0.4695, 0.0650, 0.0080, 0.0405, 0.0331, 0.0514,\n",
      "         0.0169, 0.0881],\n",
      "        [0.1006, 0.0639, 0.2039, 0.2660, 0.0856, 0.0064, 0.0558, 0.0242, 0.1023,\n",
      "         0.0253, 0.0659],\n",
      "        [0.0776, 0.0398, 0.2679, 0.2835, 0.0343, 0.0080, 0.0640, 0.0322, 0.0642,\n",
      "         0.0215, 0.1069],\n",
      "        [0.0763, 0.0465, 0.2997, 0.0683, 0.0929, 0.0053, 0.0735, 0.0727, 0.0612,\n",
      "         0.0233, 0.1801],\n",
      "        [0.0687, 0.0257, 0.2147, 0.1781, 0.0490, 0.0121, 0.0998, 0.0629, 0.1442,\n",
      "         0.0101, 0.1345],\n",
      "        [0.1393, 0.0414, 0.2785, 0.1112, 0.0594, 0.0057, 0.0680, 0.1155, 0.0322,\n",
      "         0.0285, 0.1203],\n",
      "        [0.1779, 0.0516, 0.2104, 0.1049, 0.1294, 0.0056, 0.1062, 0.0523, 0.0483,\n",
      "         0.0202, 0.0931],\n",
      "        [0.0672, 0.0429, 0.4569, 0.1407, 0.0312, 0.0073, 0.0848, 0.0282, 0.0443,\n",
      "         0.0289, 0.0676]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 61 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0132, 0.0204, 0.0436, 0.0910, 0.0719, 0.0083, 0.0299, 0.0227, 0.0637,\n",
      "         0.0839, 0.0403, 0.0120, 0.4702, 0.0289],\n",
      "        [0.0177, 0.0154, 0.0206, 0.0413, 0.0303, 0.0076, 0.0113, 0.0409, 0.0242,\n",
      "         0.0528, 0.0257, 0.0215, 0.6646, 0.0260],\n",
      "        [0.0414, 0.0204, 0.0377, 0.1836, 0.0508, 0.0126, 0.0398, 0.0517, 0.0660,\n",
      "         0.0727, 0.0896, 0.0042, 0.2617, 0.0678],\n",
      "        [0.0262, 0.0165, 0.0719, 0.1770, 0.0394, 0.0128, 0.0258, 0.0292, 0.0583,\n",
      "         0.0344, 0.0673, 0.0069, 0.4012, 0.0331],\n",
      "        [0.0214, 0.0174, 0.0666, 0.1134, 0.0707, 0.0411, 0.0418, 0.0402, 0.0415,\n",
      "         0.0606, 0.0505, 0.0034, 0.3697, 0.0617],\n",
      "        [0.0240, 0.0117, 0.0532, 0.1698, 0.0215, 0.0159, 0.0234, 0.0426, 0.0330,\n",
      "         0.0479, 0.0294, 0.0087, 0.4735, 0.0455],\n",
      "        [0.0079, 0.0264, 0.0339, 0.1085, 0.0278, 0.0211, 0.0303, 0.0390, 0.0508,\n",
      "         0.0664, 0.0345, 0.0094, 0.5262, 0.0178],\n",
      "        [0.0136, 0.0165, 0.0712, 0.1923, 0.0406, 0.0178, 0.0305, 0.0242, 0.0151,\n",
      "         0.0469, 0.0259, 0.0179, 0.4690, 0.0185],\n",
      "        [0.0335, 0.0171, 0.0435, 0.0870, 0.0385, 0.0115, 0.0281, 0.0484, 0.0198,\n",
      "         0.0583, 0.0465, 0.0058, 0.5260, 0.0358],\n",
      "        [0.0203, 0.0174, 0.0340, 0.1025, 0.1143, 0.0158, 0.0378, 0.0598, 0.0254,\n",
      "         0.0714, 0.0685, 0.0055, 0.3693, 0.0581],\n",
      "        [0.0273, 0.0177, 0.0634, 0.2144, 0.1115, 0.0096, 0.0433, 0.0821, 0.0368,\n",
      "         0.1058, 0.0621, 0.0060, 0.1714, 0.0487],\n",
      "        [0.0539, 0.0275, 0.0561, 0.2450, 0.0462, 0.0110, 0.0450, 0.0327, 0.0306,\n",
      "         0.0517, 0.0629, 0.0048, 0.2849, 0.0477],\n",
      "        [0.0419, 0.0169, 0.0585, 0.1274, 0.0497, 0.0271, 0.0302, 0.0344, 0.0410,\n",
      "         0.1044, 0.0448, 0.0037, 0.3633, 0.0566],\n",
      "        [0.0454, 0.0078, 0.0315, 0.1630, 0.1084, 0.0159, 0.0226, 0.0711, 0.0926,\n",
      "         0.0522, 0.0809, 0.0085, 0.2479, 0.0524]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 62 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 63 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0352, 0.0396, 0.0036, 0.1160, 0.0211, 0.0431, 0.0147, 0.0214, 0.0689,\n",
      "         0.1045, 0.1827, 0.1039, 0.1507, 0.0350, 0.0597],\n",
      "        [0.0273, 0.0270, 0.0101, 0.0242, 0.0214, 0.5674, 0.0326, 0.0165, 0.0175,\n",
      "         0.0140, 0.0610, 0.1224, 0.0113, 0.0178, 0.0296],\n",
      "        [0.0303, 0.0757, 0.0450, 0.0529, 0.0278, 0.4342, 0.0235, 0.0066, 0.0349,\n",
      "         0.1587, 0.0157, 0.0135, 0.0162, 0.0436, 0.0215],\n",
      "        [0.0403, 0.0524, 0.0233, 0.0188, 0.0461, 0.4306, 0.0158, 0.0315, 0.0079,\n",
      "         0.1578, 0.0590, 0.0185, 0.0533, 0.0090, 0.0356],\n",
      "        [0.0366, 0.0379, 0.0333, 0.0237, 0.0345, 0.2852, 0.0258, 0.0250, 0.0538,\n",
      "         0.0349, 0.0834, 0.1561, 0.0243, 0.0035, 0.1420],\n",
      "        [0.0169, 0.0486, 0.0210, 0.0144, 0.0366, 0.4469, 0.0297, 0.0155, 0.0534,\n",
      "         0.0461, 0.1401, 0.0309, 0.0073, 0.0145, 0.0782],\n",
      "        [0.0464, 0.0384, 0.0139, 0.1927, 0.0589, 0.0583, 0.0318, 0.0571, 0.2917,\n",
      "         0.1224, 0.0156, 0.0251, 0.0061, 0.0239, 0.0176],\n",
      "        [0.0645, 0.1095, 0.0372, 0.0989, 0.1159, 0.0385, 0.0160, 0.0799, 0.0344,\n",
      "         0.0386, 0.1289, 0.0040, 0.0134, 0.1959, 0.0246],\n",
      "        [0.0563, 0.1973, 0.0164, 0.1457, 0.0303, 0.0373, 0.0178, 0.0049, 0.0272,\n",
      "         0.0456, 0.0682, 0.0180, 0.0359, 0.2585, 0.0407],\n",
      "        [0.0436, 0.0315, 0.0106, 0.1346, 0.0135, 0.1594, 0.0235, 0.0328, 0.0528,\n",
      "         0.0171, 0.0137, 0.3878, 0.0316, 0.0134, 0.0343],\n",
      "        [0.0402, 0.0640, 0.0235, 0.0392, 0.0663, 0.3267, 0.0469, 0.0024, 0.0342,\n",
      "         0.0788, 0.0778, 0.0614, 0.0198, 0.0435, 0.0752],\n",
      "        [0.0522, 0.0751, 0.0055, 0.0523, 0.0097, 0.1002, 0.0359, 0.0204, 0.0544,\n",
      "         0.0565, 0.1980, 0.0365, 0.0161, 0.0323, 0.2551],\n",
      "        [0.0193, 0.0462, 0.0066, 0.0849, 0.0253, 0.0759, 0.0322, 0.0578, 0.1787,\n",
      "         0.1702, 0.1403, 0.0267, 0.0220, 0.0082, 0.1058],\n",
      "        [0.0102, 0.1668, 0.0194, 0.0786, 0.0286, 0.0264, 0.0044, 0.1783, 0.0755,\n",
      "         0.1240, 0.0995, 0.0436, 0.0418, 0.0377, 0.0652],\n",
      "        [0.0944, 0.0293, 0.0613, 0.1596, 0.0177, 0.0666, 0.0476, 0.0410, 0.0497,\n",
      "         0.0201, 0.0657, 0.0309, 0.2603, 0.0030, 0.0528]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 64 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0416, 0.0347, 0.0358, 0.0254, 0.2146, 0.0292, 0.0718, 0.2270, 0.0140,\n",
      "         0.0449, 0.0040, 0.1163, 0.0248, 0.0173, 0.0554, 0.0336, 0.0096],\n",
      "        [0.0674, 0.0094, 0.0917, 0.0746, 0.1420, 0.0365, 0.0102, 0.1485, 0.0344,\n",
      "         0.0270, 0.0059, 0.1209, 0.1253, 0.0116, 0.0233, 0.0419, 0.0293],\n",
      "        [0.0358, 0.0733, 0.1320, 0.0962, 0.1012, 0.0152, 0.0749, 0.0466, 0.0085,\n",
      "         0.0138, 0.0038, 0.1654, 0.0310, 0.0866, 0.0396, 0.0539, 0.0222],\n",
      "        [0.0527, 0.0402, 0.0600, 0.0473, 0.3426, 0.0061, 0.0420, 0.0985, 0.0108,\n",
      "         0.0154, 0.0346, 0.0603, 0.1016, 0.0110, 0.0124, 0.0132, 0.0513],\n",
      "        [0.0695, 0.0309, 0.1264, 0.0251, 0.0644, 0.0441, 0.0667, 0.2412, 0.0066,\n",
      "         0.0173, 0.0070, 0.1702, 0.0313, 0.0211, 0.0117, 0.0459, 0.0206],\n",
      "        [0.1626, 0.0179, 0.0961, 0.0434, 0.0581, 0.0720, 0.0645, 0.1329, 0.0274,\n",
      "         0.1180, 0.0170, 0.0458, 0.0226, 0.0493, 0.0448, 0.0255, 0.0021],\n",
      "        [0.0459, 0.0316, 0.0728, 0.0444, 0.2252, 0.0201, 0.0814, 0.2169, 0.0065,\n",
      "         0.0216, 0.0048, 0.0531, 0.0140, 0.0398, 0.0574, 0.0220, 0.0422],\n",
      "        [0.0253, 0.0357, 0.0291, 0.0563, 0.1560, 0.0413, 0.0407, 0.3082, 0.0524,\n",
      "         0.0156, 0.0032, 0.1033, 0.0440, 0.0208, 0.0382, 0.0164, 0.0136],\n",
      "        [0.0222, 0.0216, 0.1047, 0.0226, 0.1068, 0.0490, 0.0246, 0.2803, 0.1208,\n",
      "         0.0360, 0.0056, 0.1086, 0.0162, 0.0178, 0.0363, 0.0166, 0.0103],\n",
      "        [0.0774, 0.0543, 0.1999, 0.0262, 0.0624, 0.0841, 0.0167, 0.1470, 0.0437,\n",
      "         0.0270, 0.0036, 0.0975, 0.0171, 0.0665, 0.0308, 0.0376, 0.0081],\n",
      "        [0.2090, 0.0194, 0.1427, 0.0756, 0.1029, 0.0519, 0.0142, 0.0788, 0.0068,\n",
      "         0.0072, 0.0235, 0.1039, 0.0149, 0.0675, 0.0356, 0.0200, 0.0261],\n",
      "        [0.0114, 0.0404, 0.0893, 0.0193, 0.2564, 0.0077, 0.0286, 0.1811, 0.0359,\n",
      "         0.0189, 0.0066, 0.0637, 0.0876, 0.0349, 0.0740, 0.0175, 0.0268],\n",
      "        [0.0367, 0.0272, 0.0489, 0.0203, 0.1940, 0.0097, 0.0468, 0.2371, 0.0720,\n",
      "         0.0229, 0.0041, 0.1211, 0.0280, 0.0316, 0.0576, 0.0132, 0.0288],\n",
      "        [0.0336, 0.0751, 0.1232, 0.0431, 0.0565, 0.0107, 0.0050, 0.1633, 0.0534,\n",
      "         0.0300, 0.0247, 0.2165, 0.0182, 0.0263, 0.0748, 0.0379, 0.0078],\n",
      "        [0.0725, 0.0787, 0.1486, 0.0146, 0.1666, 0.0208, 0.0969, 0.0447, 0.0046,\n",
      "         0.0472, 0.0054, 0.0606, 0.0774, 0.0668, 0.0468, 0.0246, 0.0232],\n",
      "        [0.0452, 0.0403, 0.1491, 0.0679, 0.0758, 0.0119, 0.0262, 0.0990, 0.0059,\n",
      "         0.0269, 0.0043, 0.0786, 0.0656, 0.0339, 0.0359, 0.0411, 0.1924],\n",
      "        [0.0553, 0.0136, 0.1488, 0.0333, 0.2531, 0.0068, 0.0160, 0.1638, 0.0132,\n",
      "         0.0170, 0.0250, 0.0951, 0.0627, 0.0238, 0.0344, 0.0249, 0.0131]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 65 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.0336, 0.0381, 0.0096, 0.2952, 0.1182, 0.0088, 0.0371, 0.0581, 0.1811,\n",
      "         0.0677, 0.0175, 0.0678, 0.0671],\n",
      "        [0.0346, 0.0394, 0.0060, 0.2223, 0.1186, 0.0148, 0.0420, 0.0512, 0.1744,\n",
      "         0.0964, 0.0219, 0.0372, 0.1413],\n",
      "        [0.0240, 0.0327, 0.0048, 0.4448, 0.1095, 0.0333, 0.0323, 0.0285, 0.0851,\n",
      "         0.0557, 0.0253, 0.0663, 0.0579],\n",
      "        [0.0263, 0.0361, 0.0105, 0.3452, 0.0848, 0.0142, 0.0270, 0.0159, 0.1592,\n",
      "         0.1085, 0.0237, 0.0667, 0.0822],\n",
      "        [0.0337, 0.0574, 0.0088, 0.3665, 0.1089, 0.0102, 0.0468, 0.0268, 0.1518,\n",
      "         0.0692, 0.0182, 0.0490, 0.0526],\n",
      "        [0.0295, 0.0451, 0.0059, 0.1396, 0.0902, 0.0147, 0.0312, 0.0308, 0.3366,\n",
      "         0.0627, 0.0313, 0.0575, 0.1250],\n",
      "        [0.0479, 0.0274, 0.0048, 0.2682, 0.1765, 0.0282, 0.0367, 0.0232, 0.1376,\n",
      "         0.0816, 0.0315, 0.0603, 0.0762],\n",
      "        [0.0368, 0.0207, 0.0136, 0.3510, 0.1520, 0.0060, 0.0841, 0.0423, 0.0683,\n",
      "         0.0519, 0.0312, 0.0839, 0.0582],\n",
      "        [0.0327, 0.0244, 0.0165, 0.2472, 0.1472, 0.0061, 0.0677, 0.0290, 0.1828,\n",
      "         0.1042, 0.0300, 0.0538, 0.0584],\n",
      "        [0.0419, 0.0320, 0.0042, 0.1785, 0.2685, 0.0203, 0.0302, 0.0589, 0.1152,\n",
      "         0.0773, 0.0390, 0.0737, 0.0601],\n",
      "        [0.0318, 0.0382, 0.0119, 0.3118, 0.3327, 0.0114, 0.0344, 0.0219, 0.0357,\n",
      "         0.0718, 0.0210, 0.0435, 0.0339],\n",
      "        [0.0488, 0.0299, 0.0090, 0.3177, 0.0687, 0.0087, 0.0706, 0.0206, 0.0775,\n",
      "         0.1275, 0.0283, 0.0657, 0.1268],\n",
      "        [0.0239, 0.0230, 0.0083, 0.3499, 0.1667, 0.0199, 0.0403, 0.0361, 0.1379,\n",
      "         0.0898, 0.0177, 0.0473, 0.0391]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 66 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 67 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.0161, 0.2122, 0.0762, 0.1100, 0.0374, 0.3887, 0.1595],\n",
      "        [0.0245, 0.4334, 0.0218, 0.0809, 0.0998, 0.1725, 0.1670],\n",
      "        [0.0826, 0.5769, 0.0439, 0.0522, 0.0186, 0.1511, 0.0747],\n",
      "        [0.0229, 0.4979, 0.0699, 0.1127, 0.0619, 0.2056, 0.0291],\n",
      "        [0.0385, 0.4632, 0.0308, 0.1734, 0.0264, 0.1829, 0.0849],\n",
      "        [0.0327, 0.5386, 0.0554, 0.1126, 0.0240, 0.1837, 0.0530],\n",
      "        [0.0333, 0.2530, 0.0880, 0.0898, 0.0177, 0.3993, 0.1189]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 68 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.2859, 0.0536, 0.0220, 0.0736, 0.1737, 0.3913],\n",
      "        [0.3921, 0.0201, 0.0600, 0.0923, 0.1238, 0.3117],\n",
      "        [0.0914, 0.0505, 0.0746, 0.0335, 0.0463, 0.7037],\n",
      "        [0.3192, 0.0383, 0.1470, 0.0240, 0.1229, 0.3487],\n",
      "        [0.2029, 0.0771, 0.0610, 0.0202, 0.1843, 0.4544],\n",
      "        [0.1479, 0.0689, 0.0228, 0.0822, 0.0639, 0.6143]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 69 - Output:\n",
      "torch.Size([10, 10])\n",
      "1\n",
      "tensor([[0.0180, 0.0806, 0.1099, 0.0649, 0.5161, 0.0141, 0.0185, 0.0547, 0.0657,\n",
      "         0.0575],\n",
      "        [0.0082, 0.1138, 0.1291, 0.0943, 0.3545, 0.0270, 0.0281, 0.0602, 0.0612,\n",
      "         0.1236],\n",
      "        [0.0163, 0.1044, 0.0936, 0.0564, 0.5090, 0.0203, 0.0157, 0.0449, 0.0494,\n",
      "         0.0901],\n",
      "        [0.0214, 0.0610, 0.0946, 0.0781, 0.4339, 0.0181, 0.0121, 0.0689, 0.0774,\n",
      "         0.1345],\n",
      "        [0.0063, 0.1454, 0.1344, 0.0864, 0.3007, 0.0384, 0.0356, 0.0717, 0.0651,\n",
      "         0.1159],\n",
      "        [0.0124, 0.1656, 0.0990, 0.0609, 0.4462, 0.0223, 0.0236, 0.0350, 0.0570,\n",
      "         0.0779],\n",
      "        [0.0061, 0.0863, 0.1190, 0.0712, 0.3424, 0.0792, 0.0304, 0.0653, 0.0597,\n",
      "         0.1403],\n",
      "        [0.0111, 0.0666, 0.1124, 0.0777, 0.4792, 0.0263, 0.0192, 0.0719, 0.0416,\n",
      "         0.0941],\n",
      "        [0.0120, 0.0982, 0.1050, 0.0799, 0.4490, 0.0331, 0.0141, 0.0504, 0.0616,\n",
      "         0.0968],\n",
      "        [0.0085, 0.0604, 0.0647, 0.0677, 0.5478, 0.0333, 0.0233, 0.0526, 0.0588,\n",
      "         0.0826]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 70 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.0578, 0.0560, 0.0060, 0.1017, 0.0599, 0.1124, 0.0608, 0.0378, 0.0363,\n",
      "         0.0883, 0.0278, 0.0480, 0.0701, 0.1041, 0.0148, 0.0021, 0.0580, 0.0579],\n",
      "        [0.0378, 0.0323, 0.0079, 0.0306, 0.0370, 0.1844, 0.0229, 0.0257, 0.0401,\n",
      "         0.1325, 0.0283, 0.0575, 0.0594, 0.1987, 0.0167, 0.0027, 0.0539, 0.0316],\n",
      "        [0.0680, 0.0818, 0.0076, 0.0317, 0.0874, 0.0857, 0.0170, 0.0791, 0.0202,\n",
      "         0.1026, 0.0266, 0.0582, 0.0436, 0.1092, 0.0066, 0.0048, 0.0329, 0.1371],\n",
      "        [0.0543, 0.0452, 0.0071, 0.0197, 0.0325, 0.0665, 0.0894, 0.0474, 0.0189,\n",
      "         0.1166, 0.0208, 0.0411, 0.0570, 0.2757, 0.0052, 0.0067, 0.0317, 0.0644],\n",
      "        [0.0798, 0.1476, 0.0049, 0.0402, 0.0504, 0.0520, 0.0827, 0.0303, 0.0196,\n",
      "         0.0772, 0.0369, 0.0445, 0.0874, 0.1032, 0.0088, 0.0039, 0.0503, 0.0800],\n",
      "        [0.0821, 0.0474, 0.0120, 0.0389, 0.0515, 0.0829, 0.0493, 0.0371, 0.0212,\n",
      "         0.0738, 0.0247, 0.0604, 0.0995, 0.1317, 0.0112, 0.0020, 0.0720, 0.1024],\n",
      "        [0.0601, 0.1007, 0.0336, 0.0422, 0.0642, 0.1188, 0.0469, 0.0237, 0.0225,\n",
      "         0.0885, 0.0688, 0.0510, 0.1054, 0.0908, 0.0172, 0.0014, 0.0273, 0.0368],\n",
      "        [0.0858, 0.0652, 0.0155, 0.0093, 0.0374, 0.0528, 0.0133, 0.0483, 0.0165,\n",
      "         0.1430, 0.0325, 0.0513, 0.1669, 0.1347, 0.0137, 0.0038, 0.0630, 0.0469],\n",
      "        [0.0628, 0.1178, 0.0141, 0.0160, 0.0341, 0.0971, 0.0477, 0.0208, 0.0225,\n",
      "         0.1166, 0.0397, 0.0449, 0.1214, 0.1014, 0.0075, 0.0032, 0.0927, 0.0398],\n",
      "        [0.0393, 0.2034, 0.0062, 0.0428, 0.0465, 0.0604, 0.0454, 0.0404, 0.0313,\n",
      "         0.0659, 0.0315, 0.0337, 0.0547, 0.1649, 0.0145, 0.0025, 0.0380, 0.0787],\n",
      "        [0.0674, 0.0590, 0.0040, 0.0139, 0.0429, 0.1269, 0.0259, 0.0396, 0.0314,\n",
      "         0.1858, 0.0181, 0.0502, 0.0438, 0.1699, 0.0114, 0.0074, 0.0485, 0.0540],\n",
      "        [0.0340, 0.1959, 0.0199, 0.0393, 0.0691, 0.1123, 0.0625, 0.0316, 0.0147,\n",
      "         0.0524, 0.0139, 0.0400, 0.0356, 0.1480, 0.0043, 0.0053, 0.0392, 0.0818],\n",
      "        [0.0202, 0.2764, 0.0089, 0.0185, 0.0401, 0.0450, 0.0406, 0.0208, 0.0108,\n",
      "         0.1274, 0.0157, 0.0506, 0.1057, 0.0928, 0.0199, 0.0046, 0.0506, 0.0515],\n",
      "        [0.0511, 0.1524, 0.0049, 0.0879, 0.0273, 0.0749, 0.0296, 0.0787, 0.0324,\n",
      "         0.0841, 0.0111, 0.0239, 0.0667, 0.1596, 0.0106, 0.0062, 0.0574, 0.0411],\n",
      "        [0.0389, 0.1215, 0.0074, 0.0392, 0.0224, 0.0671, 0.0416, 0.0532, 0.0406,\n",
      "         0.0658, 0.0366, 0.0543, 0.0383, 0.1234, 0.0036, 0.0065, 0.1878, 0.0519],\n",
      "        [0.0478, 0.1380, 0.0121, 0.0189, 0.0375, 0.1045, 0.0579, 0.0330, 0.0352,\n",
      "         0.1177, 0.0284, 0.0577, 0.0589, 0.1399, 0.0105, 0.0023, 0.0378, 0.0619],\n",
      "        [0.0654, 0.0507, 0.0052, 0.0229, 0.0264, 0.2058, 0.0438, 0.0166, 0.0194,\n",
      "         0.1363, 0.0247, 0.0269, 0.1703, 0.0437, 0.0142, 0.0060, 0.0305, 0.0910],\n",
      "        [0.0550, 0.0294, 0.0073, 0.0248, 0.0956, 0.0576, 0.0523, 0.0773, 0.0188,\n",
      "         0.1035, 0.0265, 0.0391, 0.1140, 0.0710, 0.0100, 0.0033, 0.1525, 0.0621]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 71 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.1074, 0.0336, 0.0239, 0.2092, 0.0871, 0.0171, 0.0223, 0.0669, 0.0943,\n",
      "         0.0092, 0.0202, 0.2682, 0.0155, 0.0253],\n",
      "        [0.0837, 0.0160, 0.0292, 0.1060, 0.1435, 0.0247, 0.0089, 0.0717, 0.2538,\n",
      "         0.0181, 0.0227, 0.1748, 0.0192, 0.0278],\n",
      "        [0.0474, 0.0219, 0.0411, 0.2028, 0.1440, 0.0354, 0.0249, 0.0776, 0.2704,\n",
      "         0.0193, 0.0247, 0.0550, 0.0052, 0.0303],\n",
      "        [0.1099, 0.0205, 0.0396, 0.0832, 0.0546, 0.0360, 0.0081, 0.1111, 0.2023,\n",
      "         0.0329, 0.0245, 0.2430, 0.0095, 0.0248],\n",
      "        [0.0808, 0.0193, 0.0402, 0.1139, 0.1082, 0.0174, 0.0267, 0.0806, 0.0645,\n",
      "         0.0109, 0.0127, 0.3782, 0.0133, 0.0332],\n",
      "        [0.0898, 0.0159, 0.0282, 0.3186, 0.0635, 0.0479, 0.0143, 0.1112, 0.1590,\n",
      "         0.0213, 0.0108, 0.0758, 0.0129, 0.0308],\n",
      "        [0.1309, 0.0282, 0.0499, 0.0481, 0.1098, 0.0143, 0.0258, 0.0921, 0.2380,\n",
      "         0.0303, 0.0093, 0.1680, 0.0092, 0.0460],\n",
      "        [0.0720, 0.0297, 0.0539, 0.2629, 0.0845, 0.0243, 0.0282, 0.1275, 0.0741,\n",
      "         0.0154, 0.0461, 0.1412, 0.0042, 0.0360],\n",
      "        [0.0855, 0.0249, 0.0348, 0.0977, 0.1024, 0.0172, 0.0293, 0.0944, 0.1051,\n",
      "         0.0182, 0.0118, 0.3501, 0.0119, 0.0168],\n",
      "        [0.0570, 0.0256, 0.0574, 0.0933, 0.0338, 0.0423, 0.0231, 0.1813, 0.0914,\n",
      "         0.0121, 0.0091, 0.3380, 0.0156, 0.0201],\n",
      "        [0.0981, 0.0188, 0.0318, 0.0720, 0.1487, 0.0154, 0.0316, 0.0392, 0.2889,\n",
      "         0.0222, 0.0088, 0.1807, 0.0226, 0.0211],\n",
      "        [0.1516, 0.0248, 0.0374, 0.1980, 0.1020, 0.0214, 0.0075, 0.0731, 0.2472,\n",
      "         0.0238, 0.0355, 0.0430, 0.0197, 0.0150],\n",
      "        [0.0843, 0.0311, 0.0385, 0.1863, 0.1939, 0.1019, 0.0099, 0.1107, 0.0876,\n",
      "         0.0156, 0.0094, 0.0847, 0.0177, 0.0286],\n",
      "        [0.2216, 0.0228, 0.0212, 0.1339, 0.1100, 0.0199, 0.0181, 0.0587, 0.1401,\n",
      "         0.0183, 0.0338, 0.1703, 0.0097, 0.0216]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 72 - Output:\n",
      "torch.Size([8, 8])\n",
      "1\n",
      "tensor([[0.0269, 0.0436, 0.1219, 0.4710, 0.0323, 0.0852, 0.1961, 0.0230],\n",
      "        [0.0216, 0.0523, 0.1866, 0.3856, 0.0787, 0.0916, 0.1663, 0.0172],\n",
      "        [0.0124, 0.0779, 0.2198, 0.3324, 0.0489, 0.1138, 0.1588, 0.0360],\n",
      "        [0.0108, 0.0811, 0.2009, 0.3412, 0.0971, 0.0904, 0.1432, 0.0354],\n",
      "        [0.0126, 0.0903, 0.1193, 0.2941, 0.0470, 0.1317, 0.2705, 0.0346],\n",
      "        [0.0200, 0.1148, 0.0836, 0.3368, 0.0408, 0.0831, 0.2990, 0.0219],\n",
      "        [0.0313, 0.0504, 0.0481, 0.2845, 0.0558, 0.0441, 0.4633, 0.0224],\n",
      "        [0.0191, 0.1301, 0.1105, 0.2273, 0.0660, 0.1868, 0.2440, 0.0162]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 73 - Output:\n",
      "torch.Size([4, 4])\n",
      "1\n",
      "tensor([[0.0646, 0.6449, 0.0565, 0.2339],\n",
      "        [0.0681, 0.0681, 0.1040, 0.7598],\n",
      "        [0.2528, 0.3441, 0.0328, 0.3703],\n",
      "        [0.4626, 0.4090, 0.0822, 0.0462]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 74 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.0173, 0.1243, 0.0601, 0.0960, 0.4640, 0.1989, 0.0394],\n",
      "        [0.0229, 0.0907, 0.0612, 0.0871, 0.1814, 0.5289, 0.0278],\n",
      "        [0.0483, 0.2726, 0.0277, 0.2784, 0.1839, 0.1660, 0.0231],\n",
      "        [0.0265, 0.1007, 0.0333, 0.1611, 0.3700, 0.2740, 0.0344],\n",
      "        [0.0196, 0.1540, 0.0332, 0.1174, 0.0521, 0.4572, 0.1666],\n",
      "        [0.1042, 0.0697, 0.0426, 0.0680, 0.3779, 0.3196, 0.0179],\n",
      "        [0.0248, 0.1835, 0.0326, 0.0840, 0.5584, 0.0445, 0.0722]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 75 - Output:\n",
      "torch.Size([19, 19])\n",
      "1\n",
      "tensor([[0.0328, 0.0371, 0.0565, 0.0067, 0.0221, 0.1083, 0.0984, 0.0050, 0.0530,\n",
      "         0.0059, 0.2250, 0.0174, 0.0341, 0.0553, 0.0209, 0.0161, 0.0760, 0.0669,\n",
      "         0.0624],\n",
      "        [0.0364, 0.0390, 0.0677, 0.0159, 0.0187, 0.1222, 0.1678, 0.0125, 0.0521,\n",
      "         0.0022, 0.1474, 0.0192, 0.0242, 0.0709, 0.0217, 0.0157, 0.0515, 0.0473,\n",
      "         0.0676],\n",
      "        [0.0348, 0.0356, 0.0705, 0.0074, 0.0127, 0.0864, 0.0805, 0.0092, 0.0532,\n",
      "         0.0053, 0.2224, 0.0375, 0.0355, 0.0602, 0.0199, 0.0077, 0.0719, 0.0691,\n",
      "         0.0803],\n",
      "        [0.0312, 0.0372, 0.0722, 0.0116, 0.0162, 0.0877, 0.1751, 0.0077, 0.0613,\n",
      "         0.0025, 0.1181, 0.0240, 0.0369, 0.0801, 0.0379, 0.0185, 0.0611, 0.0487,\n",
      "         0.0720],\n",
      "        [0.0324, 0.0281, 0.0718, 0.0150, 0.0252, 0.1135, 0.1191, 0.0074, 0.0412,\n",
      "         0.0028, 0.2408, 0.0208, 0.0295, 0.0589, 0.0239, 0.0134, 0.0627, 0.0411,\n",
      "         0.0523],\n",
      "        [0.0342, 0.0363, 0.0773, 0.0179, 0.0120, 0.1112, 0.0430, 0.0040, 0.0500,\n",
      "         0.0071, 0.2060, 0.0332, 0.0332, 0.0430, 0.0198, 0.0087, 0.1173, 0.0865,\n",
      "         0.0593],\n",
      "        [0.0353, 0.0356, 0.0753, 0.0079, 0.0139, 0.1215, 0.1351, 0.0046, 0.0532,\n",
      "         0.0074, 0.1386, 0.0269, 0.0302, 0.0983, 0.0242, 0.0121, 0.0676, 0.0605,\n",
      "         0.0519],\n",
      "        [0.0359, 0.0314, 0.0674, 0.0113, 0.0135, 0.0570, 0.1484, 0.0041, 0.0567,\n",
      "         0.0084, 0.2579, 0.0150, 0.0305, 0.0468, 0.0270, 0.0106, 0.0753, 0.0606,\n",
      "         0.0423],\n",
      "        [0.0278, 0.0378, 0.0651, 0.0082, 0.0261, 0.0877, 0.1741, 0.0076, 0.0552,\n",
      "         0.0038, 0.1995, 0.0186, 0.0310, 0.0590, 0.0214, 0.0137, 0.0716, 0.0476,\n",
      "         0.0441],\n",
      "        [0.0325, 0.0423, 0.0741, 0.0229, 0.0170, 0.1107, 0.0572, 0.0022, 0.0510,\n",
      "         0.0070, 0.2239, 0.0275, 0.0350, 0.0703, 0.0304, 0.0137, 0.0655, 0.0627,\n",
      "         0.0541],\n",
      "        [0.0367, 0.0299, 0.0674, 0.0135, 0.0191, 0.0895, 0.1823, 0.0062, 0.0560,\n",
      "         0.0031, 0.1151, 0.0239, 0.0328, 0.0814, 0.0216, 0.0162, 0.0988, 0.0463,\n",
      "         0.0601],\n",
      "        [0.0406, 0.0337, 0.0523, 0.0099, 0.0213, 0.0856, 0.1159, 0.0059, 0.0349,\n",
      "         0.0043, 0.2325, 0.0195, 0.0261, 0.0854, 0.0272, 0.0128, 0.1073, 0.0404,\n",
      "         0.0443],\n",
      "        [0.0343, 0.0360, 0.0732, 0.0118, 0.0144, 0.0421, 0.0981, 0.0059, 0.0527,\n",
      "         0.0046, 0.2774, 0.0259, 0.0243, 0.1082, 0.0214, 0.0105, 0.0469, 0.0647,\n",
      "         0.0476],\n",
      "        [0.0445, 0.0434, 0.0590, 0.0055, 0.0176, 0.1283, 0.1324, 0.0039, 0.0482,\n",
      "         0.0094, 0.1684, 0.0234, 0.0262, 0.0579, 0.0259, 0.0171, 0.0672, 0.0649,\n",
      "         0.0569],\n",
      "        [0.0317, 0.0366, 0.0834, 0.0086, 0.0208, 0.0646, 0.1591, 0.0060, 0.0445,\n",
      "         0.0043, 0.1907, 0.0258, 0.0307, 0.0912, 0.0228, 0.0128, 0.0579, 0.0480,\n",
      "         0.0605],\n",
      "        [0.0381, 0.0381, 0.0566, 0.0079, 0.0294, 0.1020, 0.1436, 0.0062, 0.0681,\n",
      "         0.0044, 0.1318, 0.0199, 0.0345, 0.0897, 0.0276, 0.0103, 0.0781, 0.0485,\n",
      "         0.0650],\n",
      "        [0.0288, 0.0306, 0.0713, 0.0063, 0.0145, 0.0917, 0.1148, 0.0052, 0.0518,\n",
      "         0.0128, 0.2246, 0.0175, 0.0299, 0.0773, 0.0275, 0.0092, 0.0961, 0.0439,\n",
      "         0.0463],\n",
      "        [0.0309, 0.0323, 0.0680, 0.0043, 0.0224, 0.0828, 0.1126, 0.0166, 0.0397,\n",
      "         0.0039, 0.2670, 0.0244, 0.0259, 0.0762, 0.0209, 0.0146, 0.0582, 0.0558,\n",
      "         0.0434],\n",
      "        [0.0395, 0.0348, 0.1082, 0.0076, 0.0156, 0.1045, 0.1067, 0.0071, 0.0512,\n",
      "         0.0041, 0.2172, 0.0238, 0.0304, 0.0611, 0.0214, 0.0171, 0.0565, 0.0442,\n",
      "         0.0489]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 76 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.0211, 0.2597, 0.2261, 0.1511, 0.3421],\n",
      "        [0.2111, 0.2717, 0.0804, 0.0253, 0.4116],\n",
      "        [0.0245, 0.2605, 0.1104, 0.4802, 0.1244],\n",
      "        [0.0251, 0.0932, 0.5171, 0.2143, 0.1504],\n",
      "        [0.1708, 0.0237, 0.3506, 0.1016, 0.3533]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 77 - Output:\n",
      "torch.Size([2, 2])\n",
      "1\n",
      "tensor([[0.8808, 0.1192],\n",
      "        [0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 78 - Output:\n",
      "torch.Size([14, 14])\n",
      "1\n",
      "tensor([[0.0830, 0.1170, 0.1128, 0.0472, 0.1648, 0.0133, 0.0686, 0.0087, 0.0311,\n",
      "         0.1491, 0.0127, 0.0133, 0.1327, 0.0458],\n",
      "        [0.0434, 0.1325, 0.0574, 0.1342, 0.0635, 0.0142, 0.0571, 0.0175, 0.0085,\n",
      "         0.2707, 0.0209, 0.0118, 0.0763, 0.0920],\n",
      "        [0.0357, 0.0608, 0.1081, 0.0889, 0.3080, 0.0076, 0.0167, 0.0456, 0.0192,\n",
      "         0.1590, 0.0292, 0.0103, 0.0511, 0.0598],\n",
      "        [0.1017, 0.1464, 0.0652, 0.0425, 0.1716, 0.0364, 0.0570, 0.0130, 0.0312,\n",
      "         0.1350, 0.0244, 0.0044, 0.1378, 0.0334],\n",
      "        [0.0488, 0.0809, 0.0358, 0.0859, 0.2472, 0.0190, 0.0169, 0.0367, 0.0128,\n",
      "         0.3109, 0.0165, 0.0099, 0.0376, 0.0412],\n",
      "        [0.0439, 0.0570, 0.0695, 0.0974, 0.2592, 0.0239, 0.0666, 0.0134, 0.0148,\n",
      "         0.2177, 0.0181, 0.0071, 0.0607, 0.0508],\n",
      "        [0.0385, 0.1197, 0.0694, 0.0813, 0.3998, 0.0126, 0.0432, 0.0284, 0.0212,\n",
      "         0.0357, 0.0130, 0.0070, 0.0517, 0.0784],\n",
      "        [0.0317, 0.0609, 0.1185, 0.1357, 0.0994, 0.0329, 0.0286, 0.0174, 0.0377,\n",
      "         0.2963, 0.0103, 0.0073, 0.0934, 0.0299],\n",
      "        [0.0370, 0.0840, 0.0591, 0.0669, 0.1181, 0.0062, 0.0470, 0.0252, 0.0238,\n",
      "         0.2591, 0.0150, 0.0135, 0.0982, 0.1471],\n",
      "        [0.0336, 0.0649, 0.0620, 0.0806, 0.1062, 0.0111, 0.0351, 0.0071, 0.0176,\n",
      "         0.4054, 0.0311, 0.0174, 0.0912, 0.0365],\n",
      "        [0.1042, 0.0715, 0.0607, 0.1080, 0.3418, 0.0098, 0.0362, 0.0218, 0.0334,\n",
      "         0.0917, 0.0136, 0.0077, 0.0408, 0.0586],\n",
      "        [0.0966, 0.0727, 0.0778, 0.0828, 0.1172, 0.0332, 0.0329, 0.0368, 0.0126,\n",
      "         0.2580, 0.0291, 0.0038, 0.0723, 0.0741],\n",
      "        [0.0678, 0.0983, 0.0554, 0.0574, 0.2325, 0.0269, 0.0330, 0.0137, 0.0236,\n",
      "         0.2255, 0.0274, 0.0049, 0.0869, 0.0466],\n",
      "        [0.0370, 0.1451, 0.0750, 0.0495, 0.1409, 0.0158, 0.0399, 0.0185, 0.0370,\n",
      "         0.2661, 0.0191, 0.0058, 0.1169, 0.0334]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 79 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.0151, 0.0528, 0.0127, 0.1647, 0.0342, 0.0854, 0.1218, 0.0074, 0.0779,\n",
      "         0.0507, 0.0054, 0.0451, 0.1022, 0.0139, 0.0509, 0.0368, 0.1229],\n",
      "        [0.0259, 0.0353, 0.0124, 0.1735, 0.0134, 0.0610, 0.0556, 0.0050, 0.0283,\n",
      "         0.0486, 0.0093, 0.0235, 0.0625, 0.0198, 0.0451, 0.0501, 0.3308],\n",
      "        [0.0421, 0.0357, 0.0537, 0.0389, 0.0214, 0.1363, 0.0849, 0.0022, 0.0436,\n",
      "         0.0402, 0.0098, 0.0410, 0.0939, 0.0429, 0.0773, 0.0285, 0.2077],\n",
      "        [0.0297, 0.0108, 0.0264, 0.0793, 0.1407, 0.1850, 0.0349, 0.0065, 0.0675,\n",
      "         0.1178, 0.0066, 0.0180, 0.0823, 0.0359, 0.0297, 0.0226, 0.1059],\n",
      "        [0.0050, 0.0218, 0.0576, 0.2301, 0.0575, 0.0538, 0.0652, 0.0076, 0.0141,\n",
      "         0.0747, 0.0248, 0.0298, 0.0411, 0.0307, 0.0161, 0.0298, 0.2404],\n",
      "        [0.0090, 0.0242, 0.0103, 0.1960, 0.0348, 0.0712, 0.0180, 0.0182, 0.0430,\n",
      "         0.0351, 0.0272, 0.0223, 0.0452, 0.0066, 0.0204, 0.0540, 0.3645],\n",
      "        [0.0145, 0.0461, 0.0152, 0.0977, 0.0202, 0.2301, 0.0757, 0.0175, 0.0859,\n",
      "         0.0378, 0.0076, 0.0485, 0.0321, 0.0084, 0.0143, 0.0374, 0.2110],\n",
      "        [0.0391, 0.0605, 0.0172, 0.2022, 0.0260, 0.1042, 0.0627, 0.0038, 0.0421,\n",
      "         0.0411, 0.0062, 0.0349, 0.1051, 0.0433, 0.0210, 0.0503, 0.1405],\n",
      "        [0.0140, 0.0295, 0.0115, 0.0210, 0.0182, 0.0396, 0.0238, 0.0591, 0.1640,\n",
      "         0.0377, 0.0162, 0.0278, 0.0158, 0.0251, 0.0073, 0.0108, 0.4787],\n",
      "        [0.0091, 0.0447, 0.0263, 0.0375, 0.0359, 0.0204, 0.0155, 0.0183, 0.0210,\n",
      "         0.0667, 0.0103, 0.0137, 0.0216, 0.0057, 0.0238, 0.0567, 0.5728],\n",
      "        [0.0135, 0.0406, 0.0188, 0.1370, 0.0271, 0.0962, 0.1240, 0.0222, 0.0574,\n",
      "         0.1310, 0.0030, 0.0287, 0.0639, 0.0154, 0.0339, 0.0604, 0.1270],\n",
      "        [0.0127, 0.0429, 0.0090, 0.0702, 0.0148, 0.0663, 0.0147, 0.0111, 0.1582,\n",
      "         0.0741, 0.0144, 0.0207, 0.0670, 0.0154, 0.0966, 0.0684, 0.2435],\n",
      "        [0.0286, 0.0137, 0.0353, 0.1195, 0.0169, 0.0760, 0.0351, 0.0024, 0.0622,\n",
      "         0.1018, 0.0400, 0.0351, 0.0452, 0.0317, 0.0225, 0.0958, 0.2382],\n",
      "        [0.0273, 0.0154, 0.0251, 0.0601, 0.0223, 0.1833, 0.0386, 0.0036, 0.0574,\n",
      "         0.0844, 0.0190, 0.0419, 0.0905, 0.0253, 0.0407, 0.0115, 0.2535],\n",
      "        [0.0262, 0.0471, 0.0096, 0.1671, 0.0349, 0.0637, 0.0437, 0.0047, 0.0910,\n",
      "         0.0596, 0.0117, 0.0528, 0.1269, 0.0345, 0.0174, 0.0153, 0.1937],\n",
      "        [0.0086, 0.0183, 0.0096, 0.0159, 0.0157, 0.0570, 0.0294, 0.0194, 0.0101,\n",
      "         0.0258, 0.0116, 0.0193, 0.0309, 0.0075, 0.0305, 0.0210, 0.6693],\n",
      "        [0.0102, 0.0222, 0.0116, 0.0436, 0.0621, 0.0193, 0.0433, 0.0093, 0.0145,\n",
      "         0.0510, 0.0080, 0.0212, 0.0378, 0.0128, 0.0281, 0.0155, 0.5896]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 80 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.5499, 0.0271, 0.1833, 0.1367, 0.0476, 0.0554],\n",
      "        [0.2801, 0.0283, 0.2381, 0.3551, 0.0505, 0.0478],\n",
      "        [0.2264, 0.0946, 0.2725, 0.3429, 0.0386, 0.0248],\n",
      "        [0.2160, 0.0264, 0.5307, 0.1123, 0.0711, 0.0436],\n",
      "        [0.3478, 0.0165, 0.2293, 0.1445, 0.0723, 0.1896],\n",
      "        [0.1578, 0.0174, 0.1569, 0.4747, 0.0717, 0.1215]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 81 - Output:\n",
      "torch.Size([19, 19])\n",
      "1\n",
      "tensor([[0.0116, 0.0059, 0.0579, 0.0133, 0.0289, 0.0794, 0.0306, 0.0476, 0.0181,\n",
      "         0.0081, 0.0491, 0.0285, 0.0226, 0.4188, 0.0082, 0.0684, 0.0685, 0.0124,\n",
      "         0.0220],\n",
      "        [0.0154, 0.0531, 0.0187, 0.0337, 0.2144, 0.0601, 0.0226, 0.0903, 0.0131,\n",
      "         0.0541, 0.0303, 0.0274, 0.0937, 0.0867, 0.0019, 0.0326, 0.0441, 0.0210,\n",
      "         0.0868],\n",
      "        [0.0103, 0.0152, 0.0274, 0.0093, 0.0292, 0.0869, 0.1470, 0.0227, 0.0301,\n",
      "         0.0208, 0.0180, 0.0408, 0.0180, 0.2877, 0.0052, 0.1057, 0.0694, 0.0122,\n",
      "         0.0440],\n",
      "        [0.0202, 0.0353, 0.0524, 0.0316, 0.0609, 0.0797, 0.1044, 0.0811, 0.0287,\n",
      "         0.1044, 0.0319, 0.0357, 0.0303, 0.1466, 0.0012, 0.0379, 0.0735, 0.0203,\n",
      "         0.0240],\n",
      "        [0.0470, 0.0357, 0.0251, 0.0147, 0.0155, 0.0580, 0.0302, 0.0189, 0.0141,\n",
      "         0.0709, 0.0479, 0.0059, 0.0070, 0.4711, 0.0069, 0.0527, 0.0215, 0.0142,\n",
      "         0.0426],\n",
      "        [0.0524, 0.0274, 0.0299, 0.0201, 0.0132, 0.0205, 0.0468, 0.0454, 0.0094,\n",
      "         0.1412, 0.0138, 0.0150, 0.0529, 0.2937, 0.0051, 0.1371, 0.0452, 0.0106,\n",
      "         0.0202],\n",
      "        [0.0106, 0.0404, 0.0486, 0.0146, 0.0167, 0.0120, 0.1541, 0.0791, 0.0353,\n",
      "         0.2140, 0.0095, 0.0161, 0.0414, 0.1048, 0.0049, 0.0250, 0.0836, 0.0604,\n",
      "         0.0291],\n",
      "        [0.0569, 0.0214, 0.0169, 0.0089, 0.0207, 0.1359, 0.0454, 0.1087, 0.0599,\n",
      "         0.0874, 0.0390, 0.0111, 0.0901, 0.0107, 0.0048, 0.0132, 0.1521, 0.0379,\n",
      "         0.0789],\n",
      "        [0.0139, 0.0167, 0.0381, 0.0149, 0.0064, 0.0290, 0.0387, 0.0761, 0.0320,\n",
      "         0.0369, 0.0269, 0.1158, 0.0209, 0.0863, 0.0046, 0.0878, 0.2473, 0.0140,\n",
      "         0.0937],\n",
      "        [0.0408, 0.0371, 0.0380, 0.0163, 0.0061, 0.0588, 0.0847, 0.0718, 0.0837,\n",
      "         0.0074, 0.0231, 0.0720, 0.0616, 0.1930, 0.0061, 0.0087, 0.0369, 0.0390,\n",
      "         0.1151],\n",
      "        [0.0520, 0.0160, 0.2036, 0.0133, 0.0322, 0.0127, 0.0503, 0.0359, 0.0236,\n",
      "         0.0353, 0.0210, 0.0388, 0.1014, 0.1099, 0.0022, 0.0411, 0.0255, 0.0315,\n",
      "         0.1536],\n",
      "        [0.0202, 0.0119, 0.0227, 0.0121, 0.0087, 0.0422, 0.0963, 0.0783, 0.2652,\n",
      "         0.0246, 0.0246, 0.0104, 0.0698, 0.2112, 0.0080, 0.0166, 0.0344, 0.0260,\n",
      "         0.0170],\n",
      "        [0.0287, 0.0043, 0.0231, 0.0566, 0.0170, 0.1117, 0.0639, 0.1004, 0.0084,\n",
      "         0.0652, 0.0712, 0.0580, 0.0380, 0.0836, 0.0049, 0.0243, 0.0274, 0.0381,\n",
      "         0.1750],\n",
      "        [0.0209, 0.0212, 0.0061, 0.0216, 0.0947, 0.0356, 0.0135, 0.0223, 0.0096,\n",
      "         0.0105, 0.0486, 0.0349, 0.0268, 0.3577, 0.0198, 0.0379, 0.1265, 0.0080,\n",
      "         0.0837],\n",
      "        [0.0396, 0.0148, 0.0561, 0.0212, 0.0155, 0.0611, 0.2981, 0.1009, 0.0088,\n",
      "         0.0494, 0.0188, 0.0272, 0.0392, 0.1408, 0.0074, 0.0047, 0.0216, 0.0397,\n",
      "         0.0350],\n",
      "        [0.0266, 0.0053, 0.0276, 0.0131, 0.0085, 0.0535, 0.0137, 0.0359, 0.0797,\n",
      "         0.2819, 0.0775, 0.0195, 0.0238, 0.1748, 0.0082, 0.0194, 0.0387, 0.0340,\n",
      "         0.0583],\n",
      "        [0.0123, 0.0151, 0.0130, 0.0203, 0.0108, 0.0214, 0.0353, 0.0403, 0.0064,\n",
      "         0.0917, 0.0270, 0.0572, 0.0372, 0.4995, 0.0044, 0.0184, 0.0268, 0.0255,\n",
      "         0.0372],\n",
      "        [0.0147, 0.0126, 0.0120, 0.0216, 0.0061, 0.1847, 0.0220, 0.0229, 0.0433,\n",
      "         0.0144, 0.0503, 0.0136, 0.0389, 0.2935, 0.0090, 0.0437, 0.0757, 0.0258,\n",
      "         0.0953],\n",
      "        [0.0223, 0.0435, 0.0117, 0.0162, 0.0455, 0.0694, 0.1404, 0.0343, 0.0114,\n",
      "         0.0873, 0.0742, 0.0230, 0.0261, 0.0509, 0.0026, 0.0167, 0.0994, 0.1688,\n",
      "         0.0562]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 82 - Output:\n",
      "torch.Size([10, 10])\n",
      "1\n",
      "tensor([[0.0899, 0.0859, 0.0592, 0.1089, 0.1749, 0.0080, 0.2594, 0.1020, 0.0957,\n",
      "         0.0162],\n",
      "        [0.0254, 0.0683, 0.1146, 0.1257, 0.1687, 0.0407, 0.1055, 0.1187, 0.2257,\n",
      "         0.0067],\n",
      "        [0.2237, 0.0431, 0.0483, 0.1332, 0.2080, 0.0062, 0.0535, 0.1415, 0.0950,\n",
      "         0.0475],\n",
      "        [0.0686, 0.0571, 0.0386, 0.0972, 0.3478, 0.0561, 0.1843, 0.0754, 0.0686,\n",
      "         0.0061],\n",
      "        [0.1097, 0.0318, 0.1509, 0.0777, 0.0674, 0.0083, 0.1810, 0.1932, 0.1586,\n",
      "         0.0213],\n",
      "        [0.1282, 0.0437, 0.0366, 0.1286, 0.2734, 0.0098, 0.0958, 0.0948, 0.1714,\n",
      "         0.0176],\n",
      "        [0.3391, 0.0497, 0.0794, 0.1313, 0.1094, 0.0089, 0.0925, 0.0704, 0.1040,\n",
      "         0.0153],\n",
      "        [0.0208, 0.0499, 0.0959, 0.0934, 0.3102, 0.0158, 0.2247, 0.1049, 0.0691,\n",
      "         0.0152],\n",
      "        [0.1146, 0.1095, 0.0410, 0.0954, 0.0794, 0.0056, 0.1661, 0.1602, 0.1904,\n",
      "         0.0377],\n",
      "        [0.1620, 0.0577, 0.0790, 0.1348, 0.3055, 0.0065, 0.0772, 0.0721, 0.0794,\n",
      "         0.0258]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 83 - Output:\n",
      "torch.Size([12, 12])\n",
      "1\n",
      "tensor([[0.0331, 0.0215, 0.0614, 0.3233, 0.1036, 0.1957, 0.1231, 0.0204, 0.0129,\n",
      "         0.0162, 0.0267, 0.0621],\n",
      "        [0.0350, 0.0224, 0.1575, 0.1228, 0.0796, 0.1545, 0.1281, 0.1120, 0.0149,\n",
      "         0.0074, 0.1334, 0.0324],\n",
      "        [0.0172, 0.0102, 0.1026, 0.2467, 0.2470, 0.0148, 0.0897, 0.1053, 0.0433,\n",
      "         0.0536, 0.0332, 0.0365],\n",
      "        [0.0598, 0.0110, 0.0283, 0.0195, 0.5879, 0.0245, 0.0212, 0.0520, 0.0390,\n",
      "         0.0206, 0.1091, 0.0271],\n",
      "        [0.0172, 0.0086, 0.0412, 0.0454, 0.1092, 0.3734, 0.1964, 0.0474, 0.0215,\n",
      "         0.0310, 0.0719, 0.0369],\n",
      "        [0.0298, 0.0242, 0.0187, 0.0362, 0.5414, 0.0773, 0.0289, 0.0546, 0.0285,\n",
      "         0.0100, 0.1268, 0.0236],\n",
      "        [0.0437, 0.0324, 0.0201, 0.0158, 0.6316, 0.0113, 0.0467, 0.0513, 0.0429,\n",
      "         0.0158, 0.0245, 0.0640],\n",
      "        [0.0276, 0.0727, 0.2618, 0.0197, 0.1057, 0.3315, 0.0567, 0.0426, 0.0192,\n",
      "         0.0268, 0.0132, 0.0224],\n",
      "        [0.0326, 0.0177, 0.0418, 0.0518, 0.0182, 0.3503, 0.0281, 0.1282, 0.0422,\n",
      "         0.0105, 0.2168, 0.0617],\n",
      "        [0.0294, 0.0151, 0.1330, 0.0461, 0.4663, 0.1464, 0.0351, 0.0411, 0.0199,\n",
      "         0.0192, 0.0320, 0.0165],\n",
      "        [0.0334, 0.0066, 0.2890, 0.0795, 0.0273, 0.1362, 0.0701, 0.1799, 0.0752,\n",
      "         0.0279, 0.0548, 0.0200],\n",
      "        [0.0220, 0.0197, 0.1477, 0.0265, 0.2320, 0.3590, 0.0477, 0.0170, 0.0300,\n",
      "         0.0185, 0.0504, 0.0295]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 84 - Output:\n",
      "torch.Size([12, 12])\n",
      "1\n",
      "tensor([[0.0908, 0.1286, 0.0495, 0.0480, 0.0081, 0.1098, 0.0945, 0.1975, 0.0366,\n",
      "         0.0081, 0.0763, 0.1521],\n",
      "        [0.0297, 0.0991, 0.0710, 0.0396, 0.0045, 0.0883, 0.1693, 0.2628, 0.0322,\n",
      "         0.0353, 0.0856, 0.0826],\n",
      "        [0.0350, 0.0380, 0.0779, 0.0068, 0.0168, 0.2002, 0.1055, 0.1825, 0.0585,\n",
      "         0.0229, 0.1447, 0.1111],\n",
      "        [0.0323, 0.0795, 0.0350, 0.0417, 0.0067, 0.0719, 0.1224, 0.1912, 0.0218,\n",
      "         0.0246, 0.0520, 0.3208],\n",
      "        [0.0196, 0.0529, 0.0549, 0.0306, 0.0287, 0.0591, 0.1094, 0.3380, 0.0276,\n",
      "         0.0073, 0.0565, 0.2154],\n",
      "        [0.0831, 0.0801, 0.0454, 0.0340, 0.0034, 0.1337, 0.1100, 0.1840, 0.0416,\n",
      "         0.1256, 0.0994, 0.0597],\n",
      "        [0.0353, 0.1695, 0.0443, 0.0367, 0.0135, 0.1031, 0.0453, 0.2092, 0.0708,\n",
      "         0.0066, 0.1123, 0.1535],\n",
      "        [0.0268, 0.0420, 0.0848, 0.0329, 0.0085, 0.1056, 0.3471, 0.0834, 0.0525,\n",
      "         0.0109, 0.0725, 0.1330],\n",
      "        [0.0326, 0.0992, 0.0677, 0.0149, 0.0089, 0.1157, 0.3484, 0.0973, 0.0279,\n",
      "         0.0185, 0.0799, 0.0889],\n",
      "        [0.0171, 0.1330, 0.0241, 0.0101, 0.0169, 0.0534, 0.1954, 0.1473, 0.0573,\n",
      "         0.0381, 0.1247, 0.1828],\n",
      "        [0.0824, 0.0743, 0.0654, 0.0281, 0.0048, 0.0782, 0.1194, 0.2428, 0.0571,\n",
      "         0.0177, 0.0875, 0.1423],\n",
      "        [0.0364, 0.1394, 0.0602, 0.0050, 0.0280, 0.0787, 0.2367, 0.1098, 0.0308,\n",
      "         0.0299, 0.0891, 0.1558]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 85 - Output:\n",
      "torch.Size([8, 8])\n",
      "1\n",
      "tensor([[0.1583, 0.0193, 0.0302, 0.0853, 0.4931, 0.0439, 0.1349, 0.0350],\n",
      "        [0.2174, 0.0619, 0.0567, 0.1061, 0.3650, 0.0098, 0.0974, 0.0857],\n",
      "        [0.1150, 0.0113, 0.0619, 0.1292, 0.3493, 0.0351, 0.0935, 0.2045],\n",
      "        [0.0853, 0.0593, 0.0496, 0.0517, 0.6320, 0.0196, 0.0224, 0.0801],\n",
      "        [0.1486, 0.0258, 0.2595, 0.2595, 0.1117, 0.0132, 0.1207, 0.0609],\n",
      "        [0.3612, 0.1022, 0.0461, 0.0368, 0.2854, 0.1202, 0.0273, 0.0207],\n",
      "        [0.2088, 0.0668, 0.0877, 0.0909, 0.2720, 0.0084, 0.1166, 0.1488],\n",
      "        [0.1945, 0.0225, 0.0236, 0.0784, 0.3968, 0.0298, 0.1689, 0.0856]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 86 - Output:\n",
      "torch.Size([8, 8])\n",
      "1\n",
      "tensor([[0.2074, 0.0558, 0.3879, 0.1350, 0.0318, 0.0395, 0.1268, 0.0158],\n",
      "        [0.2278, 0.0296, 0.1557, 0.2495, 0.0221, 0.1392, 0.1552, 0.0210],\n",
      "        [0.1298, 0.0870, 0.3117, 0.3094, 0.0188, 0.0639, 0.0586, 0.0207],\n",
      "        [0.2536, 0.0213, 0.2363, 0.1227, 0.0653, 0.1112, 0.1747, 0.0149],\n",
      "        [0.3394, 0.0198, 0.1907, 0.1408, 0.0181, 0.1165, 0.1290, 0.0458],\n",
      "        [0.5227, 0.0313, 0.0888, 0.1360, 0.0253, 0.1297, 0.0421, 0.0241],\n",
      "        [0.2182, 0.0197, 0.2380, 0.2216, 0.0179, 0.0737, 0.1552, 0.0557],\n",
      "        [0.4000, 0.0248, 0.1545, 0.2669, 0.0397, 0.0396, 0.0415, 0.0330]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 87 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.1492, 0.0490, 0.0157, 0.0564, 0.2035, 0.0993, 0.4270],\n",
      "        [0.1913, 0.1535, 0.0678, 0.0153, 0.1814, 0.0411, 0.3496],\n",
      "        [0.2493, 0.1260, 0.0758, 0.0347, 0.3017, 0.0165, 0.1959],\n",
      "        [0.0509, 0.1702, 0.1556, 0.0143, 0.1503, 0.0630, 0.3957],\n",
      "        [0.0873, 0.2609, 0.0461, 0.0297, 0.2365, 0.0246, 0.3150],\n",
      "        [0.1746, 0.0890, 0.0296, 0.1175, 0.2160, 0.0179, 0.3554],\n",
      "        [0.1167, 0.1019, 0.0343, 0.0182, 0.2055, 0.0653, 0.4581]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 88 - Output:\n",
      "torch.Size([15, 15])\n",
      "1\n",
      "tensor([[0.0236, 0.1223, 0.0245, 0.0375, 0.0205, 0.0288, 0.0165, 0.0302, 0.0487,\n",
      "         0.0167, 0.0883, 0.2151, 0.0056, 0.2576, 0.0640],\n",
      "        [0.0128, 0.2323, 0.0497, 0.0262, 0.0130, 0.0486, 0.0059, 0.0328, 0.0546,\n",
      "         0.0461, 0.1070, 0.1131, 0.0159, 0.1193, 0.1227],\n",
      "        [0.0192, 0.1499, 0.0140, 0.0366, 0.0061, 0.0583, 0.0106, 0.0329, 0.0484,\n",
      "         0.0329, 0.1941, 0.0913, 0.0414, 0.2192, 0.0451],\n",
      "        [0.0183, 0.2361, 0.0197, 0.0264, 0.0165, 0.0334, 0.0115, 0.0321, 0.0725,\n",
      "         0.0241, 0.2242, 0.0715, 0.0084, 0.0980, 0.1073],\n",
      "        [0.0361, 0.0467, 0.0183, 0.0201, 0.0213, 0.0393, 0.0085, 0.0379, 0.0173,\n",
      "         0.0191, 0.0865, 0.1309, 0.0094, 0.4623, 0.0463],\n",
      "        [0.0189, 0.1130, 0.0223, 0.0214, 0.0124, 0.0274, 0.0119, 0.0182, 0.0232,\n",
      "         0.0157, 0.0645, 0.1321, 0.0186, 0.4698, 0.0306],\n",
      "        [0.0312, 0.2970, 0.0294, 0.0405, 0.0071, 0.0360, 0.0057, 0.0272, 0.0429,\n",
      "         0.0480, 0.1362, 0.1463, 0.0264, 0.0780, 0.0481],\n",
      "        [0.0157, 0.1134, 0.0146, 0.0299, 0.0150, 0.0488, 0.0146, 0.0477, 0.0331,\n",
      "         0.0191, 0.2527, 0.1183, 0.0110, 0.2283, 0.0377],\n",
      "        [0.0174, 0.1514, 0.0291, 0.0263, 0.0092, 0.0545, 0.0101, 0.0247, 0.0376,\n",
      "         0.0315, 0.0821, 0.1230, 0.0127, 0.3532, 0.0372],\n",
      "        [0.0305, 0.2544, 0.0270, 0.0250, 0.0126, 0.0543, 0.0070, 0.0288, 0.0386,\n",
      "         0.0313, 0.0929, 0.1747, 0.0109, 0.0663, 0.1455],\n",
      "        [0.0278, 0.0990, 0.0279, 0.0202, 0.0090, 0.0385, 0.0098, 0.0322, 0.0602,\n",
      "         0.0152, 0.0410, 0.1152, 0.0127, 0.4482, 0.0431],\n",
      "        [0.0221, 0.1481, 0.0222, 0.0177, 0.0245, 0.0309, 0.0076, 0.0224, 0.0491,\n",
      "         0.0571, 0.2025, 0.1950, 0.0114, 0.1388, 0.0508],\n",
      "        [0.0157, 0.0399, 0.0362, 0.0412, 0.0070, 0.0354, 0.0244, 0.0236, 0.0164,\n",
      "         0.0407, 0.1556, 0.3417, 0.0128, 0.1659, 0.0433],\n",
      "        [0.0304, 0.0696, 0.0175, 0.0313, 0.0111, 0.0193, 0.0069, 0.0474, 0.0308,\n",
      "         0.0158, 0.0774, 0.0862, 0.0179, 0.4940, 0.0442],\n",
      "        [0.0160, 0.0575, 0.0180, 0.0356, 0.0172, 0.0308, 0.0139, 0.0230, 0.0333,\n",
      "         0.0187, 0.0511, 0.2253, 0.0084, 0.4146, 0.0365]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 89 - Output:\n",
      "torch.Size([18, 18])\n",
      "1\n",
      "tensor([[0.0209, 0.0237, 0.0074, 0.0367, 0.0544, 0.0257, 0.0132, 0.0965, 0.0519,\n",
      "         0.0527, 0.0446, 0.4272, 0.0262, 0.0152, 0.0041, 0.0599, 0.0170, 0.0228],\n",
      "        [0.0299, 0.0517, 0.0118, 0.0075, 0.0275, 0.0525, 0.0281, 0.1874, 0.0627,\n",
      "         0.0549, 0.0361, 0.0128, 0.1769, 0.0038, 0.0562, 0.0787, 0.0326, 0.0890],\n",
      "        [0.0656, 0.0700, 0.0174, 0.0165, 0.0151, 0.0527, 0.0126, 0.1923, 0.1562,\n",
      "         0.0596, 0.0059, 0.0265, 0.1935, 0.0213, 0.0103, 0.0228, 0.0424, 0.0194],\n",
      "        [0.0646, 0.0053, 0.0108, 0.0317, 0.0180, 0.0138, 0.0159, 0.0491, 0.0221,\n",
      "         0.0726, 0.0273, 0.0703, 0.1514, 0.0126, 0.3017, 0.0715, 0.0155, 0.0456],\n",
      "        [0.0183, 0.0864, 0.0076, 0.0039, 0.0185, 0.0321, 0.0575, 0.1332, 0.0400,\n",
      "         0.0737, 0.0213, 0.2234, 0.0732, 0.0344, 0.0106, 0.0707, 0.0555, 0.0396],\n",
      "        [0.0306, 0.1071, 0.0631, 0.0252, 0.0157, 0.0419, 0.0644, 0.1020, 0.2874,\n",
      "         0.0475, 0.0259, 0.0344, 0.0357, 0.0069, 0.0106, 0.0784, 0.0045, 0.0189],\n",
      "        [0.0445, 0.1101, 0.0390, 0.0420, 0.0794, 0.0329, 0.0224, 0.1844, 0.0408,\n",
      "         0.0199, 0.0186, 0.0236, 0.0065, 0.0057, 0.0100, 0.0617, 0.0226, 0.2361],\n",
      "        [0.0998, 0.1140, 0.0280, 0.0118, 0.0213, 0.0324, 0.0083, 0.0799, 0.0789,\n",
      "         0.0388, 0.0359, 0.0965, 0.1448, 0.0129, 0.0088, 0.0588, 0.0069, 0.1224],\n",
      "        [0.0688, 0.2012, 0.0181, 0.0151, 0.0417, 0.0332, 0.0287, 0.0732, 0.1006,\n",
      "         0.0217, 0.0171, 0.2026, 0.0654, 0.0134, 0.0102, 0.0727, 0.0082, 0.0082],\n",
      "        [0.0500, 0.0616, 0.0034, 0.0115, 0.0336, 0.0633, 0.0399, 0.1104, 0.1240,\n",
      "         0.0614, 0.0217, 0.0536, 0.0460, 0.0124, 0.0136, 0.0998, 0.0136, 0.1801],\n",
      "        [0.0126, 0.0345, 0.0302, 0.0266, 0.0280, 0.0390, 0.0398, 0.1631, 0.0723,\n",
      "         0.0543, 0.0408, 0.0275, 0.2073, 0.0081, 0.0404, 0.0455, 0.0028, 0.1271],\n",
      "        [0.0146, 0.0461, 0.0151, 0.0044, 0.0506, 0.0315, 0.0144, 0.2632, 0.0274,\n",
      "         0.0240, 0.0372, 0.0256, 0.0232, 0.0165, 0.2295, 0.1088, 0.0141, 0.0540],\n",
      "        [0.2584, 0.0683, 0.0123, 0.0143, 0.0075, 0.0486, 0.0187, 0.0257, 0.0237,\n",
      "         0.0477, 0.0804, 0.0162, 0.0289, 0.0067, 0.1863, 0.0764, 0.0175, 0.0625],\n",
      "        [0.0309, 0.1031, 0.0114, 0.0770, 0.0304, 0.0786, 0.0159, 0.0949, 0.0264,\n",
      "         0.0998, 0.0505, 0.0386, 0.1913, 0.0059, 0.0043, 0.0416, 0.0240, 0.0755],\n",
      "        [0.0674, 0.1560, 0.0125, 0.0173, 0.0560, 0.0459, 0.0524, 0.0599, 0.0316,\n",
      "         0.0170, 0.0076, 0.1231, 0.0307, 0.0040, 0.0655, 0.0255, 0.0269, 0.2010],\n",
      "        [0.1023, 0.0229, 0.0206, 0.0127, 0.0288, 0.0243, 0.0293, 0.2500, 0.0302,\n",
      "         0.0144, 0.0324, 0.0864, 0.1364, 0.0079, 0.0450, 0.0567, 0.0046, 0.0951],\n",
      "        [0.1982, 0.0429, 0.0052, 0.0388, 0.0585, 0.0249, 0.0383, 0.0572, 0.1345,\n",
      "         0.0151, 0.0116, 0.0306, 0.0506, 0.0203, 0.0091, 0.0743, 0.0131, 0.1765],\n",
      "        [0.1356, 0.2017, 0.0154, 0.0063, 0.0116, 0.0128, 0.0594, 0.1136, 0.1374,\n",
      "         0.0501, 0.0360, 0.0593, 0.0136, 0.0143, 0.0238, 0.0217, 0.0166, 0.0708]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 90 - Output:\n",
      "torch.Size([4, 4])\n",
      "1\n",
      "tensor([[0.2117, 0.3418, 0.0331, 0.4134],\n",
      "        [0.1590, 0.5789, 0.0359, 0.2261],\n",
      "        [0.2584, 0.4661, 0.0330, 0.2425],\n",
      "        [0.3066, 0.4142, 0.0326, 0.2466]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 91 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 92 - Output:\n",
      "torch.Size([10, 10])\n",
      "1\n",
      "tensor([[0.0607, 0.1154, 0.2699, 0.0207, 0.0957, 0.0928, 0.0346, 0.0095, 0.0508,\n",
      "         0.2499],\n",
      "        [0.2439, 0.0550, 0.4485, 0.0289, 0.0218, 0.0354, 0.0807, 0.0370, 0.0181,\n",
      "         0.0306],\n",
      "        [0.1431, 0.1516, 0.3505, 0.0202, 0.0245, 0.0591, 0.0292, 0.0523, 0.0159,\n",
      "         0.1536],\n",
      "        [0.0799, 0.2552, 0.1319, 0.1178, 0.0755, 0.1603, 0.0224, 0.1204, 0.0086,\n",
      "         0.0280],\n",
      "        [0.3828, 0.1408, 0.0192, 0.0621, 0.0398, 0.0219, 0.0634, 0.0175, 0.0408,\n",
      "         0.2117],\n",
      "        [0.6205, 0.0730, 0.0485, 0.0128, 0.0321, 0.0218, 0.0390, 0.0475, 0.0254,\n",
      "         0.0794],\n",
      "        [0.1416, 0.0235, 0.2197, 0.0475, 0.2281, 0.0299, 0.0862, 0.0090, 0.0876,\n",
      "         0.1269],\n",
      "        [0.4532, 0.0899, 0.0893, 0.0370, 0.0295, 0.0623, 0.0154, 0.0282, 0.0196,\n",
      "         0.1756],\n",
      "        [0.0987, 0.0981, 0.0451, 0.0109, 0.2100, 0.1524, 0.1363, 0.0482, 0.0142,\n",
      "         0.1862],\n",
      "        [0.0355, 0.1815, 0.1790, 0.0471, 0.0243, 0.1363, 0.0480, 0.0211, 0.0156,\n",
      "         0.3116]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 93 - Output:\n",
      "torch.Size([3, 3])\n",
      "1\n",
      "tensor([[0.7815, 0.0719, 0.1466],\n",
      "        [0.8054, 0.0897, 0.1049],\n",
      "        [0.5519, 0.0569, 0.3912]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 94 - Output:\n",
      "torch.Size([1, 1])\n",
      "1\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 95 - Output:\n",
      "torch.Size([17, 17])\n",
      "1\n",
      "tensor([[0.1050, 0.0311, 0.0062, 0.0596, 0.0311, 0.1764, 0.0297, 0.0131, 0.1765,\n",
      "         0.0369, 0.0299, 0.0745, 0.0900, 0.0421, 0.0050, 0.0756, 0.0173],\n",
      "        [0.0596, 0.0408, 0.0067, 0.0674, 0.0513, 0.1891, 0.0131, 0.0146, 0.2374,\n",
      "         0.0202, 0.0368, 0.1038, 0.0653, 0.0465, 0.0134, 0.0258, 0.0082],\n",
      "        [0.0498, 0.0158, 0.0061, 0.0684, 0.0471, 0.1557, 0.0672, 0.0195, 0.1879,\n",
      "         0.0359, 0.0279, 0.1442, 0.0559, 0.0740, 0.0069, 0.0268, 0.0109],\n",
      "        [0.0285, 0.0111, 0.0099, 0.0779, 0.0310, 0.3050, 0.0602, 0.0175, 0.0873,\n",
      "         0.0268, 0.0223, 0.1468, 0.0834, 0.0215, 0.0072, 0.0513, 0.0124],\n",
      "        [0.0556, 0.0331, 0.0097, 0.0672, 0.0277, 0.1395, 0.0686, 0.0111, 0.0717,\n",
      "         0.0208, 0.0496, 0.2723, 0.0547, 0.0593, 0.0050, 0.0441, 0.0099],\n",
      "        [0.2876, 0.0289, 0.0090, 0.0770, 0.0404, 0.0997, 0.0239, 0.0151, 0.0210,\n",
      "         0.0260, 0.0267, 0.0979, 0.1128, 0.0935, 0.0077, 0.0235, 0.0092],\n",
      "        [0.1770, 0.0212, 0.0135, 0.0803, 0.0564, 0.1380, 0.0200, 0.0059, 0.0848,\n",
      "         0.0201, 0.0447, 0.0740, 0.1349, 0.0550, 0.0163, 0.0507, 0.0070],\n",
      "        [0.1443, 0.0517, 0.0075, 0.1035, 0.0455, 0.1681, 0.0204, 0.0100, 0.0855,\n",
      "         0.0623, 0.0612, 0.0831, 0.0688, 0.0234, 0.0090, 0.0480, 0.0076],\n",
      "        [0.1410, 0.0097, 0.0092, 0.0322, 0.0686, 0.1667, 0.0587, 0.0120, 0.0570,\n",
      "         0.0332, 0.0414, 0.1179, 0.0660, 0.1367, 0.0080, 0.0301, 0.0116],\n",
      "        [0.1185, 0.0268, 0.0041, 0.0354, 0.0351, 0.0390, 0.0728, 0.0137, 0.1491,\n",
      "         0.0379, 0.0361, 0.1334, 0.0844, 0.1197, 0.0062, 0.0635, 0.0241],\n",
      "        [0.0420, 0.0225, 0.0104, 0.0273, 0.0417, 0.2247, 0.0206, 0.0119, 0.1905,\n",
      "         0.0451, 0.0267, 0.0633, 0.0795, 0.1099, 0.0041, 0.0565, 0.0234],\n",
      "        [0.0396, 0.0270, 0.0113, 0.1796, 0.0289, 0.2629, 0.0272, 0.0069, 0.0785,\n",
      "         0.0273, 0.0237, 0.0916, 0.0781, 0.0500, 0.0103, 0.0486, 0.0086],\n",
      "        [0.0373, 0.0339, 0.0143, 0.0506, 0.0585, 0.0328, 0.0519, 0.0200, 0.0854,\n",
      "         0.0390, 0.0264, 0.1835, 0.2464, 0.0297, 0.0029, 0.0751, 0.0123],\n",
      "        [0.0332, 0.0266, 0.0129, 0.0674, 0.0484, 0.1404, 0.0790, 0.0214, 0.0861,\n",
      "         0.0203, 0.0388, 0.2187, 0.0884, 0.0483, 0.0030, 0.0540, 0.0130],\n",
      "        [0.2818, 0.0220, 0.0098, 0.0321, 0.0239, 0.1867, 0.0646, 0.0137, 0.1293,\n",
      "         0.0260, 0.0489, 0.0215, 0.0560, 0.0333, 0.0067, 0.0333, 0.0104],\n",
      "        [0.0770, 0.0489, 0.0040, 0.0926, 0.0273, 0.1643, 0.0235, 0.0261, 0.0869,\n",
      "         0.0460, 0.0363, 0.1396, 0.0797, 0.0890, 0.0068, 0.0403, 0.0117],\n",
      "        [0.0888, 0.0144, 0.0091, 0.0694, 0.0457, 0.1775, 0.0392, 0.0194, 0.0398,\n",
      "         0.0129, 0.0316, 0.1432, 0.2197, 0.0194, 0.0115, 0.0497, 0.0089]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 96 - Output:\n",
      "torch.Size([5, 5])\n",
      "1\n",
      "tensor([[0.0723, 0.0609, 0.2284, 0.5996, 0.0387],\n",
      "        [0.2681, 0.1004, 0.3733, 0.2350, 0.0232],\n",
      "        [0.1719, 0.1148, 0.5185, 0.1713, 0.0236],\n",
      "        [0.1817, 0.1029, 0.2212, 0.4706, 0.0237],\n",
      "        [0.0383, 0.1610, 0.3905, 0.3662, 0.0440]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 97 - Output:\n",
      "torch.Size([13, 13])\n",
      "1\n",
      "tensor([[0.0315, 0.2702, 0.0481, 0.0794, 0.0045, 0.1074, 0.0264, 0.1060, 0.0691,\n",
      "         0.0757, 0.1334, 0.0207, 0.0275],\n",
      "        [0.0221, 0.2026, 0.0389, 0.0940, 0.0048, 0.0557, 0.0238, 0.0499, 0.0319,\n",
      "         0.3239, 0.0684, 0.0457, 0.0383],\n",
      "        [0.0281, 0.2432, 0.0571, 0.0522, 0.0038, 0.1530, 0.0855, 0.1144, 0.0501,\n",
      "         0.0432, 0.1099, 0.0252, 0.0343],\n",
      "        [0.0307, 0.1218, 0.0221, 0.1781, 0.0074, 0.0740, 0.0473, 0.0856, 0.0300,\n",
      "         0.2828, 0.0819, 0.0144, 0.0240],\n",
      "        [0.0230, 0.1595, 0.0232, 0.0774, 0.0168, 0.1271, 0.1556, 0.1958, 0.0300,\n",
      "         0.0129, 0.1378, 0.0259, 0.0151],\n",
      "        [0.0459, 0.0966, 0.0246, 0.1016, 0.0167, 0.0840, 0.0209, 0.0540, 0.0082,\n",
      "         0.4456, 0.0231, 0.0595, 0.0193],\n",
      "        [0.0308, 0.2696, 0.0417, 0.0857, 0.0044, 0.0557, 0.0527, 0.2043, 0.0606,\n",
      "         0.0786, 0.0712, 0.0230, 0.0216],\n",
      "        [0.0460, 0.1839, 0.0301, 0.0636, 0.0054, 0.0710, 0.0548, 0.0611, 0.0170,\n",
      "         0.0249, 0.3549, 0.0599, 0.0273],\n",
      "        [0.0374, 0.3310, 0.0515, 0.1052, 0.0047, 0.0995, 0.0268, 0.0487, 0.0219,\n",
      "         0.1006, 0.0962, 0.0552, 0.0214],\n",
      "        [0.0233, 0.0935, 0.0429, 0.3906, 0.0052, 0.0618, 0.0221, 0.0480, 0.0349,\n",
      "         0.0792, 0.1370, 0.0381, 0.0235],\n",
      "        [0.0330, 0.0551, 0.0240, 0.2090, 0.0039, 0.1377, 0.1295, 0.0849, 0.0261,\n",
      "         0.1446, 0.0456, 0.0520, 0.0546],\n",
      "        [0.0230, 0.3694, 0.0221, 0.0374, 0.0135, 0.1365, 0.0299, 0.1104, 0.0383,\n",
      "         0.0746, 0.1142, 0.0200, 0.0107],\n",
      "        [0.0375, 0.1240, 0.0335, 0.1772, 0.0129, 0.0920, 0.0363, 0.1307, 0.0067,\n",
      "         0.1597, 0.1339, 0.0244, 0.0312]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 98 - Output:\n",
      "torch.Size([9, 9])\n",
      "1\n",
      "tensor([[0.2019, 0.0184, 0.0753, 0.0107, 0.0959, 0.2877, 0.0970, 0.0872, 0.1261],\n",
      "        [0.0746, 0.0337, 0.0915, 0.0084, 0.1324, 0.2623, 0.0523, 0.2483, 0.0966],\n",
      "        [0.4935, 0.0335, 0.0253, 0.0131, 0.0963, 0.1297, 0.0469, 0.0566, 0.1052],\n",
      "        [0.5170, 0.0177, 0.0366, 0.0201, 0.0826, 0.1686, 0.0453, 0.0404, 0.0715],\n",
      "        [0.1726, 0.0463, 0.0397, 0.0078, 0.1064, 0.3144, 0.1195, 0.0858, 0.1076],\n",
      "        [0.1658, 0.0214, 0.0759, 0.0103, 0.1057, 0.2164, 0.0867, 0.2587, 0.0591],\n",
      "        [0.1655, 0.0192, 0.0399, 0.0138, 0.2637, 0.2238, 0.0747, 0.0584, 0.1411],\n",
      "        [0.1909, 0.0521, 0.2112, 0.0073, 0.0830, 0.1406, 0.0538, 0.0611, 0.1998],\n",
      "        [0.3190, 0.0626, 0.0294, 0.0083, 0.0973, 0.1508, 0.0787, 0.1537, 0.1002]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 99 - Output:\n",
      "torch.Size([7, 7])\n",
      "1\n",
      "tensor([[0.2043, 0.1646, 0.0132, 0.0527, 0.3347, 0.1558, 0.0747],\n",
      "        [0.2987, 0.1327, 0.1030, 0.0499, 0.3293, 0.0720, 0.0145],\n",
      "        [0.2930, 0.1018, 0.0410, 0.1077, 0.4003, 0.0282, 0.0280],\n",
      "        [0.2733, 0.1509, 0.0202, 0.2333, 0.1948, 0.1041, 0.0234],\n",
      "        [0.2169, 0.0768, 0.0286, 0.0996, 0.4764, 0.0805, 0.0212],\n",
      "        [0.2835, 0.0847, 0.0140, 0.0963, 0.3168, 0.1554, 0.0492],\n",
      "        [0.2135, 0.1081, 0.0247, 0.0809, 0.2893, 0.2620, 0.0214]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 100 - Output:\n",
      "torch.Size([6, 6])\n",
      "1\n",
      "tensor([[0.4404, 0.0529, 0.2955, 0.0228, 0.1069, 0.0816],\n",
      "        [0.2061, 0.0306, 0.5686, 0.0381, 0.0788, 0.0778],\n",
      "        [0.0604, 0.7302, 0.0478, 0.0752, 0.0432, 0.0432],\n",
      "        [0.0754, 0.0437, 0.1129, 0.0253, 0.1557, 0.5870],\n",
      "        [0.5604, 0.0285, 0.1103, 0.0355, 0.0854, 0.1799],\n",
      "        [0.1725, 0.0839, 0.0747, 0.0183, 0.2962, 0.3544]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "gat_models = []\n",
    "for graph_idx, (x, adj_tensor) in enumerate(graphs):\n",
    "    in_features = x.shape[1]\n",
    "    n_heads = adj_tensor.shape[2]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "    gat_models.append(gat_model)\n",
    "    x = x.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(x, adj_tensor)\n",
    "    print(f\"Graph {graph_idx+1} - Output:\")\n",
    "    print(output.shape)\n",
    "    #output : 각 노드에 대한 클래스 라벨 예측 값\n",
    "\n",
    "    # Generate v_prev tensor\n",
    "    v_i = torch.randn(1).cuda()\n",
    "    print(v_i.size(0))\n",
    "    # Generate neighbors tensor\n",
    "    v_j = torch.randn(1).cuda()\n",
    "\n",
    "    x, decode_output, attn_weights = gat_model.decode(output, v_i, v_j)\n",
    "    # print(f\"Graph {graph_idx+1} - Decode Output:\")\n",
    "    # print(decode_output)\n",
    "    # print(\"Attention Weights:\")\n",
    "    # print(attn_weights)\n",
    "    print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "https://chioni.github.io/posts/gat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0001"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[0.0128, 0.3112, 0.0174, 0.0471, 0.0988, 0.0335, 0.0245, 0.0602, 0.0088, 0.0746, 0.1281, 0.0120, 0.0783, 0.0139, 0.0789]\n",
    "sum(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1: Epoch: 001, Loss: -0.0254\n",
      "Graph 1: Epoch: 002, Loss: -0.1674\n",
      "Graph 1: Epoch: 003, Loss: -0.3029\n",
      "Graph 1: Epoch: 004, Loss: -0.3608\n",
      "Graph 1: Epoch: 005, Loss: -0.3142\n",
      "Graph 1: Epoch: 006, Loss: -0.4000\n",
      "Graph 1: Epoch: 007, Loss: -0.5569\n",
      "Graph 1: Epoch: 008, Loss: -0.4095\n",
      "Graph 1: Epoch: 009, Loss: -0.3047\n",
      "Graph 1: Epoch: 010, Loss: -0.4282\n",
      "Graph 1: Epoch: 011, Loss: -0.4983\n",
      "Graph 1: Epoch: 012, Loss: -0.6975\n",
      "Graph 1: Epoch: 013, Loss: -0.4644\n",
      "Graph 1: Epoch: 014, Loss: -0.5200\n",
      "Graph 1: Epoch: 015, Loss: -0.2665\n",
      "Graph 1: Epoch: 016, Loss: -0.6714\n",
      "Graph 1: Epoch: 017, Loss: -0.4025\n",
      "Graph 1: Epoch: 018, Loss: -0.4637\n",
      "Graph 1: Epoch: 019, Loss: -0.6675\n",
      "Graph 1: Epoch: 020, Loss: -0.4056\n",
      "Graph 1: Epoch: 021, Loss: -0.5430\n",
      "Graph 1: Epoch: 022, Loss: -0.5476\n",
      "Graph 1: Epoch: 023, Loss: -0.4104\n",
      "Graph 1: Epoch: 024, Loss: -0.6197\n",
      "Graph 1: Epoch: 025, Loss: -0.2773\n",
      "Graph 1: Epoch: 026, Loss: -0.2756\n",
      "Graph 1: Epoch: 027, Loss: -0.5531\n",
      "Graph 1: Epoch: 028, Loss: -0.3454\n",
      "Graph 1: Epoch: 029, Loss: -0.5513\n",
      "Graph 1: Epoch: 030, Loss: -0.2761\n",
      "Graph 1: Epoch: 031, Loss: -0.6228\n",
      "Graph 1: Epoch: 032, Loss: -0.4871\n",
      "Graph 1: Epoch: 033, Loss: -0.6961\n",
      "Graph 1: Epoch: 034, Loss: -0.6280\n",
      "Graph 1: Epoch: 035, Loss: -0.5584\n",
      "Graph 1: Epoch: 036, Loss: -0.4861\n",
      "Graph 1: Epoch: 037, Loss: -0.3499\n",
      "Graph 1: Epoch: 038, Loss: -0.5595\n",
      "Graph 1: Epoch: 039, Loss: -0.3501\n",
      "Graph 1: Epoch: 040, Loss: -0.4906\n",
      "Graph 1: Epoch: 041, Loss: -0.4214\n",
      "Graph 1: Epoch: 042, Loss: -0.4918\n",
      "Graph 1: Epoch: 043, Loss: -0.6318\n",
      "Graph 1: Epoch: 044, Loss: -0.4918\n",
      "Graph 1: Epoch: 045, Loss: -0.4212\n",
      "Graph 1: Epoch: 046, Loss: -0.5626\n",
      "Graph 1: Epoch: 047, Loss: -0.7035\n",
      "Graph 1: Epoch: 048, Loss: -0.4926\n",
      "Graph 1: Epoch: 049, Loss: -0.3522\n",
      "Graph 1: Epoch: 050, Loss: -0.4220\n",
      "Graph 1: Epoch: 051, Loss: -0.3525\n",
      "Graph 1: Epoch: 052, Loss: -0.3520\n",
      "Graph 1: Epoch: 053, Loss: -0.4937\n",
      "Graph 1: Epoch: 054, Loss: -0.6347\n",
      "Graph 1: Epoch: 055, Loss: -0.4233\n",
      "Graph 1: Epoch: 056, Loss: -0.3525\n",
      "Graph 1: Epoch: 057, Loss: -0.5646\n",
      "Graph 1: Epoch: 058, Loss: -0.4237\n",
      "Graph 1: Epoch: 059, Loss: -0.4941\n",
      "Graph 1: Epoch: 060, Loss: -0.4941\n",
      "Graph 1: Epoch: 061, Loss: -0.6349\n",
      "Graph 1: Epoch: 062, Loss: -0.4234\n",
      "Graph 1: Epoch: 063, Loss: -0.5638\n",
      "Graph 1: Epoch: 064, Loss: -0.7771\n",
      "Graph 1: Epoch: 065, Loss: -0.5655\n",
      "Graph 1: Epoch: 066, Loss: -0.3537\n",
      "Graph 1: Epoch: 067, Loss: -0.5656\n",
      "Graph 1: Epoch: 068, Loss: -0.5650\n",
      "Graph 1: Epoch: 069, Loss: -0.5657\n",
      "Graph 1: Epoch: 070, Loss: -0.6368\n",
      "Graph 1: Epoch: 071, Loss: -0.4955\n",
      "Graph 1: Epoch: 072, Loss: -0.7778\n",
      "Graph 1: Epoch: 073, Loss: -0.6370\n",
      "Graph 1: Epoch: 074, Loss: -0.5662\n",
      "Graph 1: Epoch: 075, Loss: -0.5665\n",
      "Graph 1: Epoch: 076, Loss: -0.2837\n",
      "Graph 1: Epoch: 077, Loss: -0.4250\n",
      "Graph 1: Epoch: 078, Loss: -0.5666\n",
      "Graph 1: Epoch: 079, Loss: -0.4959\n",
      "Graph 1: Epoch: 080, Loss: -0.4961\n",
      "Graph 1: Epoch: 081, Loss: -0.2127\n",
      "Graph 1: Epoch: 082, Loss: -0.4253\n",
      "Graph 1: Epoch: 083, Loss: -0.5669\n",
      "Graph 1: Epoch: 084, Loss: -0.7795\n",
      "Graph 1: Epoch: 085, Loss: -0.5668\n",
      "Graph 1: Epoch: 086, Loss: -0.4963\n",
      "Graph 1: Epoch: 087, Loss: -0.3788\n",
      "Graph 1: Epoch: 088, Loss: -0.5668\n",
      "Graph 1: Epoch: 089, Loss: -0.6336\n",
      "Graph 1: Epoch: 090, Loss: -0.4256\n",
      "Graph 1: Epoch: 091, Loss: -0.4961\n",
      "Graph 1: Epoch: 092, Loss: -0.4964\n",
      "Graph 1: Epoch: 093, Loss: -0.2837\n",
      "Graph 1: Epoch: 094, Loss: -0.4955\n",
      "Graph 1: Epoch: 095, Loss: -0.6340\n",
      "Graph 1: Epoch: 096, Loss: -0.6376\n",
      "Graph 1: Epoch: 097, Loss: -0.6368\n",
      "Graph 1: Epoch: 098, Loss: -0.3543\n",
      "Graph 1: Epoch: 099, Loss: -0.5673\n",
      "Graph 1: Epoch: 100, Loss: -0.6379\n",
      "Graph 2: Epoch: 001, Loss: -0.0317\n",
      "Graph 2: Epoch: 002, Loss: -0.1278\n",
      "Graph 2: Epoch: 003, Loss: -0.3099\n",
      "Graph 2: Epoch: 004, Loss: -0.3086\n",
      "Graph 2: Epoch: 005, Loss: -0.3227\n",
      "Graph 2: Epoch: 006, Loss: -0.3420\n",
      "Graph 2: Epoch: 007, Loss: -0.3696\n",
      "Graph 2: Epoch: 008, Loss: -0.3748\n",
      "Graph 2: Epoch: 009, Loss: -0.3308\n",
      "Graph 2: Epoch: 010, Loss: -0.4138\n",
      "Graph 2: Epoch: 011, Loss: -0.3885\n",
      "Graph 2: Epoch: 012, Loss: -0.4716\n",
      "Graph 2: Epoch: 013, Loss: -0.5010\n",
      "Graph 2: Epoch: 014, Loss: -0.3723\n",
      "Graph 2: Epoch: 015, Loss: -0.5863\n",
      "Graph 2: Epoch: 016, Loss: -0.4698\n",
      "Graph 2: Epoch: 017, Loss: -0.5446\n",
      "Graph 2: Epoch: 018, Loss: -0.6173\n",
      "Graph 2: Epoch: 019, Loss: -0.4486\n",
      "Graph 2: Epoch: 020, Loss: -0.3699\n",
      "Graph 2: Epoch: 021, Loss: -0.3827\n",
      "Graph 2: Epoch: 022, Loss: -0.5122\n",
      "Graph 2: Epoch: 023, Loss: -0.4496\n",
      "Graph 2: Epoch: 024, Loss: -0.5759\n",
      "Graph 2: Epoch: 025, Loss: -0.3896\n",
      "Graph 2: Epoch: 026, Loss: -0.4579\n",
      "Graph 2: Epoch: 027, Loss: -0.3239\n",
      "Graph 2: Epoch: 028, Loss: -0.4464\n",
      "Graph 2: Epoch: 029, Loss: -0.5825\n",
      "Graph 2: Epoch: 030, Loss: -0.3886\n",
      "Graph 2: Epoch: 031, Loss: -0.4544\n",
      "Graph 2: Epoch: 032, Loss: -0.4552\n",
      "Graph 2: Epoch: 033, Loss: -0.6504\n",
      "Graph 2: Epoch: 034, Loss: -0.4522\n",
      "Graph 2: Epoch: 035, Loss: -0.5877\n",
      "Graph 2: Epoch: 036, Loss: -0.5242\n",
      "Graph 2: Epoch: 037, Loss: -0.5895\n",
      "Graph 2: Epoch: 038, Loss: -0.7205\n",
      "Graph 2: Epoch: 039, Loss: -0.3286\n",
      "Graph 2: Epoch: 040, Loss: -0.5239\n",
      "Graph 2: Epoch: 041, Loss: -0.5255\n",
      "Graph 2: Epoch: 042, Loss: -0.3940\n",
      "Graph 2: Epoch: 043, Loss: -0.6568\n",
      "Graph 2: Epoch: 044, Loss: -0.2633\n",
      "Graph 2: Epoch: 045, Loss: -0.5253\n",
      "Graph 2: Epoch: 046, Loss: -0.2637\n",
      "Graph 2: Epoch: 047, Loss: -0.6577\n",
      "Graph 2: Epoch: 048, Loss: -0.6564\n",
      "Graph 2: Epoch: 049, Loss: -0.5795\n",
      "Graph 2: Epoch: 050, Loss: -0.3941\n",
      "Graph 2: Epoch: 051, Loss: -0.3927\n",
      "Graph 2: Epoch: 052, Loss: -0.5931\n",
      "Graph 2: Epoch: 053, Loss: -0.3298\n",
      "Graph 2: Epoch: 054, Loss: -0.6591\n",
      "Graph 2: Epoch: 055, Loss: -0.2639\n",
      "Graph 2: Epoch: 056, Loss: -0.2640\n",
      "Graph 2: Epoch: 057, Loss: -0.5272\n",
      "Graph 2: Epoch: 058, Loss: -0.3958\n",
      "Graph 2: Epoch: 059, Loss: -0.5275\n",
      "Graph 2: Epoch: 060, Loss: -0.3295\n",
      "Graph 2: Epoch: 061, Loss: -0.6586\n",
      "Graph 2: Epoch: 062, Loss: -0.3956\n",
      "Graph 2: Epoch: 063, Loss: -0.5278\n",
      "Graph 2: Epoch: 064, Loss: -0.3952\n",
      "Graph 2: Epoch: 065, Loss: -0.3962\n",
      "Graph 2: Epoch: 066, Loss: -0.1983\n",
      "Graph 2: Epoch: 067, Loss: -0.4622\n",
      "Graph 2: Epoch: 068, Loss: -0.7923\n",
      "Graph 2: Epoch: 069, Loss: -0.2643\n",
      "Graph 2: Epoch: 070, Loss: -0.3954\n",
      "Graph 2: Epoch: 071, Loss: -0.3962\n",
      "Graph 2: Epoch: 072, Loss: -0.4618\n",
      "Graph 2: Epoch: 073, Loss: -0.3968\n",
      "Graph 2: Epoch: 074, Loss: -0.5949\n",
      "Graph 2: Epoch: 075, Loss: -0.4627\n",
      "Graph 2: Epoch: 076, Loss: -0.4633\n",
      "Graph 2: Epoch: 077, Loss: -0.5289\n",
      "Graph 2: Epoch: 078, Loss: -0.7276\n",
      "Graph 2: Epoch: 079, Loss: -0.4633\n",
      "Graph 2: Epoch: 080, Loss: -0.3973\n",
      "Graph 2: Epoch: 081, Loss: -0.4634\n",
      "Graph 2: Epoch: 082, Loss: -0.3970\n",
      "Graph 2: Epoch: 083, Loss: -0.4634\n",
      "Graph 2: Epoch: 084, Loss: -0.3972\n",
      "Graph 2: Epoch: 085, Loss: -0.4636\n",
      "Graph 2: Epoch: 086, Loss: -0.3974\n",
      "Graph 2: Epoch: 087, Loss: -0.7283\n",
      "Graph 2: Epoch: 088, Loss: -0.5961\n",
      "Graph 2: Epoch: 089, Loss: -0.4637\n",
      "Graph 2: Epoch: 090, Loss: -0.4635\n",
      "Graph 2: Epoch: 091, Loss: -0.6623\n",
      "Graph 2: Epoch: 092, Loss: -0.4638\n",
      "Graph 2: Epoch: 093, Loss: -0.5297\n",
      "Graph 2: Epoch: 094, Loss: -0.6626\n",
      "Graph 2: Epoch: 095, Loss: -0.4640\n",
      "Graph 2: Epoch: 096, Loss: -0.1990\n",
      "Graph 2: Epoch: 097, Loss: -0.4639\n",
      "Graph 2: Epoch: 098, Loss: -0.5298\n",
      "Graph 2: Epoch: 099, Loss: -0.4640\n",
      "Graph 2: Epoch: 100, Loss: -0.2653\n",
      "Graph 3: Epoch: 001, Loss: -0.1160\n",
      "Graph 3: Epoch: 002, Loss: -0.1551\n",
      "Graph 3: Epoch: 003, Loss: -0.0963\n",
      "Graph 3: Epoch: 004, Loss: -0.3034\n",
      "Graph 3: Epoch: 005, Loss: -0.3802\n",
      "Graph 3: Epoch: 006, Loss: -0.3129\n",
      "Graph 3: Epoch: 007, Loss: -0.2046\n",
      "Graph 3: Epoch: 008, Loss: -0.2526\n",
      "Graph 3: Epoch: 009, Loss: -0.5675\n",
      "Graph 3: Epoch: 010, Loss: -0.2992\n",
      "Graph 3: Epoch: 011, Loss: -0.3168\n",
      "Graph 3: Epoch: 012, Loss: -0.5204\n",
      "Graph 3: Epoch: 013, Loss: -0.3273\n",
      "Graph 3: Epoch: 014, Loss: -0.5102\n",
      "Graph 3: Epoch: 015, Loss: -0.3524\n",
      "Graph 3: Epoch: 016, Loss: -0.5275\n",
      "Graph 3: Epoch: 017, Loss: -0.4500\n",
      "Graph 3: Epoch: 018, Loss: -0.2869\n",
      "Graph 3: Epoch: 019, Loss: -0.3679\n",
      "Graph 3: Epoch: 020, Loss: -0.5281\n",
      "Graph 3: Epoch: 021, Loss: -0.6290\n",
      "Graph 3: Epoch: 022, Loss: -0.5495\n",
      "Graph 3: Epoch: 023, Loss: -0.3675\n",
      "Graph 3: Epoch: 024, Loss: -0.3942\n",
      "Graph 3: Epoch: 025, Loss: -0.5462\n",
      "Graph 3: Epoch: 026, Loss: -0.5647\n",
      "Graph 3: Epoch: 027, Loss: -0.5594\n",
      "Graph 3: Epoch: 028, Loss: -0.2760\n",
      "Graph 3: Epoch: 029, Loss: -0.1934\n",
      "Graph 3: Epoch: 030, Loss: -0.3796\n",
      "Graph 3: Epoch: 031, Loss: -0.5782\n",
      "Graph 3: Epoch: 032, Loss: -0.4764\n",
      "Graph 3: Epoch: 033, Loss: -0.4782\n",
      "Graph 3: Epoch: 034, Loss: -0.4761\n",
      "Graph 3: Epoch: 035, Loss: -0.3843\n",
      "Graph 3: Epoch: 036, Loss: -0.4835\n",
      "Graph 3: Epoch: 037, Loss: -0.2927\n",
      "Graph 3: Epoch: 038, Loss: -0.7734\n",
      "Graph 3: Epoch: 039, Loss: -0.7750\n",
      "Graph 3: Epoch: 040, Loss: -0.4864\n",
      "Graph 3: Epoch: 041, Loss: -0.2935\n",
      "Graph 3: Epoch: 042, Loss: -0.4865\n",
      "Graph 3: Epoch: 043, Loss: -0.3905\n",
      "Graph 3: Epoch: 044, Loss: -0.3910\n",
      "Graph 3: Epoch: 045, Loss: -0.2934\n",
      "Graph 3: Epoch: 046, Loss: -0.5083\n",
      "Graph 3: Epoch: 047, Loss: -0.3895\n",
      "Graph 3: Epoch: 048, Loss: -0.1971\n",
      "Graph 3: Epoch: 049, Loss: -0.7798\n",
      "Graph 3: Epoch: 050, Loss: -0.5867\n",
      "Graph 3: Epoch: 051, Loss: -0.4869\n",
      "Graph 3: Epoch: 052, Loss: -0.5818\n",
      "Graph 3: Epoch: 053, Loss: -0.5802\n",
      "Graph 3: Epoch: 054, Loss: -0.4899\n",
      "Graph 3: Epoch: 055, Loss: -0.4873\n",
      "Graph 3: Epoch: 056, Loss: -0.4896\n",
      "Graph 3: Epoch: 057, Loss: -0.3936\n",
      "Graph 3: Epoch: 058, Loss: -0.2952\n",
      "Graph 3: Epoch: 059, Loss: -0.4875\n",
      "Graph 3: Epoch: 060, Loss: -0.5891\n",
      "Graph 3: Epoch: 061, Loss: -0.8680\n",
      "Graph 3: Epoch: 062, Loss: -0.3949\n",
      "Graph 3: Epoch: 063, Loss: -0.5861\n",
      "Graph 3: Epoch: 064, Loss: -0.3943\n",
      "Graph 3: Epoch: 065, Loss: -0.4914\n",
      "Graph 3: Epoch: 066, Loss: -0.5903\n",
      "Graph 3: Epoch: 067, Loss: -0.4934\n",
      "Graph 3: Epoch: 068, Loss: -0.4929\n",
      "Graph 3: Epoch: 069, Loss: -0.2966\n",
      "Graph 3: Epoch: 070, Loss: -0.4918\n",
      "Graph 3: Epoch: 071, Loss: -0.5913\n",
      "Graph 3: Epoch: 072, Loss: -0.5897\n",
      "Graph 3: Epoch: 073, Loss: -0.2965\n",
      "Graph 3: Epoch: 074, Loss: -0.4923\n",
      "Graph 3: Epoch: 075, Loss: -0.3956\n",
      "Graph 3: Epoch: 076, Loss: -0.5922\n",
      "Graph 3: Epoch: 077, Loss: -0.2160\n",
      "Graph 3: Epoch: 078, Loss: -0.2976\n",
      "Graph 3: Epoch: 079, Loss: -0.6327\n",
      "Graph 3: Epoch: 080, Loss: -0.2961\n",
      "Graph 3: Epoch: 081, Loss: -0.7106\n",
      "Graph 3: Epoch: 082, Loss: -0.6915\n",
      "Graph 3: Epoch: 083, Loss: -0.3932\n",
      "Graph 3: Epoch: 084, Loss: -0.3942\n",
      "Graph 3: Epoch: 085, Loss: -0.5940\n",
      "Graph 3: Epoch: 086, Loss: -0.4863\n",
      "Graph 3: Epoch: 087, Loss: -0.4913\n",
      "Graph 3: Epoch: 088, Loss: -0.3951\n",
      "Graph 3: Epoch: 089, Loss: -0.6926\n",
      "Graph 3: Epoch: 090, Loss: -0.4943\n",
      "Graph 3: Epoch: 091, Loss: -0.3963\n",
      "Graph 3: Epoch: 092, Loss: -0.5925\n",
      "Graph 3: Epoch: 093, Loss: -0.3961\n",
      "Graph 3: Epoch: 094, Loss: -0.4953\n",
      "Graph 3: Epoch: 095, Loss: -0.3827\n",
      "Graph 3: Epoch: 096, Loss: -0.4928\n",
      "Graph 3: Epoch: 097, Loss: -0.3967\n",
      "Graph 3: Epoch: 098, Loss: -0.2976\n",
      "Graph 3: Epoch: 099, Loss: -0.1984\n",
      "Graph 3: Epoch: 100, Loss: -0.4939\n",
      "Graph 4: Epoch: 001, Loss: -0.0640\n",
      "Graph 4: Epoch: 002, Loss: -0.1900\n",
      "Graph 4: Epoch: 003, Loss: -0.3772\n",
      "Graph 4: Epoch: 004, Loss: -0.0578\n",
      "Graph 4: Epoch: 005, Loss: -0.1921\n",
      "Graph 4: Epoch: 006, Loss: -0.3946\n",
      "Graph 4: Epoch: 007, Loss: -0.4470\n",
      "Graph 4: Epoch: 008, Loss: -0.4805\n",
      "Graph 4: Epoch: 009, Loss: -0.3849\n",
      "Graph 4: Epoch: 010, Loss: -0.3015\n",
      "Graph 4: Epoch: 011, Loss: -0.1798\n",
      "Graph 4: Epoch: 012, Loss: -0.4784\n",
      "Graph 4: Epoch: 013, Loss: -0.3716\n",
      "Graph 4: Epoch: 014, Loss: -0.3074\n",
      "Graph 4: Epoch: 015, Loss: -0.4906\n",
      "Graph 4: Epoch: 016, Loss: -0.6624\n",
      "Graph 4: Epoch: 017, Loss: -0.5994\n",
      "Graph 4: Epoch: 018, Loss: -0.5039\n",
      "Graph 4: Epoch: 019, Loss: -0.3873\n",
      "Graph 4: Epoch: 020, Loss: -0.3277\n",
      "Graph 4: Epoch: 021, Loss: -0.4087\n",
      "Graph 4: Epoch: 022, Loss: -0.4145\n",
      "Graph 4: Epoch: 023, Loss: -0.3050\n",
      "Graph 4: Epoch: 024, Loss: -0.2911\n",
      "Graph 4: Epoch: 025, Loss: -0.4469\n",
      "Graph 4: Epoch: 026, Loss: -0.3027\n",
      "Graph 4: Epoch: 027, Loss: -0.4188\n",
      "Graph 4: Epoch: 028, Loss: -0.4100\n",
      "Graph 4: Epoch: 029, Loss: -0.6282\n",
      "Graph 4: Epoch: 030, Loss: -0.5196\n",
      "Graph 4: Epoch: 031, Loss: -0.5265\n",
      "Graph 4: Epoch: 032, Loss: -0.6257\n",
      "Graph 4: Epoch: 033, Loss: -0.4147\n",
      "Graph 4: Epoch: 034, Loss: -0.3179\n",
      "Graph 4: Epoch: 035, Loss: -0.4313\n",
      "Graph 4: Epoch: 036, Loss: -0.3217\n",
      "Graph 4: Epoch: 037, Loss: -0.5222\n",
      "Graph 4: Epoch: 038, Loss: -0.7403\n",
      "Graph 4: Epoch: 039, Loss: -0.6413\n",
      "Graph 4: Epoch: 040, Loss: -0.5516\n",
      "Graph 4: Epoch: 041, Loss: -0.5347\n",
      "Graph 4: Epoch: 042, Loss: -0.2203\n",
      "Graph 4: Epoch: 043, Loss: -0.6415\n",
      "Graph 4: Epoch: 044, Loss: -0.6448\n",
      "Graph 4: Epoch: 045, Loss: -0.3127\n",
      "Graph 4: Epoch: 046, Loss: -0.6369\n",
      "Graph 4: Epoch: 047, Loss: -0.5596\n",
      "Graph 4: Epoch: 048, Loss: -0.3251\n",
      "Graph 4: Epoch: 049, Loss: -0.5409\n",
      "Graph 4: Epoch: 050, Loss: -0.6685\n",
      "Graph 4: Epoch: 051, Loss: -0.2182\n",
      "Graph 4: Epoch: 052, Loss: -0.7579\n",
      "Graph 4: Epoch: 053, Loss: -0.4332\n",
      "Graph 4: Epoch: 054, Loss: -0.5418\n",
      "Graph 4: Epoch: 055, Loss: -0.6503\n",
      "Graph 4: Epoch: 056, Loss: -0.5398\n",
      "Graph 4: Epoch: 057, Loss: -0.4327\n",
      "Graph 4: Epoch: 058, Loss: -0.7585\n",
      "Graph 4: Epoch: 059, Loss: -0.3281\n",
      "Graph 4: Epoch: 060, Loss: -0.3274\n",
      "Graph 4: Epoch: 061, Loss: -0.4346\n",
      "Graph 4: Epoch: 062, Loss: -0.6410\n",
      "Graph 4: Epoch: 063, Loss: -0.3301\n",
      "Graph 4: Epoch: 064, Loss: -0.4368\n",
      "Graph 4: Epoch: 065, Loss: -0.3286\n",
      "Graph 4: Epoch: 066, Loss: -0.4364\n",
      "Graph 4: Epoch: 067, Loss: -0.7616\n",
      "Graph 4: Epoch: 068, Loss: -0.6540\n",
      "Graph 4: Epoch: 069, Loss: -0.2196\n",
      "Graph 4: Epoch: 070, Loss: -0.5454\n",
      "Graph 4: Epoch: 071, Loss: -0.4376\n",
      "Graph 4: Epoch: 072, Loss: -0.4380\n",
      "Graph 4: Epoch: 073, Loss: -0.6527\n",
      "Graph 4: Epoch: 074, Loss: -0.4360\n",
      "Graph 4: Epoch: 075, Loss: -0.4393\n",
      "Graph 4: Epoch: 076, Loss: -0.4376\n",
      "Graph 4: Epoch: 077, Loss: -0.4376\n",
      "Graph 4: Epoch: 078, Loss: -0.6560\n",
      "Graph 4: Epoch: 079, Loss: -0.6562\n",
      "Graph 4: Epoch: 080, Loss: -0.5471\n",
      "Graph 4: Epoch: 081, Loss: -0.4370\n",
      "Graph 4: Epoch: 082, Loss: -0.6570\n",
      "Graph 4: Epoch: 083, Loss: -0.4390\n",
      "Graph 4: Epoch: 084, Loss: -0.3284\n",
      "Graph 4: Epoch: 085, Loss: -0.4384\n",
      "Graph 4: Epoch: 086, Loss: -0.4380\n",
      "Graph 4: Epoch: 087, Loss: -0.6281\n",
      "Graph 4: Epoch: 088, Loss: -0.5484\n",
      "Graph 4: Epoch: 089, Loss: -0.0181\n",
      "Graph 4: Epoch: 090, Loss: -0.7656\n",
      "Graph 4: Epoch: 091, Loss: -0.3298\n",
      "Graph 4: Epoch: 092, Loss: -0.7660\n",
      "Graph 4: Epoch: 093, Loss: -0.7666\n",
      "Graph 4: Epoch: 094, Loss: -0.5490\n",
      "Graph 4: Epoch: 095, Loss: -0.4511\n",
      "Graph 4: Epoch: 096, Loss: -0.6477\n",
      "Graph 4: Epoch: 097, Loss: -0.4394\n",
      "Graph 4: Epoch: 098, Loss: -0.1277\n",
      "Graph 4: Epoch: 099, Loss: -0.5482\n",
      "Graph 4: Epoch: 100, Loss: -0.4395\n",
      "Graph 5: Epoch: 001, Loss: -0.0253\n",
      "Graph 5: Epoch: 002, Loss: -0.1847\n",
      "Graph 5: Epoch: 003, Loss: -0.4842\n",
      "Graph 5: Epoch: 004, Loss: -0.5755\n",
      "Graph 5: Epoch: 005, Loss: -0.6246\n",
      "Graph 5: Epoch: 006, Loss: -0.2431\n",
      "Graph 5: Epoch: 007, Loss: -0.3726\n",
      "Graph 5: Epoch: 008, Loss: -0.4899\n",
      "Graph 5: Epoch: 009, Loss: -0.4953\n",
      "Graph 5: Epoch: 010, Loss: -0.3992\n",
      "Graph 5: Epoch: 011, Loss: -0.4139\n",
      "Graph 5: Epoch: 012, Loss: -0.5699\n",
      "Graph 5: Epoch: 013, Loss: -0.6324\n",
      "Graph 5: Epoch: 014, Loss: -0.4747\n",
      "Graph 5: Epoch: 015, Loss: -0.5286\n",
      "Graph 5: Epoch: 016, Loss: -0.5723\n",
      "Graph 5: Epoch: 017, Loss: -0.3472\n",
      "Graph 5: Epoch: 018, Loss: -0.5294\n",
      "Graph 5: Epoch: 019, Loss: -0.5313\n",
      "Graph 5: Epoch: 020, Loss: -0.5389\n",
      "Graph 5: Epoch: 021, Loss: -0.3777\n",
      "Graph 5: Epoch: 022, Loss: -0.5931\n",
      "Graph 5: Epoch: 023, Loss: -0.3789\n",
      "Graph 5: Epoch: 024, Loss: -0.5943\n",
      "Graph 5: Epoch: 025, Loss: -0.4867\n",
      "Graph 5: Epoch: 026, Loss: -0.4886\n",
      "Graph 5: Epoch: 027, Loss: -0.5969\n",
      "Graph 5: Epoch: 028, Loss: -0.7603\n",
      "Graph 5: Epoch: 029, Loss: -0.4860\n",
      "Graph 5: Epoch: 030, Loss: -0.4886\n",
      "Graph 5: Epoch: 031, Loss: -0.4365\n",
      "Graph 5: Epoch: 032, Loss: -0.6546\n",
      "Graph 5: Epoch: 033, Loss: -0.6005\n",
      "Graph 5: Epoch: 034, Loss: -0.5461\n",
      "Graph 5: Epoch: 035, Loss: -0.5472\n",
      "Graph 5: Epoch: 036, Loss: -0.4926\n",
      "Graph 5: Epoch: 037, Loss: -0.4380\n",
      "Graph 5: Epoch: 038, Loss: -0.4930\n",
      "Graph 5: Epoch: 039, Loss: -0.4931\n",
      "Graph 5: Epoch: 040, Loss: -0.4936\n",
      "Graph 5: Epoch: 041, Loss: -0.6028\n",
      "Graph 5: Epoch: 042, Loss: -0.7123\n",
      "Graph 5: Epoch: 043, Loss: -0.6035\n",
      "Graph 5: Epoch: 044, Loss: -0.4945\n",
      "Graph 5: Epoch: 045, Loss: -0.6045\n",
      "Graph 5: Epoch: 046, Loss: -0.2199\n",
      "Graph 5: Epoch: 047, Loss: -0.5494\n",
      "Graph 5: Epoch: 048, Loss: -0.6031\n",
      "Graph 5: Epoch: 049, Loss: -0.2749\n",
      "Graph 5: Epoch: 050, Loss: -0.5496\n",
      "Graph 5: Epoch: 051, Loss: -0.4399\n",
      "Graph 5: Epoch: 052, Loss: -0.5499\n",
      "Graph 5: Epoch: 053, Loss: -0.4404\n",
      "Graph 5: Epoch: 054, Loss: -0.4954\n",
      "Graph 5: Epoch: 055, Loss: -0.3851\n",
      "Graph 5: Epoch: 056, Loss: -0.6056\n",
      "Graph 5: Epoch: 057, Loss: -0.4402\n",
      "Graph 5: Epoch: 058, Loss: -0.6055\n",
      "Graph 5: Epoch: 059, Loss: -0.3858\n",
      "Graph 5: Epoch: 060, Loss: -0.6058\n",
      "Graph 5: Epoch: 061, Loss: -0.2758\n",
      "Graph 5: Epoch: 062, Loss: -0.7706\n",
      "Graph 5: Epoch: 063, Loss: -0.7713\n",
      "Graph 5: Epoch: 064, Loss: -0.5511\n",
      "Graph 5: Epoch: 065, Loss: -0.3860\n",
      "Graph 5: Epoch: 066, Loss: -0.2207\n",
      "Graph 5: Epoch: 067, Loss: -0.6064\n",
      "Graph 5: Epoch: 068, Loss: -0.4962\n",
      "Graph 5: Epoch: 069, Loss: -0.5514\n",
      "Graph 5: Epoch: 070, Loss: -0.6620\n",
      "Graph 5: Epoch: 071, Loss: -0.4413\n",
      "Graph 5: Epoch: 072, Loss: -0.4413\n",
      "Graph 5: Epoch: 073, Loss: -0.3863\n",
      "Graph 5: Epoch: 074, Loss: -0.3863\n",
      "Graph 5: Epoch: 075, Loss: -0.5518\n",
      "Graph 5: Epoch: 076, Loss: -0.4967\n",
      "Graph 5: Epoch: 077, Loss: -0.5520\n",
      "Graph 5: Epoch: 078, Loss: -0.5519\n",
      "Graph 5: Epoch: 079, Loss: -0.6072\n",
      "Graph 5: Epoch: 080, Loss: -0.3865\n",
      "Graph 5: Epoch: 081, Loss: -0.3865\n",
      "Graph 5: Epoch: 082, Loss: -0.2762\n",
      "Graph 5: Epoch: 083, Loss: -0.5521\n",
      "Graph 5: Epoch: 084, Loss: -0.6071\n",
      "Graph 5: Epoch: 085, Loss: -0.4419\n",
      "Graph 5: Epoch: 086, Loss: -0.7180\n",
      "Graph 5: Epoch: 087, Loss: -0.2762\n",
      "Graph 5: Epoch: 088, Loss: -0.4971\n",
      "Graph 5: Epoch: 089, Loss: -0.4419\n",
      "Graph 5: Epoch: 090, Loss: -0.4971\n",
      "Graph 5: Epoch: 091, Loss: -0.4972\n",
      "Graph 5: Epoch: 092, Loss: -0.4972\n",
      "Graph 5: Epoch: 093, Loss: -0.2212\n",
      "Graph 5: Epoch: 094, Loss: -0.4421\n",
      "Graph 5: Epoch: 095, Loss: -0.4974\n",
      "Graph 5: Epoch: 096, Loss: -0.4974\n",
      "Graph 5: Epoch: 097, Loss: -0.4421\n",
      "Graph 5: Epoch: 098, Loss: -0.5526\n",
      "Graph 5: Epoch: 099, Loss: -0.6076\n",
      "Graph 5: Epoch: 100, Loss: -0.6633\n",
      "Graph 6: Epoch: 001, Loss: -0.0251\n",
      "Graph 6: Epoch: 002, Loss: -0.0902\n",
      "Graph 6: Epoch: 003, Loss: -0.2417\n",
      "Graph 6: Epoch: 004, Loss: -0.2965\n",
      "Graph 6: Epoch: 005, Loss: -0.2709\n",
      "Graph 6: Epoch: 006, Loss: -0.2614\n",
      "Graph 6: Epoch: 007, Loss: -0.4426\n",
      "Graph 6: Epoch: 008, Loss: -0.3218\n",
      "Graph 6: Epoch: 009, Loss: -0.2370\n",
      "Graph 6: Epoch: 010, Loss: -0.3281\n",
      "Graph 6: Epoch: 011, Loss: -0.5503\n",
      "Graph 6: Epoch: 012, Loss: -0.4240\n",
      "Graph 6: Epoch: 013, Loss: -0.2714\n",
      "Graph 6: Epoch: 014, Loss: -0.4895\n",
      "Graph 6: Epoch: 015, Loss: -0.5082\n",
      "Graph 6: Epoch: 016, Loss: -0.5345\n",
      "Graph 6: Epoch: 017, Loss: -0.3655\n",
      "Graph 6: Epoch: 018, Loss: -0.5490\n",
      "Graph 6: Epoch: 019, Loss: -0.4583\n",
      "Graph 6: Epoch: 020, Loss: -0.3446\n",
      "Graph 6: Epoch: 021, Loss: -0.5577\n",
      "Graph 6: Epoch: 022, Loss: -0.4664\n",
      "Graph 6: Epoch: 023, Loss: -0.7456\n",
      "Graph 6: Epoch: 024, Loss: -0.4716\n",
      "Graph 6: Epoch: 025, Loss: -0.4745\n",
      "Graph 6: Epoch: 026, Loss: -0.7326\n",
      "Graph 6: Epoch: 027, Loss: -0.4716\n",
      "Graph 6: Epoch: 028, Loss: -0.6595\n",
      "Graph 6: Epoch: 029, Loss: -0.6621\n",
      "Graph 6: Epoch: 030, Loss: -0.3841\n",
      "Graph 6: Epoch: 031, Loss: -0.2812\n",
      "Graph 6: Epoch: 032, Loss: -0.2888\n",
      "Graph 6: Epoch: 033, Loss: -0.1943\n",
      "Graph 6: Epoch: 034, Loss: -0.2886\n",
      "Graph 6: Epoch: 035, Loss: -0.2886\n",
      "Graph 6: Epoch: 036, Loss: -0.5624\n",
      "Graph 6: Epoch: 037, Loss: -0.5786\n",
      "Graph 6: Epoch: 038, Loss: -0.4846\n",
      "Graph 6: Epoch: 039, Loss: -0.6766\n",
      "Graph 6: Epoch: 040, Loss: -0.4848\n",
      "Graph 6: Epoch: 041, Loss: -0.3887\n",
      "Graph 6: Epoch: 042, Loss: -0.4845\n",
      "Graph 6: Epoch: 043, Loss: -0.4858\n",
      "Graph 6: Epoch: 044, Loss: -0.5837\n",
      "Graph 6: Epoch: 045, Loss: -0.2085\n",
      "Graph 6: Epoch: 046, Loss: -0.2922\n",
      "Graph 6: Epoch: 047, Loss: -0.5855\n",
      "Graph 6: Epoch: 048, Loss: -0.2933\n",
      "Graph 6: Epoch: 049, Loss: -0.5029\n",
      "Graph 6: Epoch: 050, Loss: -0.2941\n",
      "Graph 6: Epoch: 051, Loss: -0.5849\n",
      "Graph 6: Epoch: 052, Loss: -0.4867\n",
      "Graph 6: Epoch: 053, Loss: -0.1975\n",
      "Graph 6: Epoch: 054, Loss: -0.2946\n",
      "Graph 6: Epoch: 055, Loss: -0.3927\n",
      "Graph 6: Epoch: 056, Loss: -0.4903\n",
      "Graph 6: Epoch: 057, Loss: -0.2399\n",
      "Graph 6: Epoch: 058, Loss: -0.4904\n",
      "Graph 6: Epoch: 059, Loss: -0.3935\n",
      "Graph 6: Epoch: 060, Loss: -0.2944\n",
      "Graph 6: Epoch: 061, Loss: -0.5893\n",
      "Graph 6: Epoch: 062, Loss: -0.4885\n",
      "Graph 6: Epoch: 063, Loss: -0.5881\n",
      "Graph 6: Epoch: 064, Loss: -0.8776\n",
      "Graph 6: Epoch: 065, Loss: -0.5896\n",
      "Graph 6: Epoch: 066, Loss: -0.5889\n",
      "Graph 6: Epoch: 067, Loss: -0.4902\n",
      "Graph 6: Epoch: 068, Loss: -0.5896\n",
      "Graph 6: Epoch: 069, Loss: -0.2968\n",
      "Graph 6: Epoch: 070, Loss: -0.4927\n",
      "Graph 6: Epoch: 071, Loss: -0.5908\n",
      "Graph 6: Epoch: 072, Loss: -0.5907\n",
      "Graph 6: Epoch: 073, Loss: -0.4904\n",
      "Graph 6: Epoch: 074, Loss: -0.5377\n",
      "Graph 6: Epoch: 075, Loss: -0.5910\n",
      "Graph 6: Epoch: 076, Loss: -0.3948\n",
      "Graph 6: Epoch: 077, Loss: -0.4929\n",
      "Graph 6: Epoch: 078, Loss: -0.6645\n",
      "Graph 6: Epoch: 079, Loss: -0.2965\n",
      "Graph 6: Epoch: 080, Loss: -0.5920\n",
      "Graph 6: Epoch: 081, Loss: -0.2969\n",
      "Graph 6: Epoch: 082, Loss: -0.5900\n",
      "Graph 6: Epoch: 083, Loss: -0.5907\n",
      "Graph 6: Epoch: 084, Loss: -0.5920\n",
      "Graph 6: Epoch: 085, Loss: -0.3889\n",
      "Graph 6: Epoch: 086, Loss: -0.5879\n",
      "Graph 6: Epoch: 087, Loss: -0.6911\n",
      "Graph 6: Epoch: 088, Loss: -0.6882\n",
      "Graph 6: Epoch: 089, Loss: -0.4929\n",
      "Graph 6: Epoch: 090, Loss: -0.5917\n",
      "Graph 6: Epoch: 091, Loss: -0.3961\n",
      "Graph 6: Epoch: 092, Loss: -0.1986\n",
      "Graph 6: Epoch: 093, Loss: -0.3951\n",
      "Graph 6: Epoch: 094, Loss: -0.4960\n",
      "Graph 6: Epoch: 095, Loss: -0.2935\n",
      "Graph 6: Epoch: 096, Loss: -0.4949\n",
      "Graph 6: Epoch: 097, Loss: -0.6895\n",
      "Graph 6: Epoch: 098, Loss: -0.6906\n",
      "Graph 6: Epoch: 099, Loss: -0.5940\n",
      "Graph 6: Epoch: 100, Loss: -0.6928\n",
      "Graph 7: Epoch: 001, Loss: -0.1571\n",
      "Graph 7: Epoch: 002, Loss: -0.4607\n",
      "Graph 7: Epoch: 003, Loss: -0.1774\n",
      "Graph 7: Epoch: 004, Loss: -0.2597\n",
      "Graph 7: Epoch: 005, Loss: -0.2780\n",
      "Graph 7: Epoch: 006, Loss: -0.3823\n",
      "Graph 7: Epoch: 007, Loss: -0.0640\n",
      "Graph 7: Epoch: 008, Loss: -0.2610\n",
      "Graph 7: Epoch: 009, Loss: -0.1328\n",
      "Graph 7: Epoch: 010, Loss: -0.7526\n",
      "Graph 7: Epoch: 011, Loss: -0.4181\n",
      "Graph 7: Epoch: 012, Loss: -0.1652\n",
      "Graph 7: Epoch: 013, Loss: -0.0415\n",
      "Graph 7: Epoch: 014, Loss: -0.4720\n",
      "Graph 7: Epoch: 015, Loss: -0.7601\n",
      "Graph 7: Epoch: 016, Loss: -0.4497\n",
      "Graph 7: Epoch: 017, Loss: -0.5681\n",
      "Graph 7: Epoch: 018, Loss: -0.4329\n",
      "Graph 7: Epoch: 019, Loss: -0.5845\n",
      "Graph 7: Epoch: 020, Loss: -0.2633\n",
      "Graph 7: Epoch: 021, Loss: -0.4286\n",
      "Graph 7: Epoch: 022, Loss: -0.2423\n",
      "Graph 7: Epoch: 023, Loss: -0.5560\n",
      "Graph 7: Epoch: 024, Loss: -0.3413\n",
      "Graph 7: Epoch: 025, Loss: -0.7852\n",
      "Graph 7: Epoch: 026, Loss: -0.4117\n",
      "Graph 7: Epoch: 027, Loss: -0.3549\n",
      "Graph 7: Epoch: 028, Loss: -0.2893\n",
      "Graph 7: Epoch: 029, Loss: -0.4544\n",
      "Graph 7: Epoch: 030, Loss: -0.5738\n",
      "Graph 7: Epoch: 031, Loss: -0.2631\n",
      "Graph 7: Epoch: 032, Loss: -0.5885\n",
      "Graph 7: Epoch: 033, Loss: -0.4575\n",
      "Graph 7: Epoch: 034, Loss: -0.4559\n",
      "Graph 7: Epoch: 035, Loss: -0.2771\n",
      "Graph 7: Epoch: 036, Loss: -0.5723\n",
      "Graph 7: Epoch: 037, Loss: -0.2719\n",
      "Graph 7: Epoch: 038, Loss: -0.8352\n",
      "Graph 7: Epoch: 039, Loss: -0.2781\n",
      "Graph 7: Epoch: 040, Loss: -0.6558\n",
      "Graph 7: Epoch: 041, Loss: -0.6577\n",
      "Graph 7: Epoch: 042, Loss: -0.6547\n",
      "Graph 7: Epoch: 043, Loss: -0.5759\n",
      "Graph 7: Epoch: 044, Loss: -0.4671\n",
      "Graph 7: Epoch: 045, Loss: -0.6430\n",
      "Graph 7: Epoch: 046, Loss: -0.8556\n",
      "Graph 7: Epoch: 047, Loss: -0.8596\n",
      "Graph 7: Epoch: 048, Loss: -0.1191\n",
      "Graph 7: Epoch: 049, Loss: -0.2605\n",
      "Graph 7: Epoch: 050, Loss: -0.5905\n",
      "Graph 7: Epoch: 051, Loss: -0.4674\n",
      "Graph 7: Epoch: 052, Loss: -0.0442\n",
      "Graph 7: Epoch: 053, Loss: -0.2480\n",
      "Graph 7: Epoch: 054, Loss: -0.3933\n",
      "Graph 7: Epoch: 055, Loss: -0.2724\n",
      "Graph 7: Epoch: 056, Loss: -0.7861\n",
      "Graph 7: Epoch: 057, Loss: -0.3171\n",
      "Graph 7: Epoch: 058, Loss: -0.3002\n",
      "Graph 7: Epoch: 059, Loss: -0.4815\n",
      "Graph 7: Epoch: 060, Loss: -0.4730\n",
      "Graph 7: Epoch: 061, Loss: -0.6725\n",
      "Graph 7: Epoch: 062, Loss: -0.4776\n",
      "Graph 7: Epoch: 063, Loss: -0.4846\n",
      "Graph 7: Epoch: 064, Loss: -0.4091\n",
      "Graph 7: Epoch: 065, Loss: -0.6089\n",
      "Graph 7: Epoch: 066, Loss: -0.3269\n",
      "Graph 7: Epoch: 067, Loss: -0.8779\n",
      "Graph 7: Epoch: 068, Loss: -0.6165\n",
      "Graph 7: Epoch: 069, Loss: -0.4240\n",
      "Graph 7: Epoch: 070, Loss: -0.0787\n",
      "Graph 7: Epoch: 071, Loss: -0.2932\n",
      "Graph 7: Epoch: 072, Loss: -0.4406\n",
      "Graph 7: Epoch: 073, Loss: -0.2027\n",
      "Graph 7: Epoch: 074, Loss: -0.5357\n",
      "Graph 7: Epoch: 075, Loss: -0.4973\n",
      "Graph 7: Epoch: 076, Loss: -0.0756\n",
      "Graph 7: Epoch: 077, Loss: -0.6935\n",
      "Graph 7: Epoch: 078, Loss: -0.1533\n",
      "Graph 7: Epoch: 079, Loss: -0.2932\n",
      "Graph 7: Epoch: 080, Loss: -0.3122\n",
      "Graph 7: Epoch: 081, Loss: -0.8616\n",
      "Graph 7: Epoch: 082, Loss: -0.2749\n",
      "Graph 7: Epoch: 083, Loss: -0.4456\n",
      "Graph 7: Epoch: 084, Loss: -0.5538\n",
      "Graph 7: Epoch: 085, Loss: -0.8257\n",
      "Graph 7: Epoch: 086, Loss: -0.6907\n",
      "Graph 7: Epoch: 087, Loss: -0.2382\n",
      "Graph 7: Epoch: 088, Loss: -0.4834\n",
      "Graph 7: Epoch: 089, Loss: -0.7977\n",
      "Graph 7: Epoch: 090, Loss: -0.5754\n",
      "Graph 7: Epoch: 091, Loss: -0.4752\n",
      "Graph 7: Epoch: 092, Loss: -0.7283\n",
      "Graph 7: Epoch: 093, Loss: -0.3980\n",
      "Graph 7: Epoch: 094, Loss: -0.4613\n",
      "Graph 7: Epoch: 095, Loss: -0.6064\n",
      "Graph 7: Epoch: 096, Loss: -0.6219\n",
      "Graph 7: Epoch: 097, Loss: -0.6296\n",
      "Graph 7: Epoch: 098, Loss: -0.4761\n",
      "Graph 7: Epoch: 099, Loss: -0.3939\n",
      "Graph 7: Epoch: 100, Loss: -0.2010\n",
      "Graph 8: Epoch: 001, Loss: -0.2218\n",
      "Graph 8: Epoch: 002, Loss: -0.1664\n",
      "Graph 8: Epoch: 003, Loss: -0.2485\n",
      "Graph 8: Epoch: 004, Loss: -0.3023\n",
      "Graph 8: Epoch: 005, Loss: -0.3395\n",
      "Graph 8: Epoch: 006, Loss: -0.3205\n",
      "Graph 8: Epoch: 007, Loss: -0.3971\n",
      "Graph 8: Epoch: 008, Loss: -0.3041\n",
      "Graph 8: Epoch: 009, Loss: -0.3991\n",
      "Graph 8: Epoch: 010, Loss: -0.4140\n",
      "Graph 8: Epoch: 011, Loss: -0.4629\n",
      "Graph 8: Epoch: 012, Loss: -0.1937\n",
      "Graph 8: Epoch: 013, Loss: -0.3065\n",
      "Graph 8: Epoch: 014, Loss: -0.4298\n",
      "Graph 8: Epoch: 015, Loss: -0.1699\n",
      "Graph 8: Epoch: 016, Loss: -0.0590\n",
      "Graph 8: Epoch: 017, Loss: -0.3635\n",
      "Graph 8: Epoch: 018, Loss: -0.5869\n",
      "Graph 8: Epoch: 019, Loss: -0.6963\n",
      "Graph 8: Epoch: 020, Loss: -0.4324\n",
      "Graph 8: Epoch: 021, Loss: -0.1161\n",
      "Graph 8: Epoch: 022, Loss: -0.7112\n",
      "Graph 8: Epoch: 023, Loss: -0.4470\n",
      "Graph 8: Epoch: 024, Loss: -0.4460\n",
      "Graph 8: Epoch: 025, Loss: -0.3124\n",
      "Graph 8: Epoch: 026, Loss: -0.6816\n",
      "Graph 8: Epoch: 027, Loss: -0.4421\n",
      "Graph 8: Epoch: 028, Loss: -0.5476\n",
      "Graph 8: Epoch: 029, Loss: -0.4354\n",
      "Graph 8: Epoch: 030, Loss: -0.5613\n",
      "Graph 8: Epoch: 031, Loss: -0.2941\n",
      "Graph 8: Epoch: 032, Loss: -0.1729\n",
      "Graph 8: Epoch: 033, Loss: -0.4329\n",
      "Graph 8: Epoch: 034, Loss: -0.3080\n",
      "Graph 8: Epoch: 035, Loss: -0.4721\n",
      "Graph 8: Epoch: 036, Loss: -0.3634\n",
      "Graph 8: Epoch: 037, Loss: -0.3206\n",
      "Graph 8: Epoch: 038, Loss: -0.1652\n",
      "Graph 8: Epoch: 039, Loss: -0.6250\n",
      "Graph 8: Epoch: 040, Loss: -0.1694\n",
      "Graph 8: Epoch: 041, Loss: -0.3163\n",
      "Graph 8: Epoch: 042, Loss: -0.3234\n",
      "Graph 8: Epoch: 043, Loss: -0.4680\n",
      "Graph 8: Epoch: 044, Loss: -0.3160\n",
      "Graph 8: Epoch: 045, Loss: -0.3424\n",
      "Graph 8: Epoch: 046, Loss: -0.6308\n",
      "Graph 8: Epoch: 047, Loss: -0.6636\n",
      "Graph 8: Epoch: 048, Loss: -0.3642\n",
      "Graph 8: Epoch: 049, Loss: -0.1692\n",
      "Graph 8: Epoch: 050, Loss: -0.3636\n",
      "Graph 8: Epoch: 051, Loss: -0.3602\n",
      "Graph 8: Epoch: 052, Loss: -0.4732\n",
      "Graph 8: Epoch: 053, Loss: -0.3531\n",
      "Graph 8: Epoch: 054, Loss: -0.6341\n",
      "Graph 8: Epoch: 055, Loss: -0.0218\n",
      "Graph 8: Epoch: 056, Loss: -0.6335\n",
      "Graph 8: Epoch: 057, Loss: -0.3246\n",
      "Graph 8: Epoch: 058, Loss: -0.4667\n",
      "Graph 8: Epoch: 059, Loss: -0.6353\n",
      "Graph 8: Epoch: 060, Loss: -0.4800\n",
      "Graph 8: Epoch: 061, Loss: -0.4811\n",
      "Graph 8: Epoch: 062, Loss: -0.5085\n",
      "Graph 8: Epoch: 063, Loss: -0.4793\n",
      "Graph 8: Epoch: 064, Loss: -0.2148\n",
      "Graph 8: Epoch: 065, Loss: -0.5197\n",
      "Graph 8: Epoch: 066, Loss: -0.4844\n",
      "Graph 8: Epoch: 067, Loss: -0.3872\n",
      "Graph 8: Epoch: 068, Loss: -0.4862\n",
      "Graph 8: Epoch: 069, Loss: -0.1668\n",
      "Graph 8: Epoch: 070, Loss: -0.9650\n",
      "Graph 8: Epoch: 071, Loss: -0.4823\n",
      "Graph 8: Epoch: 072, Loss: -0.3281\n",
      "Graph 8: Epoch: 073, Loss: -0.4871\n",
      "Graph 8: Epoch: 074, Loss: -0.1692\n",
      "Graph 8: Epoch: 075, Loss: -0.6900\n",
      "Graph 8: Epoch: 076, Loss: -0.7007\n",
      "Graph 8: Epoch: 077, Loss: -0.3245\n",
      "Graph 8: Epoch: 078, Loss: -0.3262\n",
      "Graph 8: Epoch: 079, Loss: -0.1702\n",
      "Graph 8: Epoch: 080, Loss: -0.7206\n",
      "Graph 8: Epoch: 081, Loss: -0.6353\n",
      "Graph 8: Epoch: 082, Loss: -0.3258\n",
      "Graph 8: Epoch: 083, Loss: -0.4100\n",
      "Graph 8: Epoch: 084, Loss: -0.6503\n",
      "Graph 8: Epoch: 085, Loss: -0.4926\n",
      "Graph 8: Epoch: 086, Loss: -0.4804\n",
      "Graph 8: Epoch: 087, Loss: -0.3291\n",
      "Graph 8: Epoch: 088, Loss: -0.6513\n",
      "Graph 8: Epoch: 089, Loss: -0.4915\n",
      "Graph 8: Epoch: 090, Loss: -0.1595\n",
      "Graph 8: Epoch: 091, Loss: -0.4872\n",
      "Graph 8: Epoch: 092, Loss: -0.6527\n",
      "Graph 8: Epoch: 093, Loss: -0.4738\n",
      "Graph 8: Epoch: 094, Loss: -0.1702\n",
      "Graph 8: Epoch: 095, Loss: -0.2054\n",
      "Graph 8: Epoch: 096, Loss: -0.2704\n",
      "Graph 8: Epoch: 097, Loss: -0.6455\n",
      "Graph 8: Epoch: 098, Loss: -0.5733\n",
      "Graph 8: Epoch: 099, Loss: -0.2114\n",
      "Graph 8: Epoch: 100, Loss: -0.6492\n",
      "Graph 9: Epoch: 001, Loss: -0.0587\n",
      "Graph 9: Epoch: 002, Loss: -0.0592\n",
      "Graph 9: Epoch: 003, Loss: -0.0321\n",
      "Graph 9: Epoch: 004, Loss: -0.0951\n",
      "Graph 9: Epoch: 005, Loss: -0.2230\n",
      "Graph 9: Epoch: 006, Loss: -0.0631\n",
      "Graph 9: Epoch: 007, Loss: -0.1576\n",
      "Graph 9: Epoch: 008, Loss: -0.2137\n",
      "Graph 9: Epoch: 009, Loss: -0.0420\n",
      "Graph 9: Epoch: 010, Loss: -0.2547\n",
      "Graph 9: Epoch: 011, Loss: -0.0577\n",
      "Graph 9: Epoch: 012, Loss: -0.3431\n",
      "Graph 9: Epoch: 013, Loss: -0.5652\n",
      "Graph 9: Epoch: 014, Loss: -0.3491\n",
      "Graph 9: Epoch: 015, Loss: -0.3120\n",
      "Graph 9: Epoch: 016, Loss: -0.3939\n",
      "Graph 9: Epoch: 017, Loss: -0.3030\n",
      "Graph 9: Epoch: 018, Loss: -0.3231\n",
      "Graph 9: Epoch: 019, Loss: -0.3237\n",
      "Graph 9: Epoch: 020, Loss: -0.4142\n",
      "Graph 9: Epoch: 021, Loss: -0.2404\n",
      "Graph 9: Epoch: 022, Loss: -0.3131\n",
      "Graph 9: Epoch: 023, Loss: -0.3332\n",
      "Graph 9: Epoch: 024, Loss: -0.5357\n",
      "Graph 9: Epoch: 025, Loss: -0.4953\n",
      "Graph 9: Epoch: 026, Loss: -0.3679\n",
      "Graph 9: Epoch: 027, Loss: -0.5211\n",
      "Graph 9: Epoch: 028, Loss: -0.3205\n",
      "Graph 9: Epoch: 029, Loss: -0.3164\n",
      "Graph 9: Epoch: 030, Loss: -0.3159\n",
      "Graph 9: Epoch: 031, Loss: -0.3966\n",
      "Graph 9: Epoch: 032, Loss: -0.4441\n",
      "Graph 9: Epoch: 033, Loss: -0.3492\n",
      "Graph 9: Epoch: 034, Loss: -0.6756\n",
      "Graph 9: Epoch: 035, Loss: -0.6165\n",
      "Graph 9: Epoch: 036, Loss: -0.7128\n",
      "Graph 9: Epoch: 037, Loss: -0.6031\n",
      "Graph 9: Epoch: 038, Loss: -0.4755\n",
      "Graph 9: Epoch: 039, Loss: -0.3918\n",
      "Graph 9: Epoch: 040, Loss: -0.3243\n",
      "Graph 9: Epoch: 041, Loss: -0.3033\n",
      "Graph 9: Epoch: 042, Loss: -0.4660\n",
      "Graph 9: Epoch: 043, Loss: -0.1052\n",
      "Graph 9: Epoch: 044, Loss: -0.4526\n",
      "Graph 9: Epoch: 045, Loss: -0.4445\n",
      "Graph 9: Epoch: 046, Loss: -0.5425\n",
      "Graph 9: Epoch: 047, Loss: -0.1597\n",
      "Graph 9: Epoch: 048, Loss: -0.6077\n",
      "Graph 9: Epoch: 049, Loss: -0.2171\n",
      "Graph 9: Epoch: 050, Loss: -0.4482\n",
      "Graph 9: Epoch: 051, Loss: -0.1808\n",
      "Graph 9: Epoch: 052, Loss: -0.3380\n",
      "Graph 9: Epoch: 053, Loss: -0.8118\n",
      "Graph 9: Epoch: 054, Loss: -0.1462\n",
      "Graph 9: Epoch: 055, Loss: -0.4026\n",
      "Graph 9: Epoch: 056, Loss: -0.3683\n",
      "Graph 9: Epoch: 057, Loss: -0.5462\n",
      "Graph 9: Epoch: 058, Loss: -0.4601\n",
      "Graph 9: Epoch: 059, Loss: -0.5440\n",
      "Graph 9: Epoch: 060, Loss: -0.5538\n",
      "Graph 9: Epoch: 061, Loss: -0.4762\n",
      "Graph 9: Epoch: 062, Loss: -0.3401\n",
      "Graph 9: Epoch: 063, Loss: -0.5501\n",
      "Graph 9: Epoch: 064, Loss: -0.7065\n",
      "Graph 9: Epoch: 065, Loss: -0.9089\n",
      "Graph 9: Epoch: 066, Loss: -0.5043\n",
      "Graph 9: Epoch: 067, Loss: -0.5138\n",
      "Graph 9: Epoch: 068, Loss: -0.6310\n",
      "Graph 9: Epoch: 069, Loss: -0.3507\n",
      "Graph 9: Epoch: 070, Loss: -0.0266\n",
      "Graph 9: Epoch: 071, Loss: -0.3115\n",
      "Graph 9: Epoch: 072, Loss: -0.7241\n",
      "Graph 9: Epoch: 073, Loss: -0.4125\n",
      "Graph 9: Epoch: 074, Loss: -0.4755\n",
      "Graph 9: Epoch: 075, Loss: -0.6991\n",
      "Graph 9: Epoch: 076, Loss: -0.4899\n",
      "Graph 9: Epoch: 077, Loss: -0.8208\n",
      "Graph 9: Epoch: 078, Loss: -0.1392\n",
      "Graph 9: Epoch: 079, Loss: -0.3015\n",
      "Graph 9: Epoch: 080, Loss: -0.6394\n",
      "Graph 9: Epoch: 081, Loss: -0.3205\n",
      "Graph 9: Epoch: 082, Loss: -0.6608\n",
      "Graph 9: Epoch: 083, Loss: -0.9230\n",
      "Graph 9: Epoch: 084, Loss: -0.3241\n",
      "Graph 9: Epoch: 085, Loss: -0.6938\n",
      "Graph 9: Epoch: 086, Loss: -0.8297\n",
      "Graph 9: Epoch: 087, Loss: -0.3803\n",
      "Graph 9: Epoch: 088, Loss: -0.4226\n",
      "Graph 9: Epoch: 089, Loss: -0.9411\n",
      "Graph 9: Epoch: 090, Loss: -0.9373\n",
      "Graph 9: Epoch: 091, Loss: -0.1461\n",
      "Graph 9: Epoch: 092, Loss: -0.3134\n",
      "Graph 9: Epoch: 093, Loss: -0.4781\n",
      "Graph 9: Epoch: 094, Loss: -0.8121\n",
      "Graph 9: Epoch: 095, Loss: -0.6026\n",
      "Graph 9: Epoch: 096, Loss: -0.5886\n",
      "Graph 9: Epoch: 097, Loss: -0.1159\n",
      "Graph 9: Epoch: 098, Loss: -0.2761\n",
      "Graph 9: Epoch: 099, Loss: -0.3177\n",
      "Graph 9: Epoch: 100, Loss: -0.3333\n",
      "Graph 10: Epoch: 001, Loss: -0.0758\n",
      "Graph 10: Epoch: 002, Loss: -0.2877\n",
      "Graph 10: Epoch: 003, Loss: -0.3808\n",
      "Graph 10: Epoch: 004, Loss: -0.2659\n",
      "Graph 10: Epoch: 005, Loss: -0.4308\n",
      "Graph 10: Epoch: 006, Loss: -0.3286\n",
      "Graph 10: Epoch: 007, Loss: -0.3311\n",
      "Graph 10: Epoch: 008, Loss: -0.4139\n",
      "Graph 10: Epoch: 009, Loss: -0.4534\n",
      "Graph 10: Epoch: 010, Loss: -0.6259\n",
      "Graph 10: Epoch: 011, Loss: -0.4072\n",
      "Graph 10: Epoch: 012, Loss: -0.5821\n",
      "Graph 10: Epoch: 013, Loss: -0.2962\n",
      "Graph 10: Epoch: 014, Loss: -0.4058\n",
      "Graph 10: Epoch: 015, Loss: -0.3568\n",
      "Graph 10: Epoch: 016, Loss: -0.4735\n",
      "Graph 10: Epoch: 017, Loss: -0.6499\n",
      "Graph 10: Epoch: 018, Loss: -0.3613\n",
      "Graph 10: Epoch: 019, Loss: -0.5359\n",
      "Graph 10: Epoch: 020, Loss: -0.3594\n",
      "Graph 10: Epoch: 021, Loss: -0.6030\n",
      "Graph 10: Epoch: 022, Loss: -0.5999\n",
      "Graph 10: Epoch: 023, Loss: -0.4818\n",
      "Graph 10: Epoch: 024, Loss: -0.7270\n",
      "Graph 10: Epoch: 025, Loss: -0.3630\n",
      "Graph 10: Epoch: 026, Loss: -0.4269\n",
      "Graph 10: Epoch: 027, Loss: -0.4880\n",
      "Graph 10: Epoch: 028, Loss: -0.6643\n",
      "Graph 10: Epoch: 029, Loss: -0.4892\n",
      "Graph 10: Epoch: 030, Loss: -0.3062\n",
      "Graph 10: Epoch: 031, Loss: -0.5501\n",
      "Graph 10: Epoch: 032, Loss: -0.3678\n",
      "Graph 10: Epoch: 033, Loss: -0.4291\n",
      "Graph 10: Epoch: 034, Loss: -0.4280\n",
      "Graph 10: Epoch: 035, Loss: -0.3689\n",
      "Graph 10: Epoch: 036, Loss: -0.6132\n",
      "Graph 10: Epoch: 037, Loss: -0.4291\n",
      "Graph 10: Epoch: 038, Loss: -0.5517\n",
      "Graph 10: Epoch: 039, Loss: -0.4918\n",
      "Graph 10: Epoch: 040, Loss: -0.3696\n",
      "Graph 10: Epoch: 041, Loss: -0.5546\n",
      "Graph 10: Epoch: 042, Loss: -0.4919\n",
      "Graph 10: Epoch: 043, Loss: -0.3699\n",
      "Graph 10: Epoch: 044, Loss: -0.4318\n",
      "Graph 10: Epoch: 045, Loss: -0.3695\n",
      "Graph 10: Epoch: 046, Loss: -0.4297\n",
      "Graph 10: Epoch: 047, Loss: -0.5557\n",
      "Graph 10: Epoch: 048, Loss: -0.4325\n",
      "Graph 10: Epoch: 049, Loss: -0.4328\n",
      "Graph 10: Epoch: 050, Loss: -0.4942\n",
      "Graph 10: Epoch: 051, Loss: -0.2475\n",
      "Graph 10: Epoch: 052, Loss: -0.3709\n",
      "Graph 10: Epoch: 053, Loss: -0.4327\n",
      "Graph 10: Epoch: 054, Loss: -0.3714\n",
      "Graph 10: Epoch: 055, Loss: -0.4333\n",
      "Graph 10: Epoch: 056, Loss: -0.3712\n",
      "Graph 10: Epoch: 057, Loss: -0.3096\n",
      "Graph 10: Epoch: 058, Loss: -0.4333\n",
      "Graph 10: Epoch: 059, Loss: -0.4334\n",
      "Graph 10: Epoch: 060, Loss: -0.5571\n",
      "Graph 10: Epoch: 061, Loss: -0.3099\n",
      "Graph 10: Epoch: 062, Loss: -0.7428\n",
      "Graph 10: Epoch: 063, Loss: -0.5575\n",
      "Graph 10: Epoch: 064, Loss: -0.3720\n",
      "Graph 10: Epoch: 065, Loss: -0.6195\n",
      "Graph 10: Epoch: 066, Loss: -0.4340\n",
      "Graph 10: Epoch: 067, Loss: -0.6817\n",
      "Graph 10: Epoch: 068, Loss: -0.4341\n",
      "Graph 10: Epoch: 069, Loss: -0.4339\n",
      "Graph 10: Epoch: 070, Loss: -0.4338\n",
      "Graph 10: Epoch: 071, Loss: -0.4961\n",
      "Graph 10: Epoch: 072, Loss: -0.3101\n",
      "Graph 10: Epoch: 073, Loss: -0.3722\n",
      "Graph 10: Epoch: 074, Loss: -0.4965\n",
      "Graph 10: Epoch: 075, Loss: -0.4344\n",
      "Graph 10: Epoch: 076, Loss: -0.4964\n",
      "Graph 10: Epoch: 077, Loss: -0.6825\n",
      "Graph 10: Epoch: 078, Loss: -0.4966\n",
      "Graph 10: Epoch: 079, Loss: -0.4345\n",
      "Graph 10: Epoch: 080, Loss: -0.4966\n",
      "Graph 10: Epoch: 081, Loss: -0.3725\n",
      "Graph 10: Epoch: 082, Loss: -0.4968\n",
      "Graph 10: Epoch: 083, Loss: -0.4347\n",
      "Graph 10: Epoch: 084, Loss: -0.5588\n",
      "Graph 10: Epoch: 085, Loss: -0.6209\n",
      "Graph 10: Epoch: 086, Loss: -0.4970\n",
      "Graph 10: Epoch: 087, Loss: -0.6211\n",
      "Graph 10: Epoch: 088, Loss: -0.3107\n",
      "Graph 10: Epoch: 089, Loss: -0.6831\n",
      "Graph 10: Epoch: 090, Loss: -0.3728\n",
      "Graph 10: Epoch: 091, Loss: -0.3729\n",
      "Graph 10: Epoch: 092, Loss: -0.4972\n",
      "Graph 10: Epoch: 093, Loss: -0.4351\n",
      "Graph 10: Epoch: 094, Loss: -0.5593\n",
      "Graph 10: Epoch: 095, Loss: -0.4972\n",
      "Graph 10: Epoch: 096, Loss: -0.3730\n",
      "Graph 10: Epoch: 097, Loss: -0.4973\n",
      "Graph 10: Epoch: 098, Loss: -0.4974\n",
      "Graph 10: Epoch: 099, Loss: -0.4974\n",
      "Graph 10: Epoch: 100, Loss: -0.5596\n",
      "Graph 11: Epoch: 001, Loss: -0.0165\n",
      "Graph 11: Epoch: 002, Loss: -0.0190\n",
      "Graph 11: Epoch: 003, Loss: -0.0782\n",
      "Graph 11: Epoch: 004, Loss: -0.1863\n",
      "Graph 11: Epoch: 005, Loss: -0.1107\n",
      "Graph 11: Epoch: 006, Loss: -0.2816\n",
      "Graph 11: Epoch: 007, Loss: -0.4201\n",
      "Graph 11: Epoch: 008, Loss: -0.3628\n",
      "Graph 11: Epoch: 009, Loss: -0.4078\n",
      "Graph 11: Epoch: 010, Loss: -0.3778\n",
      "Graph 11: Epoch: 011, Loss: -0.3505\n",
      "Graph 11: Epoch: 012, Loss: -0.2724\n",
      "Graph 11: Epoch: 013, Loss: -0.2506\n",
      "Graph 11: Epoch: 014, Loss: -0.4452\n",
      "Graph 11: Epoch: 015, Loss: -0.4452\n",
      "Graph 11: Epoch: 016, Loss: -0.4412\n",
      "Graph 11: Epoch: 017, Loss: -0.5264\n",
      "Graph 11: Epoch: 018, Loss: -0.5093\n",
      "Graph 11: Epoch: 019, Loss: -0.3706\n",
      "Graph 11: Epoch: 020, Loss: -0.5436\n",
      "Graph 11: Epoch: 021, Loss: -0.3645\n",
      "Graph 11: Epoch: 022, Loss: -0.5611\n",
      "Graph 11: Epoch: 023, Loss: -0.5575\n",
      "Graph 11: Epoch: 024, Loss: -0.3736\n",
      "Graph 11: Epoch: 025, Loss: -0.2821\n",
      "Graph 11: Epoch: 026, Loss: -0.3803\n",
      "Graph 11: Epoch: 027, Loss: -0.4739\n",
      "Graph 11: Epoch: 028, Loss: -0.5689\n",
      "Graph 11: Epoch: 029, Loss: -0.5723\n",
      "Graph 11: Epoch: 030, Loss: -0.4792\n",
      "Graph 11: Epoch: 031, Loss: -0.3846\n",
      "Graph 11: Epoch: 032, Loss: -0.5716\n",
      "Graph 11: Epoch: 033, Loss: -0.4797\n",
      "Graph 11: Epoch: 034, Loss: -0.3724\n",
      "Graph 11: Epoch: 035, Loss: -0.2064\n",
      "Graph 11: Epoch: 036, Loss: -0.6719\n",
      "Graph 11: Epoch: 037, Loss: -0.8640\n",
      "Graph 11: Epoch: 038, Loss: -0.5749\n",
      "Graph 11: Epoch: 039, Loss: -0.2909\n",
      "Graph 11: Epoch: 040, Loss: -0.0999\n",
      "Graph 11: Epoch: 041, Loss: -0.6771\n",
      "Graph 11: Epoch: 042, Loss: -0.4866\n",
      "Graph 11: Epoch: 043, Loss: -0.5822\n",
      "Graph 11: Epoch: 044, Loss: -0.3903\n",
      "Graph 11: Epoch: 045, Loss: -0.4842\n",
      "Graph 11: Epoch: 046, Loss: -0.3901\n",
      "Graph 11: Epoch: 047, Loss: -0.2937\n",
      "Graph 11: Epoch: 048, Loss: -0.5859\n",
      "Graph 11: Epoch: 049, Loss: -0.5858\n",
      "Graph 11: Epoch: 050, Loss: -0.3859\n",
      "Graph 11: Epoch: 051, Loss: -0.5863\n",
      "Graph 11: Epoch: 052, Loss: -0.2946\n",
      "Graph 11: Epoch: 053, Loss: -0.1972\n",
      "Graph 11: Epoch: 054, Loss: -0.4897\n",
      "Graph 11: Epoch: 055, Loss: -0.6854\n",
      "Graph 11: Epoch: 056, Loss: -0.5886\n",
      "Graph 11: Epoch: 057, Loss: -0.5881\n",
      "Graph 11: Epoch: 058, Loss: -0.2954\n",
      "Graph 11: Epoch: 059, Loss: -0.4909\n",
      "Graph 11: Epoch: 060, Loss: -0.4914\n",
      "Graph 11: Epoch: 061, Loss: -0.3937\n",
      "Graph 11: Epoch: 062, Loss: -0.2947\n",
      "Graph 11: Epoch: 063, Loss: -0.2133\n",
      "Graph 11: Epoch: 064, Loss: -0.4920\n",
      "Graph 11: Epoch: 065, Loss: -0.4920\n",
      "Graph 11: Epoch: 066, Loss: -0.5903\n",
      "Graph 11: Epoch: 067, Loss: -0.4932\n",
      "Graph 11: Epoch: 068, Loss: -0.1982\n",
      "Graph 11: Epoch: 069, Loss: -0.5911\n",
      "Graph 11: Epoch: 070, Loss: -0.3916\n",
      "Graph 11: Epoch: 071, Loss: -0.4932\n",
      "Graph 11: Epoch: 072, Loss: -0.2967\n",
      "Graph 11: Epoch: 073, Loss: -0.4933\n",
      "Graph 11: Epoch: 074, Loss: -0.2965\n",
      "Graph 11: Epoch: 075, Loss: -0.3906\n",
      "Graph 11: Epoch: 076, Loss: -0.7890\n",
      "Graph 11: Epoch: 077, Loss: -0.4933\n",
      "Graph 11: Epoch: 078, Loss: -0.4935\n",
      "Graph 11: Epoch: 079, Loss: -0.4935\n",
      "Graph 11: Epoch: 080, Loss: -0.5916\n",
      "Graph 11: Epoch: 081, Loss: -0.3953\n",
      "Graph 11: Epoch: 082, Loss: -0.3945\n",
      "Graph 11: Epoch: 083, Loss: -0.2972\n",
      "Graph 11: Epoch: 084, Loss: -0.4931\n",
      "Graph 11: Epoch: 085, Loss: -0.4940\n",
      "Graph 11: Epoch: 086, Loss: -0.6913\n",
      "Graph 11: Epoch: 087, Loss: -0.4950\n",
      "Graph 11: Epoch: 088, Loss: -0.4948\n",
      "Graph 11: Epoch: 089, Loss: -0.3959\n",
      "Graph 11: Epoch: 090, Loss: -0.2970\n",
      "Graph 11: Epoch: 091, Loss: -0.7907\n",
      "Graph 11: Epoch: 092, Loss: -0.2980\n",
      "Graph 11: Epoch: 093, Loss: -0.2977\n",
      "Graph 11: Epoch: 094, Loss: -0.3131\n",
      "Graph 11: Epoch: 095, Loss: -0.4952\n",
      "Graph 11: Epoch: 096, Loss: -0.5938\n",
      "Graph 11: Epoch: 097, Loss: -0.5099\n",
      "Graph 11: Epoch: 098, Loss: -0.4954\n",
      "Graph 11: Epoch: 099, Loss: -0.4951\n",
      "Graph 11: Epoch: 100, Loss: -0.1993\n",
      "Graph 12: Epoch: 001, Loss: -0.1762\n",
      "Graph 12: Epoch: 002, Loss: -0.1948\n",
      "Graph 12: Epoch: 003, Loss: -0.1419\n",
      "Graph 12: Epoch: 004, Loss: -0.4983\n",
      "Graph 12: Epoch: 005, Loss: -0.2024\n",
      "Graph 12: Epoch: 006, Loss: -0.3683\n",
      "Graph 12: Epoch: 007, Loss: -0.2720\n",
      "Graph 12: Epoch: 008, Loss: -0.3869\n",
      "Graph 12: Epoch: 009, Loss: -0.5226\n",
      "Graph 12: Epoch: 010, Loss: -0.3287\n",
      "Graph 12: Epoch: 011, Loss: -0.3274\n",
      "Graph 12: Epoch: 012, Loss: -0.3140\n",
      "Graph 12: Epoch: 013, Loss: -0.2716\n",
      "Graph 12: Epoch: 014, Loss: -0.2026\n",
      "Graph 12: Epoch: 015, Loss: -0.1518\n",
      "Graph 12: Epoch: 016, Loss: -0.4338\n",
      "Graph 12: Epoch: 017, Loss: -0.1339\n",
      "Graph 12: Epoch: 018, Loss: -0.4178\n",
      "Graph 12: Epoch: 019, Loss: -0.6997\n",
      "Graph 12: Epoch: 020, Loss: -0.3056\n",
      "Graph 12: Epoch: 021, Loss: -0.2502\n",
      "Graph 12: Epoch: 022, Loss: -0.3157\n",
      "Graph 12: Epoch: 023, Loss: -0.6405\n",
      "Graph 12: Epoch: 024, Loss: -0.0325\n",
      "Graph 12: Epoch: 025, Loss: -0.5370\n",
      "Graph 12: Epoch: 026, Loss: -0.3332\n",
      "Graph 12: Epoch: 027, Loss: -0.1990\n",
      "Graph 12: Epoch: 028, Loss: -0.3011\n",
      "Graph 12: Epoch: 029, Loss: -0.1232\n",
      "Graph 12: Epoch: 030, Loss: -0.1578\n",
      "Graph 12: Epoch: 031, Loss: -0.7444\n",
      "Graph 12: Epoch: 032, Loss: -0.2466\n",
      "Graph 12: Epoch: 033, Loss: -0.5121\n",
      "Graph 12: Epoch: 034, Loss: -0.1585\n",
      "Graph 12: Epoch: 035, Loss: -0.5673\n",
      "Graph 12: Epoch: 036, Loss: -0.5958\n",
      "Graph 12: Epoch: 037, Loss: -0.5514\n",
      "Graph 12: Epoch: 038, Loss: -0.3288\n",
      "Graph 12: Epoch: 039, Loss: -0.3281\n",
      "Graph 12: Epoch: 040, Loss: -0.4753\n",
      "Graph 12: Epoch: 041, Loss: -0.5109\n",
      "Graph 12: Epoch: 042, Loss: -0.5706\n",
      "Graph 12: Epoch: 043, Loss: -0.7690\n",
      "Graph 12: Epoch: 044, Loss: -0.3527\n",
      "Graph 12: Epoch: 045, Loss: -0.5201\n",
      "Graph 12: Epoch: 046, Loss: -0.6538\n",
      "Graph 12: Epoch: 047, Loss: -0.6212\n",
      "Graph 12: Epoch: 048, Loss: -0.6555\n",
      "Graph 12: Epoch: 049, Loss: -0.4791\n",
      "Graph 12: Epoch: 050, Loss: -0.7101\n",
      "Graph 12: Epoch: 051, Loss: -0.6344\n",
      "Graph 12: Epoch: 052, Loss: -0.6279\n",
      "Graph 12: Epoch: 053, Loss: -0.4703\n",
      "Graph 12: Epoch: 054, Loss: -0.1720\n",
      "Graph 12: Epoch: 055, Loss: -0.4859\n",
      "Graph 12: Epoch: 056, Loss: -0.4836\n",
      "Graph 12: Epoch: 057, Loss: -0.3540\n",
      "Graph 12: Epoch: 058, Loss: -0.4754\n",
      "Graph 12: Epoch: 059, Loss: -0.7500\n",
      "Graph 12: Epoch: 060, Loss: -0.3265\n",
      "Graph 12: Epoch: 061, Loss: -0.2885\n",
      "Graph 12: Epoch: 062, Loss: -0.1678\n",
      "Graph 12: Epoch: 063, Loss: -0.3261\n",
      "Graph 12: Epoch: 064, Loss: -0.3233\n",
      "Graph 12: Epoch: 065, Loss: -0.0060\n",
      "Graph 12: Epoch: 066, Loss: -0.6423\n",
      "Graph 12: Epoch: 067, Loss: -0.6444\n",
      "Graph 12: Epoch: 068, Loss: -0.6510\n",
      "Graph 12: Epoch: 069, Loss: -0.2082\n",
      "Graph 12: Epoch: 070, Loss: -0.4887\n",
      "Graph 12: Epoch: 071, Loss: -0.3265\n",
      "Graph 12: Epoch: 072, Loss: -0.3651\n",
      "Graph 12: Epoch: 073, Loss: -0.6493\n",
      "Graph 12: Epoch: 074, Loss: -0.5575\n",
      "Graph 12: Epoch: 075, Loss: -0.5217\n",
      "Graph 12: Epoch: 076, Loss: -0.6507\n",
      "Graph 12: Epoch: 077, Loss: -0.8058\n",
      "Graph 12: Epoch: 078, Loss: -0.3263\n",
      "Graph 12: Epoch: 079, Loss: -0.6389\n",
      "Graph 12: Epoch: 080, Loss: -0.6458\n",
      "Graph 12: Epoch: 081, Loss: -0.8136\n",
      "Graph 12: Epoch: 082, Loss: -0.7758\n",
      "Graph 12: Epoch: 083, Loss: -0.6462\n",
      "Graph 12: Epoch: 084, Loss: -0.4807\n",
      "Graph 12: Epoch: 085, Loss: -0.1690\n",
      "Graph 12: Epoch: 086, Loss: -0.5711\n",
      "Graph 12: Epoch: 087, Loss: -0.6529\n",
      "Graph 12: Epoch: 088, Loss: -0.4937\n",
      "Graph 12: Epoch: 089, Loss: -0.9656\n",
      "Graph 12: Epoch: 090, Loss: -0.3264\n",
      "Graph 12: Epoch: 091, Loss: -0.3279\n",
      "Graph 12: Epoch: 092, Loss: -0.5286\n",
      "Graph 12: Epoch: 093, Loss: -0.1663\n",
      "Graph 12: Epoch: 094, Loss: -0.5112\n",
      "Graph 12: Epoch: 095, Loss: -0.2720\n",
      "Graph 12: Epoch: 096, Loss: -0.6376\n",
      "Graph 12: Epoch: 097, Loss: -0.3616\n",
      "Graph 12: Epoch: 098, Loss: -0.6856\n",
      "Graph 12: Epoch: 099, Loss: -0.6549\n",
      "Graph 12: Epoch: 100, Loss: -0.4922\n",
      "Graph 13: Epoch: 001, Loss: -0.0217\n",
      "Graph 13: Epoch: 002, Loss: -0.0577\n",
      "Graph 13: Epoch: 003, Loss: -0.0929\n",
      "Graph 13: Epoch: 004, Loss: -0.1630\n",
      "Graph 13: Epoch: 005, Loss: -0.2599\n",
      "Graph 13: Epoch: 006, Loss: -0.2855\n",
      "Graph 13: Epoch: 007, Loss: -0.2860\n",
      "Graph 13: Epoch: 008, Loss: -0.3174\n",
      "Graph 13: Epoch: 009, Loss: -0.3981\n",
      "Graph 13: Epoch: 010, Loss: -0.3441\n",
      "Graph 13: Epoch: 011, Loss: -0.3740\n",
      "Graph 13: Epoch: 012, Loss: -0.3930\n",
      "Graph 13: Epoch: 013, Loss: -0.3243\n",
      "Graph 13: Epoch: 014, Loss: -0.5243\n",
      "Graph 13: Epoch: 015, Loss: -0.3580\n",
      "Graph 13: Epoch: 016, Loss: -0.3611\n",
      "Graph 13: Epoch: 017, Loss: -0.5079\n",
      "Graph 13: Epoch: 018, Loss: -0.3894\n",
      "Graph 13: Epoch: 019, Loss: -0.4252\n",
      "Graph 13: Epoch: 020, Loss: -0.2677\n",
      "Graph 13: Epoch: 021, Loss: -0.4163\n",
      "Graph 13: Epoch: 022, Loss: -0.6306\n",
      "Graph 13: Epoch: 023, Loss: -0.5992\n",
      "Graph 13: Epoch: 024, Loss: -0.4248\n",
      "Graph 13: Epoch: 025, Loss: -0.5557\n",
      "Graph 13: Epoch: 026, Loss: -0.5653\n",
      "Graph 13: Epoch: 027, Loss: -0.3546\n",
      "Graph 13: Epoch: 028, Loss: -0.4672\n",
      "Graph 13: Epoch: 029, Loss: -0.5525\n",
      "Graph 13: Epoch: 030, Loss: -0.6906\n",
      "Graph 13: Epoch: 031, Loss: -0.6861\n",
      "Graph 13: Epoch: 032, Loss: -0.5869\n",
      "Graph 13: Epoch: 033, Loss: -0.5951\n",
      "Graph 13: Epoch: 034, Loss: -0.4738\n",
      "Graph 13: Epoch: 035, Loss: -0.5965\n",
      "Graph 13: Epoch: 036, Loss: -0.4747\n",
      "Graph 13: Epoch: 037, Loss: -0.1241\n",
      "Graph 13: Epoch: 038, Loss: -0.4809\n",
      "Graph 13: Epoch: 039, Loss: -0.9583\n",
      "Graph 13: Epoch: 040, Loss: -0.3591\n",
      "Graph 13: Epoch: 041, Loss: -0.4835\n",
      "Graph 13: Epoch: 042, Loss: -0.3572\n",
      "Graph 13: Epoch: 043, Loss: -0.6008\n",
      "Graph 13: Epoch: 044, Loss: -0.4845\n",
      "Graph 13: Epoch: 045, Loss: -0.1256\n",
      "Graph 13: Epoch: 046, Loss: -0.4819\n",
      "Graph 13: Epoch: 047, Loss: -0.6048\n",
      "Graph 13: Epoch: 048, Loss: -0.7245\n",
      "Graph 13: Epoch: 049, Loss: -0.7223\n",
      "Graph 13: Epoch: 050, Loss: -0.0023\n",
      "Graph 13: Epoch: 051, Loss: -0.6074\n",
      "Graph 13: Epoch: 052, Loss: -0.3665\n",
      "Graph 13: Epoch: 053, Loss: -0.2447\n",
      "Graph 13: Epoch: 054, Loss: -0.4881\n",
      "Graph 13: Epoch: 055, Loss: -0.3664\n",
      "Graph 13: Epoch: 056, Loss: -0.4913\n",
      "Graph 13: Epoch: 057, Loss: -0.3675\n",
      "Graph 13: Epoch: 058, Loss: -0.3673\n",
      "Graph 13: Epoch: 059, Loss: -0.3805\n",
      "Graph 13: Epoch: 060, Loss: -0.6104\n",
      "Graph 13: Epoch: 061, Loss: -0.7332\n",
      "Graph 13: Epoch: 062, Loss: -0.6360\n",
      "Graph 13: Epoch: 063, Loss: -0.6121\n",
      "Graph 13: Epoch: 064, Loss: -0.6125\n",
      "Graph 13: Epoch: 065, Loss: -0.6260\n",
      "Graph 13: Epoch: 066, Loss: -0.6112\n",
      "Graph 13: Epoch: 067, Loss: -0.4891\n",
      "Graph 13: Epoch: 068, Loss: -0.4897\n",
      "Graph 13: Epoch: 069, Loss: -0.3688\n",
      "Graph 13: Epoch: 070, Loss: -0.1512\n",
      "Graph 13: Epoch: 071, Loss: -0.6121\n",
      "Graph 13: Epoch: 072, Loss: -0.7304\n",
      "Graph 13: Epoch: 073, Loss: -0.4913\n",
      "Graph 13: Epoch: 074, Loss: -0.4888\n",
      "Graph 13: Epoch: 075, Loss: -0.8577\n",
      "Graph 13: Epoch: 076, Loss: -0.2458\n",
      "Graph 13: Epoch: 077, Loss: -0.1255\n",
      "Graph 13: Epoch: 078, Loss: -0.4298\n",
      "Graph 13: Epoch: 079, Loss: -0.7362\n",
      "Graph 13: Epoch: 080, Loss: -0.4922\n",
      "Graph 13: Epoch: 081, Loss: -0.6057\n",
      "Graph 13: Epoch: 082, Loss: -0.2526\n",
      "Graph 13: Epoch: 083, Loss: -0.3660\n",
      "Graph 13: Epoch: 084, Loss: -0.8577\n",
      "Graph 13: Epoch: 085, Loss: -0.4929\n",
      "Graph 13: Epoch: 086, Loss: -0.6151\n",
      "Graph 13: Epoch: 087, Loss: -0.3706\n",
      "Graph 13: Epoch: 088, Loss: -0.7568\n",
      "Graph 13: Epoch: 089, Loss: -0.8564\n",
      "Graph 13: Epoch: 090, Loss: -0.4408\n",
      "Graph 13: Epoch: 091, Loss: -0.4929\n",
      "Graph 13: Epoch: 092, Loss: -0.2479\n",
      "Graph 13: Epoch: 093, Loss: -0.4937\n",
      "Graph 13: Epoch: 094, Loss: -0.2470\n",
      "Graph 13: Epoch: 095, Loss: -0.6163\n",
      "Graph 13: Epoch: 096, Loss: -0.3676\n",
      "Graph 13: Epoch: 097, Loss: -0.4942\n",
      "Graph 13: Epoch: 098, Loss: -0.3718\n",
      "Graph 13: Epoch: 099, Loss: -0.2478\n",
      "Graph 13: Epoch: 100, Loss: -0.6173\n",
      "Graph 14: Epoch: 001, Loss: -0.0819\n",
      "Graph 14: Epoch: 002, Loss: -0.1678\n",
      "Graph 14: Epoch: 003, Loss: -0.3366\n",
      "Graph 14: Epoch: 004, Loss: -0.2518\n",
      "Graph 14: Epoch: 005, Loss: -0.1799\n",
      "Graph 14: Epoch: 006, Loss: -0.2326\n",
      "Graph 14: Epoch: 007, Loss: -0.4348\n",
      "Graph 14: Epoch: 008, Loss: -0.2773\n",
      "Graph 14: Epoch: 009, Loss: -0.3494\n",
      "Graph 14: Epoch: 010, Loss: -0.1689\n",
      "Graph 14: Epoch: 011, Loss: -0.4336\n",
      "Graph 14: Epoch: 012, Loss: -0.4222\n",
      "Graph 14: Epoch: 013, Loss: -0.3859\n",
      "Graph 14: Epoch: 014, Loss: -0.5027\n",
      "Graph 14: Epoch: 015, Loss: -0.2042\n",
      "Graph 14: Epoch: 016, Loss: -0.4557\n",
      "Graph 14: Epoch: 017, Loss: -0.4413\n",
      "Graph 14: Epoch: 018, Loss: -0.4723\n",
      "Graph 14: Epoch: 019, Loss: -0.7972\n",
      "Graph 14: Epoch: 020, Loss: -0.2747\n",
      "Graph 14: Epoch: 021, Loss: -0.3984\n",
      "Graph 14: Epoch: 022, Loss: -0.8615\n",
      "Graph 14: Epoch: 023, Loss: -0.3933\n",
      "Graph 14: Epoch: 024, Loss: -0.5684\n",
      "Graph 14: Epoch: 025, Loss: -0.6841\n",
      "Graph 14: Epoch: 026, Loss: -0.1450\n",
      "Graph 14: Epoch: 027, Loss: -0.2723\n",
      "Graph 14: Epoch: 028, Loss: -0.2709\n",
      "Graph 14: Epoch: 029, Loss: -0.1454\n",
      "Graph 14: Epoch: 030, Loss: -0.2806\n",
      "Graph 14: Epoch: 031, Loss: -0.4014\n",
      "Graph 14: Epoch: 032, Loss: -0.6660\n",
      "Graph 14: Epoch: 033, Loss: -0.4279\n",
      "Graph 14: Epoch: 034, Loss: -0.7979\n",
      "Graph 14: Epoch: 035, Loss: -0.4067\n",
      "Graph 14: Epoch: 036, Loss: -0.4528\n",
      "Graph 14: Epoch: 037, Loss: -0.4070\n",
      "Graph 14: Epoch: 038, Loss: -0.2436\n",
      "Graph 14: Epoch: 039, Loss: -0.6750\n",
      "Graph 14: Epoch: 040, Loss: -0.8138\n",
      "Graph 14: Epoch: 041, Loss: -0.4073\n",
      "Graph 14: Epoch: 042, Loss: -0.3089\n",
      "Graph 14: Epoch: 043, Loss: -0.4089\n",
      "Graph 14: Epoch: 044, Loss: -0.6835\n",
      "Graph 14: Epoch: 045, Loss: -0.2682\n",
      "Graph 14: Epoch: 046, Loss: -0.4132\n",
      "Graph 14: Epoch: 047, Loss: -0.6838\n",
      "Graph 14: Epoch: 048, Loss: -0.5468\n",
      "Graph 14: Epoch: 049, Loss: -0.5489\n",
      "Graph 14: Epoch: 050, Loss: -0.4068\n",
      "Graph 14: Epoch: 051, Loss: -0.5487\n",
      "Graph 14: Epoch: 052, Loss: -0.5910\n",
      "Graph 14: Epoch: 053, Loss: -0.5474\n",
      "Graph 14: Epoch: 054, Loss: -0.5537\n",
      "Graph 14: Epoch: 055, Loss: -0.5529\n",
      "Graph 14: Epoch: 056, Loss: -0.6811\n",
      "Graph 14: Epoch: 057, Loss: -0.4168\n",
      "Graph 14: Epoch: 058, Loss: -0.1418\n",
      "Graph 14: Epoch: 059, Loss: -0.5554\n",
      "Graph 14: Epoch: 060, Loss: -0.7176\n",
      "Graph 14: Epoch: 061, Loss: -0.8121\n",
      "Graph 14: Epoch: 062, Loss: -0.6922\n",
      "Graph 14: Epoch: 063, Loss: -0.5572\n",
      "Graph 14: Epoch: 064, Loss: -0.2794\n",
      "Graph 14: Epoch: 065, Loss: -0.5573\n",
      "Graph 14: Epoch: 066, Loss: -0.8330\n",
      "Graph 14: Epoch: 067, Loss: -0.2803\n",
      "Graph 14: Epoch: 068, Loss: -0.4194\n",
      "Graph 14: Epoch: 069, Loss: -0.0467\n",
      "Graph 14: Epoch: 070, Loss: -0.5580\n",
      "Graph 14: Epoch: 071, Loss: -0.4074\n",
      "Graph 14: Epoch: 072, Loss: -0.5597\n",
      "Graph 14: Epoch: 073, Loss: -0.4181\n",
      "Graph 14: Epoch: 074, Loss: -0.2811\n",
      "Graph 14: Epoch: 075, Loss: -0.5597\n",
      "Graph 14: Epoch: 076, Loss: -0.5531\n",
      "Graph 14: Epoch: 077, Loss: -0.5593\n",
      "Graph 14: Epoch: 078, Loss: -0.6986\n",
      "Graph 14: Epoch: 079, Loss: -0.4959\n",
      "Graph 14: Epoch: 080, Loss: -0.6991\n",
      "Graph 14: Epoch: 081, Loss: -0.3101\n",
      "Graph 14: Epoch: 082, Loss: -0.5606\n",
      "Graph 14: Epoch: 083, Loss: -0.4210\n",
      "Graph 14: Epoch: 084, Loss: -0.3397\n",
      "Graph 14: Epoch: 085, Loss: -0.7008\n",
      "Graph 14: Epoch: 086, Loss: -0.1420\n",
      "Graph 14: Epoch: 087, Loss: -0.5607\n",
      "Graph 14: Epoch: 088, Loss: -0.4230\n",
      "Graph 14: Epoch: 089, Loss: -0.4233\n",
      "Graph 14: Epoch: 090, Loss: -0.4532\n",
      "Graph 14: Epoch: 091, Loss: -0.5592\n",
      "Graph 14: Epoch: 092, Loss: -0.2826\n",
      "Graph 14: Epoch: 093, Loss: -0.1430\n",
      "Graph 14: Epoch: 094, Loss: -0.4217\n",
      "Graph 14: Epoch: 095, Loss: -0.2833\n",
      "Graph 14: Epoch: 096, Loss: -0.7013\n",
      "Graph 14: Epoch: 097, Loss: -0.5613\n",
      "Graph 14: Epoch: 098, Loss: -0.4224\n",
      "Graph 14: Epoch: 099, Loss: -0.5623\n",
      "Graph 14: Epoch: 100, Loss: -0.4222\n",
      "Graph 15: Epoch: 001, Loss: -0.1041\n",
      "Graph 15: Epoch: 002, Loss: -0.2311\n",
      "Graph 15: Epoch: 003, Loss: -0.1298\n",
      "Graph 15: Epoch: 004, Loss: -0.1014\n",
      "Graph 15: Epoch: 005, Loss: -0.1983\n",
      "Graph 15: Epoch: 006, Loss: -0.0921\n",
      "Graph 15: Epoch: 007, Loss: -0.3268\n",
      "Graph 15: Epoch: 008, Loss: -0.3118\n",
      "Graph 15: Epoch: 009, Loss: -0.2436\n",
      "Graph 15: Epoch: 010, Loss: -0.3147\n",
      "Graph 15: Epoch: 011, Loss: -0.2460\n",
      "Graph 15: Epoch: 012, Loss: -0.2327\n",
      "Graph 15: Epoch: 013, Loss: -0.2605\n",
      "Graph 15: Epoch: 014, Loss: -0.2427\n",
      "Graph 15: Epoch: 015, Loss: -0.2447\n",
      "Graph 15: Epoch: 016, Loss: -0.4212\n",
      "Graph 15: Epoch: 017, Loss: -0.2308\n",
      "Graph 15: Epoch: 018, Loss: -0.4214\n",
      "Graph 15: Epoch: 019, Loss: -0.4051\n",
      "Graph 15: Epoch: 020, Loss: -0.3928\n",
      "Graph 15: Epoch: 021, Loss: -0.3736\n",
      "Graph 15: Epoch: 022, Loss: -0.4744\n",
      "Graph 15: Epoch: 023, Loss: -0.1816\n",
      "Graph 15: Epoch: 024, Loss: -0.4143\n",
      "Graph 15: Epoch: 025, Loss: -0.6143\n",
      "Graph 15: Epoch: 026, Loss: -0.3335\n",
      "Graph 15: Epoch: 027, Loss: -0.5227\n",
      "Graph 15: Epoch: 028, Loss: -0.4701\n",
      "Graph 15: Epoch: 029, Loss: -0.2538\n",
      "Graph 15: Epoch: 030, Loss: -0.3987\n",
      "Graph 15: Epoch: 031, Loss: -0.5873\n",
      "Graph 15: Epoch: 032, Loss: -0.6923\n",
      "Graph 15: Epoch: 033, Loss: -0.2131\n",
      "Graph 15: Epoch: 034, Loss: -0.5550\n",
      "Graph 15: Epoch: 035, Loss: -0.3240\n",
      "Graph 15: Epoch: 036, Loss: -0.3194\n",
      "Graph 15: Epoch: 037, Loss: -0.0588\n",
      "Graph 15: Epoch: 038, Loss: -0.5062\n",
      "Graph 15: Epoch: 039, Loss: -0.3573\n",
      "Graph 15: Epoch: 040, Loss: -0.5282\n",
      "Graph 15: Epoch: 041, Loss: -0.0662\n",
      "Graph 15: Epoch: 042, Loss: -0.5694\n",
      "Graph 15: Epoch: 043, Loss: -0.6176\n",
      "Graph 15: Epoch: 044, Loss: -0.7594\n",
      "Graph 15: Epoch: 045, Loss: -0.2671\n",
      "Graph 15: Epoch: 046, Loss: -0.5660\n",
      "Graph 15: Epoch: 047, Loss: -0.3137\n",
      "Graph 15: Epoch: 048, Loss: -0.6051\n",
      "Graph 15: Epoch: 049, Loss: -0.4014\n",
      "Graph 15: Epoch: 050, Loss: -0.8575\n",
      "Graph 15: Epoch: 051, Loss: -0.4120\n",
      "Graph 15: Epoch: 052, Loss: -0.3481\n",
      "Graph 15: Epoch: 053, Loss: -0.3363\n",
      "Graph 15: Epoch: 054, Loss: -0.3445\n",
      "Graph 15: Epoch: 055, Loss: -0.1541\n",
      "Graph 15: Epoch: 056, Loss: -0.3199\n",
      "Graph 15: Epoch: 057, Loss: -0.5974\n",
      "Graph 15: Epoch: 058, Loss: -0.1328\n",
      "Graph 15: Epoch: 059, Loss: -0.2124\n",
      "Graph 15: Epoch: 060, Loss: -0.3143\n",
      "Graph 15: Epoch: 061, Loss: -0.9160\n",
      "Graph 15: Epoch: 062, Loss: -0.6811\n",
      "Graph 15: Epoch: 063, Loss: -0.2792\n",
      "Graph 15: Epoch: 064, Loss: -0.0437\n",
      "Graph 15: Epoch: 065, Loss: -0.5009\n",
      "Graph 15: Epoch: 066, Loss: -0.0425\n",
      "Graph 15: Epoch: 067, Loss: -0.4382\n",
      "Graph 15: Epoch: 068, Loss: -0.1326\n",
      "Graph 15: Epoch: 069, Loss: -0.3301\n",
      "Graph 15: Epoch: 070, Loss: -0.2377\n",
      "Graph 15: Epoch: 071, Loss: -0.4162\n",
      "Graph 15: Epoch: 072, Loss: -0.7105\n",
      "Graph 15: Epoch: 073, Loss: -0.3435\n",
      "Graph 15: Epoch: 074, Loss: -0.4399\n",
      "Graph 15: Epoch: 075, Loss: -0.6047\n",
      "Graph 15: Epoch: 076, Loss: -0.4174\n",
      "Graph 15: Epoch: 077, Loss: -0.5969\n",
      "Graph 15: Epoch: 078, Loss: -0.6206\n",
      "Graph 15: Epoch: 079, Loss: -0.6302\n",
      "Graph 15: Epoch: 080, Loss: -0.5007\n",
      "Graph 15: Epoch: 081, Loss: -0.3471\n",
      "Graph 15: Epoch: 082, Loss: -0.4187\n",
      "Graph 15: Epoch: 083, Loss: -0.7978\n",
      "Graph 15: Epoch: 084, Loss: -0.2166\n",
      "Graph 15: Epoch: 085, Loss: -0.3361\n",
      "Graph 15: Epoch: 086, Loss: -0.4569\n",
      "Graph 15: Epoch: 087, Loss: -0.4219\n",
      "Graph 15: Epoch: 088, Loss: -0.0265\n",
      "Graph 15: Epoch: 089, Loss: -0.3010\n",
      "Graph 15: Epoch: 090, Loss: -0.6309\n",
      "Graph 15: Epoch: 091, Loss: -0.5382\n",
      "Graph 15: Epoch: 092, Loss: -0.6945\n",
      "Graph 15: Epoch: 093, Loss: -0.3503\n",
      "Graph 15: Epoch: 094, Loss: -0.7234\n",
      "Graph 15: Epoch: 095, Loss: -0.0598\n",
      "Graph 15: Epoch: 096, Loss: -0.3367\n",
      "Graph 15: Epoch: 097, Loss: -0.3429\n",
      "Graph 15: Epoch: 098, Loss: -0.3014\n",
      "Graph 15: Epoch: 099, Loss: -0.4779\n",
      "Graph 15: Epoch: 100, Loss: -0.6137\n",
      "Graph 16: Epoch: 001, Loss: -0.2886\n",
      "Graph 16: Epoch: 002, Loss: -0.1220\n",
      "Graph 16: Epoch: 003, Loss: -0.3073\n",
      "Graph 16: Epoch: 004, Loss: -0.1857\n",
      "Graph 16: Epoch: 005, Loss: -0.5199\n",
      "Graph 16: Epoch: 006, Loss: -0.1231\n",
      "Graph 16: Epoch: 007, Loss: -0.2777\n",
      "Graph 16: Epoch: 008, Loss: -0.2823\n",
      "Graph 16: Epoch: 009, Loss: -0.4372\n",
      "Graph 16: Epoch: 010, Loss: -0.3341\n",
      "Graph 16: Epoch: 011, Loss: -0.2162\n",
      "Graph 16: Epoch: 012, Loss: -0.1242\n",
      "Graph 16: Epoch: 013, Loss: -0.5327\n",
      "Graph 16: Epoch: 014, Loss: -0.5186\n",
      "Graph 16: Epoch: 015, Loss: -0.3881\n",
      "Graph 16: Epoch: 016, Loss: -0.2906\n",
      "Graph 16: Epoch: 017, Loss: -0.6015\n",
      "Graph 16: Epoch: 018, Loss: -0.4491\n",
      "Graph 16: Epoch: 019, Loss: -0.6335\n",
      "Graph 16: Epoch: 020, Loss: -0.3800\n",
      "Graph 16: Epoch: 021, Loss: -0.5976\n",
      "Graph 16: Epoch: 022, Loss: -0.2609\n",
      "Graph 16: Epoch: 023, Loss: -0.3865\n",
      "Graph 16: Epoch: 024, Loss: -0.4384\n",
      "Graph 16: Epoch: 025, Loss: -0.4199\n",
      "Graph 16: Epoch: 026, Loss: -0.3234\n",
      "Graph 16: Epoch: 027, Loss: -0.4250\n",
      "Graph 16: Epoch: 028, Loss: -0.8041\n",
      "Graph 16: Epoch: 029, Loss: -0.3599\n",
      "Graph 16: Epoch: 030, Loss: -0.2416\n",
      "Graph 16: Epoch: 031, Loss: -0.6175\n",
      "Graph 16: Epoch: 032, Loss: -0.4479\n",
      "Graph 16: Epoch: 033, Loss: -0.4093\n",
      "Graph 16: Epoch: 034, Loss: -0.0623\n",
      "Graph 16: Epoch: 035, Loss: -0.7621\n",
      "Graph 16: Epoch: 036, Loss: -0.8587\n",
      "Graph 16: Epoch: 037, Loss: -0.3902\n",
      "Graph 16: Epoch: 038, Loss: -0.4674\n",
      "Graph 16: Epoch: 039, Loss: -0.5126\n",
      "Graph 16: Epoch: 040, Loss: -0.6955\n",
      "Graph 16: Epoch: 041, Loss: -0.4404\n",
      "Graph 16: Epoch: 042, Loss: -0.3121\n",
      "Graph 16: Epoch: 043, Loss: -0.6568\n",
      "Graph 16: Epoch: 044, Loss: -0.3269\n",
      "Graph 16: Epoch: 045, Loss: -0.3097\n",
      "Graph 16: Epoch: 046, Loss: -0.1951\n",
      "Graph 16: Epoch: 047, Loss: -0.1451\n",
      "Graph 16: Epoch: 048, Loss: -0.5610\n",
      "Graph 16: Epoch: 049, Loss: -0.5781\n",
      "Graph 16: Epoch: 050, Loss: -0.8155\n",
      "Graph 16: Epoch: 051, Loss: -0.2902\n",
      "Graph 16: Epoch: 052, Loss: -0.3194\n",
      "Graph 16: Epoch: 053, Loss: -0.8735\n",
      "Graph 16: Epoch: 054, Loss: -0.1288\n",
      "Graph 16: Epoch: 055, Loss: -0.8940\n",
      "Graph 16: Epoch: 056, Loss: -0.5694\n",
      "Graph 16: Epoch: 057, Loss: -0.6123\n",
      "Graph 16: Epoch: 058, Loss: -0.4017\n",
      "Graph 16: Epoch: 059, Loss: -0.2800\n",
      "Graph 16: Epoch: 060, Loss: -0.4529\n",
      "Graph 16: Epoch: 061, Loss: -0.4419\n",
      "Graph 16: Epoch: 062, Loss: -0.9218\n",
      "Graph 16: Epoch: 063, Loss: -0.4649\n",
      "Graph 16: Epoch: 064, Loss: -0.3187\n",
      "Graph 16: Epoch: 065, Loss: -0.6698\n",
      "Graph 16: Epoch: 066, Loss: -0.4683\n",
      "Graph 16: Epoch: 067, Loss: -0.1370\n",
      "Graph 16: Epoch: 068, Loss: -0.2609\n",
      "Graph 16: Epoch: 069, Loss: -0.7922\n",
      "Graph 16: Epoch: 070, Loss: -0.1385\n",
      "Graph 16: Epoch: 071, Loss: -0.0215\n",
      "Graph 16: Epoch: 072, Loss: -0.3367\n",
      "Graph 16: Epoch: 073, Loss: -0.4830\n",
      "Graph 16: Epoch: 074, Loss: -0.6329\n",
      "Graph 16: Epoch: 075, Loss: -0.3500\n",
      "Graph 16: Epoch: 076, Loss: -0.0109\n",
      "Graph 16: Epoch: 077, Loss: -0.1847\n",
      "Graph 16: Epoch: 078, Loss: -0.0225\n",
      "Graph 16: Epoch: 079, Loss: -0.4141\n",
      "Graph 16: Epoch: 080, Loss: -0.9282\n",
      "Graph 16: Epoch: 081, Loss: -0.9393\n",
      "Graph 16: Epoch: 082, Loss: -0.3834\n",
      "Graph 16: Epoch: 083, Loss: -0.5477\n",
      "Graph 16: Epoch: 084, Loss: -0.6377\n",
      "Graph 16: Epoch: 085, Loss: -0.6287\n",
      "Graph 16: Epoch: 086, Loss: -0.4260\n",
      "Graph 16: Epoch: 087, Loss: -0.0275\n",
      "Graph 16: Epoch: 088, Loss: -0.3046\n",
      "Graph 16: Epoch: 089, Loss: -0.5921\n",
      "Graph 16: Epoch: 090, Loss: -0.5354\n",
      "Graph 16: Epoch: 091, Loss: -0.4592\n",
      "Graph 16: Epoch: 092, Loss: -0.4877\n",
      "Graph 16: Epoch: 093, Loss: -0.6360\n",
      "Graph 16: Epoch: 094, Loss: -0.5606\n",
      "Graph 16: Epoch: 095, Loss: -0.9506\n",
      "Graph 16: Epoch: 096, Loss: -0.2452\n",
      "Graph 16: Epoch: 097, Loss: -0.7686\n",
      "Graph 16: Epoch: 098, Loss: -0.3230\n",
      "Graph 16: Epoch: 099, Loss: -0.6211\n",
      "Graph 16: Epoch: 100, Loss: -0.6223\n",
      "Graph 17: Epoch: 001, Loss: -0.0147\n",
      "Graph 17: Epoch: 002, Loss: -0.0358\n",
      "Graph 17: Epoch: 003, Loss: -0.0771\n",
      "Graph 17: Epoch: 004, Loss: -0.1682\n",
      "Graph 17: Epoch: 005, Loss: -0.4037\n",
      "Graph 17: Epoch: 006, Loss: -0.4249\n",
      "Graph 17: Epoch: 007, Loss: -0.3234\n",
      "Graph 17: Epoch: 008, Loss: -0.2024\n",
      "Graph 17: Epoch: 009, Loss: -0.2756\n",
      "Graph 17: Epoch: 010, Loss: -0.4181\n",
      "Graph 17: Epoch: 011, Loss: -0.2984\n",
      "Graph 17: Epoch: 012, Loss: -0.1659\n",
      "Graph 17: Epoch: 013, Loss: -0.1554\n",
      "Graph 17: Epoch: 014, Loss: -0.4227\n",
      "Graph 17: Epoch: 015, Loss: -0.5398\n",
      "Graph 17: Epoch: 016, Loss: -0.3794\n",
      "Graph 17: Epoch: 017, Loss: -0.5975\n",
      "Graph 17: Epoch: 018, Loss: -0.4614\n",
      "Graph 17: Epoch: 019, Loss: -0.3251\n",
      "Graph 17: Epoch: 020, Loss: -0.3987\n",
      "Graph 17: Epoch: 021, Loss: -0.3336\n",
      "Graph 17: Epoch: 022, Loss: -0.7082\n",
      "Graph 17: Epoch: 023, Loss: -0.8104\n",
      "Graph 17: Epoch: 024, Loss: -0.5143\n",
      "Graph 17: Epoch: 025, Loss: -0.4071\n",
      "Graph 17: Epoch: 026, Loss: -0.3031\n",
      "Graph 17: Epoch: 027, Loss: -0.3138\n",
      "Graph 17: Epoch: 028, Loss: -0.3170\n",
      "Graph 17: Epoch: 029, Loss: -0.2168\n",
      "Graph 17: Epoch: 030, Loss: -0.4030\n",
      "Graph 17: Epoch: 031, Loss: -0.3088\n",
      "Graph 17: Epoch: 032, Loss: -0.6043\n",
      "Graph 17: Epoch: 033, Loss: -0.6229\n",
      "Graph 17: Epoch: 034, Loss: -0.5235\n",
      "Graph 17: Epoch: 035, Loss: -0.3414\n",
      "Graph 17: Epoch: 036, Loss: -0.5206\n",
      "Graph 17: Epoch: 037, Loss: -0.4482\n",
      "Graph 17: Epoch: 038, Loss: -0.3521\n",
      "Graph 17: Epoch: 039, Loss: -0.4263\n",
      "Graph 17: Epoch: 040, Loss: -0.4079\n",
      "Graph 17: Epoch: 041, Loss: -0.6374\n",
      "Graph 17: Epoch: 042, Loss: -0.4282\n",
      "Graph 17: Epoch: 043, Loss: -0.6425\n",
      "Graph 17: Epoch: 044, Loss: -0.4194\n",
      "Graph 17: Epoch: 045, Loss: -0.6301\n",
      "Graph 17: Epoch: 046, Loss: -0.4309\n",
      "Graph 17: Epoch: 047, Loss: -0.5305\n",
      "Graph 17: Epoch: 048, Loss: -0.4327\n",
      "Graph 17: Epoch: 049, Loss: -0.4316\n",
      "Graph 17: Epoch: 050, Loss: -0.2173\n",
      "Graph 17: Epoch: 051, Loss: -0.5372\n",
      "Graph 17: Epoch: 052, Loss: -0.3228\n",
      "Graph 17: Epoch: 053, Loss: -0.4337\n",
      "Graph 17: Epoch: 054, Loss: -0.6493\n",
      "Graph 17: Epoch: 055, Loss: -0.5418\n",
      "Graph 17: Epoch: 056, Loss: -0.4332\n",
      "Graph 17: Epoch: 057, Loss: -0.1099\n",
      "Graph 17: Epoch: 058, Loss: -0.4328\n",
      "Graph 17: Epoch: 059, Loss: -0.4354\n",
      "Graph 17: Epoch: 060, Loss: -0.4346\n",
      "Graph 17: Epoch: 061, Loss: -0.4364\n",
      "Graph 17: Epoch: 062, Loss: -0.4354\n",
      "Graph 17: Epoch: 063, Loss: -0.4357\n",
      "Graph 17: Epoch: 064, Loss: -0.2345\n",
      "Graph 17: Epoch: 065, Loss: -0.4363\n",
      "Graph 17: Epoch: 066, Loss: -0.2191\n",
      "Graph 17: Epoch: 067, Loss: -0.6518\n",
      "Graph 17: Epoch: 068, Loss: -0.6547\n",
      "Graph 17: Epoch: 069, Loss: -0.5455\n",
      "Graph 17: Epoch: 070, Loss: -0.6544\n",
      "Graph 17: Epoch: 071, Loss: -0.4333\n",
      "Graph 17: Epoch: 072, Loss: -0.5465\n",
      "Graph 17: Epoch: 073, Loss: -0.4354\n",
      "Graph 17: Epoch: 074, Loss: -0.2410\n",
      "Graph 17: Epoch: 075, Loss: -0.5462\n",
      "Graph 17: Epoch: 076, Loss: -0.5466\n",
      "Graph 17: Epoch: 077, Loss: -0.2212\n",
      "Graph 17: Epoch: 078, Loss: -0.4346\n",
      "Graph 17: Epoch: 079, Loss: -0.6554\n",
      "Graph 17: Epoch: 080, Loss: -0.4377\n",
      "Graph 17: Epoch: 081, Loss: -0.5476\n",
      "Graph 17: Epoch: 082, Loss: -0.4379\n",
      "Graph 17: Epoch: 083, Loss: -0.3290\n",
      "Graph 17: Epoch: 084, Loss: -0.3291\n",
      "Graph 17: Epoch: 085, Loss: -0.3304\n",
      "Graph 17: Epoch: 086, Loss: -0.3291\n",
      "Graph 17: Epoch: 087, Loss: -0.3291\n",
      "Graph 17: Epoch: 088, Loss: -0.6548\n",
      "Graph 17: Epoch: 089, Loss: -0.5479\n",
      "Graph 17: Epoch: 090, Loss: -0.2403\n",
      "Graph 17: Epoch: 091, Loss: -0.3302\n",
      "Graph 17: Epoch: 092, Loss: -0.4387\n",
      "Graph 17: Epoch: 093, Loss: -0.6569\n",
      "Graph 17: Epoch: 094, Loss: -0.7638\n",
      "Graph 17: Epoch: 095, Loss: -0.4392\n",
      "Graph 17: Epoch: 096, Loss: -0.6573\n",
      "Graph 17: Epoch: 097, Loss: -0.5490\n",
      "Graph 17: Epoch: 098, Loss: -0.4872\n",
      "Graph 17: Epoch: 099, Loss: -0.2218\n",
      "Graph 17: Epoch: 100, Loss: -0.3225\n",
      "Graph 18: Epoch: 001, Loss: -0.0102\n",
      "Graph 18: Epoch: 002, Loss: -0.0343\n",
      "Graph 18: Epoch: 003, Loss: -0.1624\n",
      "Graph 18: Epoch: 004, Loss: -0.2107\n",
      "Graph 18: Epoch: 005, Loss: -0.0868\n",
      "Graph 18: Epoch: 006, Loss: -0.1554\n",
      "Graph 18: Epoch: 007, Loss: -0.2354\n",
      "Graph 18: Epoch: 008, Loss: -0.3543\n",
      "Graph 18: Epoch: 009, Loss: -0.3336\n",
      "Graph 18: Epoch: 010, Loss: -0.3843\n",
      "Graph 18: Epoch: 011, Loss: -0.2614\n",
      "Graph 18: Epoch: 012, Loss: -0.2887\n",
      "Graph 18: Epoch: 013, Loss: -0.4981\n",
      "Graph 18: Epoch: 014, Loss: -0.5331\n",
      "Graph 18: Epoch: 015, Loss: -0.4703\n",
      "Graph 18: Epoch: 016, Loss: -0.4551\n",
      "Graph 18: Epoch: 017, Loss: -0.7112\n",
      "Graph 18: Epoch: 018, Loss: -0.3389\n",
      "Graph 18: Epoch: 019, Loss: -0.5636\n",
      "Graph 18: Epoch: 020, Loss: -0.5647\n",
      "Graph 18: Epoch: 021, Loss: -0.4931\n",
      "Graph 18: Epoch: 022, Loss: -0.4119\n",
      "Graph 18: Epoch: 023, Loss: -0.3454\n",
      "Graph 18: Epoch: 024, Loss: -0.3472\n",
      "Graph 18: Epoch: 025, Loss: -0.5158\n",
      "Graph 18: Epoch: 026, Loss: -0.2641\n",
      "Graph 18: Epoch: 027, Loss: -0.3464\n",
      "Graph 18: Epoch: 028, Loss: -0.6078\n",
      "Graph 18: Epoch: 029, Loss: -0.5209\n",
      "Graph 18: Epoch: 030, Loss: -0.4378\n",
      "Graph 18: Epoch: 031, Loss: -0.3499\n",
      "Graph 18: Epoch: 032, Loss: -0.6116\n",
      "Graph 18: Epoch: 033, Loss: -0.3517\n",
      "Graph 18: Epoch: 034, Loss: -0.3512\n",
      "Graph 18: Epoch: 035, Loss: -0.4398\n",
      "Graph 18: Epoch: 036, Loss: -0.5270\n",
      "Graph 18: Epoch: 037, Loss: -0.2621\n",
      "Graph 18: Epoch: 038, Loss: -0.3534\n",
      "Graph 18: Epoch: 039, Loss: -0.3535\n",
      "Graph 18: Epoch: 040, Loss: -0.5306\n",
      "Graph 18: Epoch: 041, Loss: -0.3547\n",
      "Graph 18: Epoch: 042, Loss: -0.7971\n",
      "Graph 18: Epoch: 043, Loss: -0.4433\n",
      "Graph 18: Epoch: 044, Loss: -0.5322\n",
      "Graph 18: Epoch: 045, Loss: -0.3558\n",
      "Graph 18: Epoch: 046, Loss: -0.3548\n",
      "Graph 18: Epoch: 047, Loss: -0.2674\n",
      "Graph 18: Epoch: 048, Loss: -0.6221\n",
      "Graph 18: Epoch: 049, Loss: -0.4426\n",
      "Graph 18: Epoch: 050, Loss: -0.2677\n",
      "Graph 18: Epoch: 051, Loss: -0.3852\n",
      "Graph 18: Epoch: 052, Loss: -0.5347\n",
      "Graph 18: Epoch: 053, Loss: -0.3572\n",
      "Graph 18: Epoch: 054, Loss: -0.7122\n",
      "Graph 18: Epoch: 055, Loss: -0.4444\n",
      "Graph 18: Epoch: 056, Loss: -0.4289\n",
      "Graph 18: Epoch: 057, Loss: -0.5342\n",
      "Graph 18: Epoch: 058, Loss: -0.4240\n",
      "Graph 18: Epoch: 059, Loss: -0.5346\n",
      "Graph 18: Epoch: 060, Loss: -0.6228\n",
      "Graph 18: Epoch: 061, Loss: -0.5356\n",
      "Graph 18: Epoch: 062, Loss: -0.4416\n",
      "Graph 18: Epoch: 063, Loss: -0.5354\n",
      "Graph 18: Epoch: 064, Loss: -0.4486\n",
      "Graph 18: Epoch: 065, Loss: -0.5366\n",
      "Graph 18: Epoch: 066, Loss: -0.6260\n",
      "Graph 18: Epoch: 067, Loss: -0.5386\n",
      "Graph 18: Epoch: 068, Loss: -0.4433\n",
      "Graph 18: Epoch: 069, Loss: -0.8029\n",
      "Graph 18: Epoch: 070, Loss: -0.4493\n",
      "Graph 18: Epoch: 071, Loss: -0.5386\n",
      "Graph 18: Epoch: 072, Loss: -0.7141\n",
      "Graph 18: Epoch: 073, Loss: -0.4481\n",
      "Graph 18: Epoch: 074, Loss: -0.3588\n",
      "Graph 18: Epoch: 075, Loss: -0.6273\n",
      "Graph 18: Epoch: 076, Loss: -0.6277\n",
      "Graph 18: Epoch: 077, Loss: -0.5392\n",
      "Graph 18: Epoch: 078, Loss: -0.3597\n",
      "Graph 18: Epoch: 079, Loss: -0.4498\n",
      "Graph 18: Epoch: 080, Loss: -0.4490\n",
      "Graph 18: Epoch: 081, Loss: -0.1813\n",
      "Graph 18: Epoch: 082, Loss: -0.6293\n",
      "Graph 18: Epoch: 083, Loss: -0.5395\n",
      "Graph 18: Epoch: 084, Loss: -0.6284\n",
      "Graph 18: Epoch: 085, Loss: -0.5406\n",
      "Graph 18: Epoch: 086, Loss: -0.4480\n",
      "Graph 18: Epoch: 087, Loss: -0.5685\n",
      "Graph 18: Epoch: 088, Loss: -0.2707\n",
      "Graph 18: Epoch: 089, Loss: -0.4497\n",
      "Graph 18: Epoch: 090, Loss: -0.4507\n",
      "Graph 18: Epoch: 091, Loss: -0.6280\n",
      "Graph 18: Epoch: 092, Loss: -0.7177\n",
      "Graph 18: Epoch: 093, Loss: -0.5396\n",
      "Graph 18: Epoch: 094, Loss: -0.7195\n",
      "Graph 18: Epoch: 095, Loss: -0.3606\n",
      "Graph 18: Epoch: 096, Loss: -0.5408\n",
      "Graph 18: Epoch: 097, Loss: -0.6305\n",
      "Graph 18: Epoch: 098, Loss: -0.3608\n",
      "Graph 18: Epoch: 099, Loss: -0.6309\n",
      "Graph 18: Epoch: 100, Loss: -0.3610\n",
      "Graph 19: Epoch: 001, Loss: -0.5000\n",
      "Graph 19: Epoch: 002, Loss: -0.4950\n",
      "Graph 19: Epoch: 003, Loss: -0.5047\n",
      "Graph 19: Epoch: 004, Loss: -0.4936\n",
      "Graph 19: Epoch: 005, Loss: -0.8834\n",
      "Graph 19: Epoch: 006, Loss: -0.5065\n",
      "Graph 19: Epoch: 007, Loss: -0.4921\n",
      "Graph 19: Epoch: 008, Loss: -0.1130\n",
      "Graph 19: Epoch: 009, Loss: -0.5077\n",
      "Graph 19: Epoch: 010, Loss: -0.1132\n",
      "Graph 19: Epoch: 011, Loss: -0.8862\n",
      "Graph 19: Epoch: 012, Loss: -0.5091\n",
      "Graph 19: Epoch: 013, Loss: -0.4896\n",
      "Graph 19: Epoch: 014, Loss: -0.8870\n",
      "Graph 19: Epoch: 015, Loss: -0.5113\n",
      "Graph 19: Epoch: 016, Loss: -0.4874\n",
      "Graph 19: Epoch: 017, Loss: -0.5129\n",
      "Graph 19: Epoch: 018, Loss: -0.5139\n",
      "Graph 19: Epoch: 019, Loss: -0.5155\n",
      "Graph 19: Epoch: 020, Loss: -0.5175\n",
      "Graph 19: Epoch: 021, Loss: -0.5199\n",
      "Graph 19: Epoch: 022, Loss: -0.4773\n",
      "Graph 19: Epoch: 023, Loss: -0.4755\n",
      "Graph 19: Epoch: 024, Loss: -0.1046\n",
      "Graph 19: Epoch: 025, Loss: -0.5262\n",
      "Graph 19: Epoch: 026, Loss: -0.8956\n",
      "Graph 19: Epoch: 027, Loss: -0.5288\n",
      "Graph 19: Epoch: 028, Loss: -0.4695\n",
      "Graph 19: Epoch: 029, Loss: -0.1024\n",
      "Graph 19: Epoch: 030, Loss: -0.5322\n",
      "Graph 19: Epoch: 031, Loss: -0.5334\n",
      "Graph 19: Epoch: 032, Loss: -0.1017\n",
      "Graph 19: Epoch: 033, Loss: -0.1017\n",
      "Graph 19: Epoch: 034, Loss: -0.8979\n",
      "Graph 19: Epoch: 035, Loss: -0.5383\n",
      "Graph 19: Epoch: 036, Loss: -0.5399\n",
      "Graph 19: Epoch: 037, Loss: -0.5419\n",
      "Graph 19: Epoch: 038, Loss: -0.4558\n",
      "Graph 19: Epoch: 039, Loss: -0.5457\n",
      "Graph 19: Epoch: 040, Loss: -0.5477\n",
      "Graph 19: Epoch: 041, Loss: -0.9007\n",
      "Graph 19: Epoch: 042, Loss: -0.4477\n",
      "Graph 19: Epoch: 043, Loss: -0.5537\n",
      "Graph 19: Epoch: 044, Loss: -0.4444\n",
      "Graph 19: Epoch: 045, Loss: -0.5568\n",
      "Graph 19: Epoch: 046, Loss: -0.9043\n",
      "Graph 19: Epoch: 047, Loss: -0.5600\n",
      "Graph 19: Epoch: 048, Loss: -0.4380\n",
      "Graph 19: Epoch: 049, Loss: -0.0930\n",
      "Graph 19: Epoch: 050, Loss: -0.4358\n",
      "Graph 19: Epoch: 051, Loss: -0.0926\n",
      "Graph 19: Epoch: 052, Loss: -0.4355\n",
      "Graph 19: Epoch: 053, Loss: -0.5640\n",
      "Graph 19: Epoch: 054, Loss: -0.4359\n",
      "Graph 19: Epoch: 055, Loss: -0.4363\n",
      "Graph 19: Epoch: 056, Loss: -0.4373\n",
      "Graph 19: Epoch: 057, Loss: -0.4387\n",
      "Graph 19: Epoch: 058, Loss: -0.5594\n",
      "Graph 19: Epoch: 059, Loss: -0.5583\n",
      "Graph 19: Epoch: 060, Loss: -0.5579\n",
      "Graph 19: Epoch: 061, Loss: -0.5581\n",
      "Graph 19: Epoch: 062, Loss: -0.9033\n",
      "Graph 19: Epoch: 063, Loss: -0.0962\n",
      "Graph 19: Epoch: 064, Loss: -0.5601\n",
      "Graph 19: Epoch: 065, Loss: -0.9042\n",
      "Graph 19: Epoch: 066, Loss: -0.5624\n",
      "Graph 19: Epoch: 067, Loss: -0.9056\n",
      "Graph 19: Epoch: 068, Loss: -0.5656\n",
      "Graph 19: Epoch: 069, Loss: -0.9077\n",
      "Graph 19: Epoch: 070, Loss: -0.0910\n",
      "Graph 19: Epoch: 071, Loss: -0.5713\n",
      "Graph 19: Epoch: 072, Loss: -0.4267\n",
      "Graph 19: Epoch: 073, Loss: -0.9115\n",
      "Graph 19: Epoch: 074, Loss: -0.4241\n",
      "Graph 19: Epoch: 075, Loss: -0.4234\n",
      "Graph 19: Epoch: 076, Loss: -0.4234\n",
      "Graph 19: Epoch: 077, Loss: -0.4240\n",
      "Graph 19: Epoch: 078, Loss: -0.5750\n",
      "Graph 19: Epoch: 079, Loss: -0.5746\n",
      "Graph 19: Epoch: 080, Loss: -0.5748\n",
      "Graph 19: Epoch: 081, Loss: -0.4244\n",
      "Graph 19: Epoch: 082, Loss: -0.4243\n",
      "Graph 19: Epoch: 083, Loss: -0.5752\n",
      "Graph 19: Epoch: 084, Loss: -0.4246\n",
      "Graph 19: Epoch: 085, Loss: -0.9151\n",
      "Graph 19: Epoch: 086, Loss: -0.4253\n",
      "Graph 19: Epoch: 087, Loss: -0.4260\n",
      "Graph 19: Epoch: 088, Loss: -0.4272\n",
      "Graph 19: Epoch: 089, Loss: -0.0848\n",
      "Graph 19: Epoch: 090, Loss: -0.0853\n",
      "Graph 19: Epoch: 091, Loss: -0.5678\n",
      "Graph 19: Epoch: 092, Loss: -0.4331\n",
      "Graph 19: Epoch: 093, Loss: -0.5655\n",
      "Graph 19: Epoch: 094, Loss: -0.5647\n",
      "Graph 19: Epoch: 095, Loss: -0.4353\n",
      "Graph 19: Epoch: 096, Loss: -0.4360\n",
      "Graph 19: Epoch: 097, Loss: -0.5629\n",
      "Graph 19: Epoch: 098, Loss: -0.5624\n",
      "Graph 19: Epoch: 099, Loss: -0.5626\n",
      "Graph 19: Epoch: 100, Loss: -0.5633\n",
      "Graph 20: Epoch: 001, Loss: -0.0760\n",
      "Graph 20: Epoch: 002, Loss: -0.0549\n",
      "Graph 20: Epoch: 003, Loss: -0.2589\n",
      "Graph 20: Epoch: 004, Loss: -0.1519\n",
      "Graph 20: Epoch: 005, Loss: -0.1991\n",
      "Graph 20: Epoch: 006, Loss: -0.1638\n",
      "Graph 20: Epoch: 007, Loss: -0.1921\n",
      "Graph 20: Epoch: 008, Loss: -0.5138\n",
      "Graph 20: Epoch: 009, Loss: -0.2716\n",
      "Graph 20: Epoch: 010, Loss: -0.3568\n",
      "Graph 20: Epoch: 011, Loss: -0.2829\n",
      "Graph 20: Epoch: 012, Loss: -0.3638\n",
      "Graph 20: Epoch: 013, Loss: -0.3068\n",
      "Graph 20: Epoch: 014, Loss: -0.4159\n",
      "Graph 20: Epoch: 015, Loss: -0.3214\n",
      "Graph 20: Epoch: 016, Loss: -0.3033\n",
      "Graph 20: Epoch: 017, Loss: -0.2802\n",
      "Graph 20: Epoch: 018, Loss: -0.2291\n",
      "Graph 20: Epoch: 019, Loss: -0.5167\n",
      "Graph 20: Epoch: 020, Loss: -0.4361\n",
      "Graph 20: Epoch: 021, Loss: -0.3831\n",
      "Graph 20: Epoch: 022, Loss: -0.3893\n",
      "Graph 20: Epoch: 023, Loss: -0.3720\n",
      "Graph 20: Epoch: 024, Loss: -0.4283\n",
      "Graph 20: Epoch: 025, Loss: -0.3120\n",
      "Graph 20: Epoch: 026, Loss: -0.3743\n",
      "Graph 20: Epoch: 027, Loss: -0.3552\n",
      "Graph 20: Epoch: 028, Loss: -0.3875\n",
      "Graph 20: Epoch: 029, Loss: -0.4581\n",
      "Graph 20: Epoch: 030, Loss: -0.3491\n",
      "Graph 20: Epoch: 031, Loss: -0.3677\n",
      "Graph 20: Epoch: 032, Loss: -0.4375\n",
      "Graph 20: Epoch: 033, Loss: -0.3678\n",
      "Graph 20: Epoch: 034, Loss: -0.5226\n",
      "Graph 20: Epoch: 035, Loss: -0.5578\n",
      "Graph 20: Epoch: 036, Loss: -0.5153\n",
      "Graph 20: Epoch: 037, Loss: -0.5651\n",
      "Graph 20: Epoch: 038, Loss: -0.6248\n",
      "Graph 20: Epoch: 039, Loss: -0.6427\n",
      "Graph 20: Epoch: 040, Loss: -0.6745\n",
      "Graph 20: Epoch: 041, Loss: -0.8575\n",
      "Graph 20: Epoch: 042, Loss: -0.6253\n",
      "Graph 20: Epoch: 043, Loss: -0.4713\n",
      "Graph 20: Epoch: 044, Loss: -0.6248\n",
      "Graph 20: Epoch: 045, Loss: -0.3615\n",
      "Graph 20: Epoch: 046, Loss: -0.3642\n",
      "Graph 20: Epoch: 047, Loss: -0.0093\n",
      "Graph 20: Epoch: 048, Loss: -0.4780\n",
      "Graph 20: Epoch: 049, Loss: -0.1680\n",
      "Graph 20: Epoch: 050, Loss: -0.6331\n",
      "Graph 20: Epoch: 051, Loss: -0.1849\n",
      "Graph 20: Epoch: 052, Loss: -0.4807\n",
      "Graph 20: Epoch: 053, Loss: -0.3237\n",
      "Graph 20: Epoch: 054, Loss: -0.4807\n",
      "Graph 20: Epoch: 055, Loss: -0.4848\n",
      "Graph 20: Epoch: 056, Loss: -0.3232\n",
      "Graph 20: Epoch: 057, Loss: -0.4849\n",
      "Graph 20: Epoch: 058, Loss: -0.2226\n",
      "Graph 20: Epoch: 059, Loss: -0.3816\n",
      "Graph 20: Epoch: 060, Loss: -0.6462\n",
      "Graph 20: Epoch: 061, Loss: -0.4830\n",
      "Graph 20: Epoch: 062, Loss: -0.3247\n",
      "Graph 20: Epoch: 063, Loss: -0.3222\n",
      "Graph 20: Epoch: 064, Loss: -0.5527\n",
      "Graph 20: Epoch: 065, Loss: -0.5668\n",
      "Graph 20: Epoch: 066, Loss: -0.4871\n",
      "Graph 20: Epoch: 067, Loss: -0.4865\n",
      "Graph 20: Epoch: 068, Loss: -0.7271\n",
      "Graph 20: Epoch: 069, Loss: -0.1775\n",
      "Graph 20: Epoch: 070, Loss: -0.6480\n",
      "Graph 20: Epoch: 071, Loss: -0.3267\n",
      "Graph 20: Epoch: 072, Loss: -0.6490\n",
      "Graph 20: Epoch: 073, Loss: -0.5189\n",
      "Graph 20: Epoch: 074, Loss: -0.6492\n",
      "Graph 20: Epoch: 075, Loss: -0.3262\n",
      "Graph 20: Epoch: 076, Loss: -0.4905\n",
      "Graph 20: Epoch: 077, Loss: -0.6462\n",
      "Graph 20: Epoch: 078, Loss: -0.3560\n",
      "Graph 20: Epoch: 079, Loss: -0.4860\n",
      "Graph 20: Epoch: 080, Loss: -0.8124\n",
      "Graph 20: Epoch: 081, Loss: -0.4044\n",
      "Graph 20: Epoch: 082, Loss: -0.6248\n",
      "Graph 20: Epoch: 083, Loss: -0.5224\n",
      "Graph 20: Epoch: 084, Loss: -0.5254\n",
      "Graph 20: Epoch: 085, Loss: -0.1690\n",
      "Graph 20: Epoch: 086, Loss: -0.4875\n",
      "Graph 20: Epoch: 087, Loss: -0.4798\n",
      "Graph 20: Epoch: 088, Loss: -0.8130\n",
      "Graph 20: Epoch: 089, Loss: -0.8155\n",
      "Graph 20: Epoch: 090, Loss: -0.3208\n",
      "Graph 20: Epoch: 091, Loss: -0.6446\n",
      "Graph 20: Epoch: 092, Loss: -0.3289\n",
      "Graph 20: Epoch: 093, Loss: -0.8332\n",
      "Graph 20: Epoch: 094, Loss: -0.4874\n",
      "Graph 20: Epoch: 095, Loss: -0.2031\n",
      "Graph 20: Epoch: 096, Loss: -0.8968\n",
      "Graph 20: Epoch: 097, Loss: -0.7652\n",
      "Graph 20: Epoch: 098, Loss: -0.1993\n",
      "Graph 20: Epoch: 099, Loss: -0.4925\n",
      "Graph 20: Epoch: 100, Loss: -0.8175\n",
      "Graph 21: Epoch: 001, Loss: -0.0822\n",
      "Graph 21: Epoch: 002, Loss: -0.2507\n",
      "Graph 21: Epoch: 003, Loss: -0.1231\n",
      "Graph 21: Epoch: 004, Loss: -0.1682\n",
      "Graph 21: Epoch: 005, Loss: -0.1908\n",
      "Graph 21: Epoch: 006, Loss: -0.3635\n",
      "Graph 21: Epoch: 007, Loss: -0.2562\n",
      "Graph 21: Epoch: 008, Loss: -0.1378\n",
      "Graph 21: Epoch: 009, Loss: -0.1513\n",
      "Graph 21: Epoch: 010, Loss: -0.4358\n",
      "Graph 21: Epoch: 011, Loss: -0.3984\n",
      "Graph 21: Epoch: 012, Loss: -0.3125\n",
      "Graph 21: Epoch: 013, Loss: -0.3327\n",
      "Graph 21: Epoch: 014, Loss: -0.3769\n",
      "Graph 21: Epoch: 015, Loss: -0.0827\n",
      "Graph 21: Epoch: 016, Loss: -0.2862\n",
      "Graph 21: Epoch: 017, Loss: -0.3634\n",
      "Graph 21: Epoch: 018, Loss: -0.5165\n",
      "Graph 21: Epoch: 019, Loss: -0.1978\n",
      "Graph 21: Epoch: 020, Loss: -0.4199\n",
      "Graph 21: Epoch: 021, Loss: -0.4020\n",
      "Graph 21: Epoch: 022, Loss: -0.4704\n",
      "Graph 21: Epoch: 023, Loss: -0.3957\n",
      "Graph 21: Epoch: 024, Loss: -0.4871\n",
      "Graph 21: Epoch: 025, Loss: -0.5770\n",
      "Graph 21: Epoch: 026, Loss: -0.6834\n",
      "Graph 21: Epoch: 027, Loss: -0.5249\n",
      "Graph 21: Epoch: 028, Loss: -0.4102\n",
      "Graph 21: Epoch: 029, Loss: -0.1836\n",
      "Graph 21: Epoch: 030, Loss: -0.3047\n",
      "Graph 21: Epoch: 031, Loss: -0.4210\n",
      "Graph 21: Epoch: 032, Loss: -0.5078\n",
      "Graph 21: Epoch: 033, Loss: -0.4422\n",
      "Graph 21: Epoch: 034, Loss: -0.6024\n",
      "Graph 21: Epoch: 035, Loss: -0.4944\n",
      "Graph 21: Epoch: 036, Loss: -0.2923\n",
      "Graph 21: Epoch: 037, Loss: -0.0475\n",
      "Graph 21: Epoch: 038, Loss: -0.6672\n",
      "Graph 21: Epoch: 039, Loss: -0.2919\n",
      "Graph 21: Epoch: 040, Loss: -0.6807\n",
      "Graph 21: Epoch: 041, Loss: -0.2735\n",
      "Graph 21: Epoch: 042, Loss: -0.4022\n",
      "Graph 21: Epoch: 043, Loss: -0.3235\n",
      "Graph 21: Epoch: 044, Loss: -0.1545\n",
      "Graph 21: Epoch: 045, Loss: -0.6841\n",
      "Graph 21: Epoch: 046, Loss: -0.4752\n",
      "Graph 21: Epoch: 047, Loss: -0.5665\n",
      "Graph 21: Epoch: 048, Loss: -0.6677\n",
      "Graph 21: Epoch: 049, Loss: -0.1376\n",
      "Graph 21: Epoch: 050, Loss: -0.6913\n",
      "Graph 21: Epoch: 051, Loss: -0.4767\n",
      "Graph 21: Epoch: 052, Loss: -0.7389\n",
      "Graph 21: Epoch: 053, Loss: -0.7408\n",
      "Graph 21: Epoch: 054, Loss: -0.5078\n",
      "Graph 21: Epoch: 055, Loss: -0.4472\n",
      "Graph 21: Epoch: 056, Loss: -0.4757\n",
      "Graph 21: Epoch: 057, Loss: -0.5866\n",
      "Graph 21: Epoch: 058, Loss: -0.5269\n",
      "Graph 21: Epoch: 059, Loss: -0.2519\n",
      "Graph 21: Epoch: 060, Loss: -0.2980\n",
      "Graph 21: Epoch: 061, Loss: -0.8694\n",
      "Graph 21: Epoch: 062, Loss: -0.5041\n",
      "Graph 21: Epoch: 063, Loss: -0.5207\n",
      "Graph 21: Epoch: 064, Loss: -0.7086\n",
      "Graph 21: Epoch: 065, Loss: -0.9414\n",
      "Graph 21: Epoch: 066, Loss: -0.2486\n",
      "Graph 21: Epoch: 067, Loss: -0.5320\n",
      "Graph 21: Epoch: 068, Loss: -0.8153\n",
      "Graph 21: Epoch: 069, Loss: -0.7234\n",
      "Graph 21: Epoch: 070, Loss: -0.3942\n",
      "Graph 21: Epoch: 071, Loss: -0.4771\n",
      "Graph 21: Epoch: 072, Loss: -0.0655\n",
      "Graph 21: Epoch: 073, Loss: -0.2487\n",
      "Graph 21: Epoch: 074, Loss: -0.0132\n",
      "Graph 21: Epoch: 075, Loss: -0.2491\n",
      "Graph 21: Epoch: 076, Loss: -0.7234\n",
      "Graph 21: Epoch: 077, Loss: -0.3056\n",
      "Graph 21: Epoch: 078, Loss: -0.7284\n",
      "Graph 21: Epoch: 079, Loss: -0.4892\n",
      "Graph 21: Epoch: 080, Loss: -0.9557\n",
      "Graph 21: Epoch: 081, Loss: -0.3517\n",
      "Graph 21: Epoch: 082, Loss: -0.4647\n",
      "Graph 21: Epoch: 083, Loss: -0.6361\n",
      "Graph 21: Epoch: 084, Loss: -0.7152\n",
      "Graph 21: Epoch: 085, Loss: -0.4835\n",
      "Graph 21: Epoch: 086, Loss: -0.2491\n",
      "Graph 21: Epoch: 087, Loss: -0.2973\n",
      "Graph 21: Epoch: 088, Loss: -0.2509\n",
      "Graph 21: Epoch: 089, Loss: -0.4086\n",
      "Graph 21: Epoch: 090, Loss: -0.1758\n",
      "Graph 21: Epoch: 091, Loss: -0.3050\n",
      "Graph 21: Epoch: 092, Loss: -0.7315\n",
      "Graph 21: Epoch: 093, Loss: -0.9674\n",
      "Graph 21: Epoch: 094, Loss: -0.5908\n",
      "Graph 21: Epoch: 095, Loss: -0.2435\n",
      "Graph 21: Epoch: 096, Loss: -0.3756\n",
      "Graph 21: Epoch: 097, Loss: -0.6045\n",
      "Graph 21: Epoch: 098, Loss: -0.2495\n",
      "Graph 21: Epoch: 099, Loss: -0.3719\n",
      "Graph 21: Epoch: 100, Loss: -0.8608\n",
      "Graph 22: Epoch: 001, Loss: -0.5000\n",
      "Graph 22: Epoch: 002, Loss: -0.5050\n",
      "Graph 22: Epoch: 003, Loss: -0.1235\n",
      "Graph 22: Epoch: 004, Loss: -0.5146\n",
      "Graph 22: Epoch: 005, Loss: -0.8698\n",
      "Graph 22: Epoch: 006, Loss: -0.8685\n",
      "Graph 22: Epoch: 007, Loss: -0.5247\n",
      "Graph 22: Epoch: 008, Loss: -0.4725\n",
      "Graph 22: Epoch: 009, Loss: -0.4714\n",
      "Graph 22: Epoch: 010, Loss: -0.5287\n",
      "Graph 22: Epoch: 011, Loss: -0.1313\n",
      "Graph 22: Epoch: 012, Loss: -0.5307\n",
      "Graph 22: Epoch: 013, Loss: -0.4675\n",
      "Graph 22: Epoch: 014, Loss: -0.1335\n",
      "Graph 22: Epoch: 015, Loss: -0.4657\n",
      "Graph 22: Epoch: 016, Loss: -0.4655\n",
      "Graph 22: Epoch: 017, Loss: -0.8645\n",
      "Graph 22: Epoch: 018, Loss: -0.8649\n",
      "Graph 22: Epoch: 019, Loss: -0.5321\n",
      "Graph 22: Epoch: 020, Loss: -0.4682\n",
      "Graph 22: Epoch: 021, Loss: -0.8673\n",
      "Graph 22: Epoch: 022, Loss: -0.4702\n",
      "Graph 22: Epoch: 023, Loss: -0.4718\n",
      "Graph 22: Epoch: 024, Loss: -0.5261\n",
      "Graph 22: Epoch: 025, Loss: -0.5249\n",
      "Graph 22: Epoch: 026, Loss: -0.5244\n",
      "Graph 22: Epoch: 027, Loss: -0.5246\n",
      "Graph 22: Epoch: 028, Loss: -0.5254\n",
      "Graph 22: Epoch: 029, Loss: -0.5267\n",
      "Graph 22: Epoch: 030, Loss: -0.8733\n",
      "Graph 22: Epoch: 031, Loss: -0.1266\n",
      "Graph 22: Epoch: 032, Loss: -0.4688\n",
      "Graph 22: Epoch: 033, Loss: -0.5320\n",
      "Graph 22: Epoch: 034, Loss: -0.4668\n",
      "Graph 22: Epoch: 035, Loss: -0.4663\n",
      "Graph 22: Epoch: 036, Loss: -0.4664\n",
      "Graph 22: Epoch: 037, Loss: -0.5329\n",
      "Graph 22: Epoch: 038, Loss: -0.5328\n",
      "Graph 22: Epoch: 039, Loss: -0.5334\n",
      "Graph 22: Epoch: 040, Loss: -0.8727\n",
      "Graph 22: Epoch: 041, Loss: -0.4649\n",
      "Graph 22: Epoch: 042, Loss: -0.5352\n",
      "Graph 22: Epoch: 043, Loss: -0.5358\n",
      "Graph 22: Epoch: 044, Loss: -0.5369\n",
      "Graph 22: Epoch: 045, Loss: -0.8731\n",
      "Graph 22: Epoch: 046, Loss: -0.8733\n",
      "Graph 22: Epoch: 047, Loss: -0.4596\n",
      "Graph 22: Epoch: 048, Loss: -0.4595\n",
      "Graph 22: Epoch: 049, Loss: -0.5401\n",
      "Graph 22: Epoch: 050, Loss: -0.4597\n",
      "Graph 22: Epoch: 051, Loss: -0.4601\n",
      "Graph 22: Epoch: 052, Loss: -0.1215\n",
      "Graph 22: Epoch: 053, Loss: -0.5384\n",
      "Graph 22: Epoch: 054, Loss: -0.5384\n",
      "Graph 22: Epoch: 055, Loss: -0.5390\n",
      "Graph 22: Epoch: 056, Loss: -0.5401\n",
      "Graph 22: Epoch: 057, Loss: -0.5416\n",
      "Graph 22: Epoch: 058, Loss: -0.5435\n",
      "Graph 22: Epoch: 059, Loss: -0.5458\n",
      "Graph 22: Epoch: 060, Loss: -0.4516\n",
      "Graph 22: Epoch: 061, Loss: -0.5502\n",
      "Graph 22: Epoch: 062, Loss: -0.8738\n",
      "Graph 22: Epoch: 063, Loss: -0.4460\n",
      "Graph 22: Epoch: 064, Loss: -0.4450\n",
      "Graph 22: Epoch: 065, Loss: -0.4447\n",
      "Graph 22: Epoch: 066, Loss: -0.5551\n",
      "Graph 22: Epoch: 067, Loss: -0.5554\n",
      "Graph 22: Epoch: 068, Loss: -0.5562\n",
      "Graph 22: Epoch: 069, Loss: -0.4425\n",
      "Graph 22: Epoch: 070, Loss: -0.4419\n",
      "Graph 22: Epoch: 071, Loss: -0.4419\n",
      "Graph 22: Epoch: 072, Loss: -0.5576\n",
      "Graph 22: Epoch: 073, Loss: -0.4423\n",
      "Graph 22: Epoch: 074, Loss: -0.4428\n",
      "Graph 22: Epoch: 075, Loss: -0.4438\n",
      "Graph 22: Epoch: 076, Loss: -0.5548\n",
      "Graph 22: Epoch: 077, Loss: -0.4459\n",
      "Graph 22: Epoch: 078, Loss: -0.5529\n",
      "Graph 22: Epoch: 079, Loss: -0.5524\n",
      "Graph 22: Epoch: 080, Loss: -0.5524\n",
      "Graph 22: Epoch: 081, Loss: -0.4470\n",
      "Graph 22: Epoch: 082, Loss: -0.8780\n",
      "Graph 22: Epoch: 083, Loss: -0.4472\n",
      "Graph 22: Epoch: 084, Loss: -0.4480\n",
      "Graph 22: Epoch: 085, Loss: -0.5508\n",
      "Graph 22: Epoch: 086, Loss: -0.5503\n",
      "Graph 22: Epoch: 087, Loss: -0.5503\n",
      "Graph 22: Epoch: 088, Loss: -0.5509\n",
      "Graph 22: Epoch: 089, Loss: -0.1187\n",
      "Graph 22: Epoch: 090, Loss: -0.4469\n",
      "Graph 22: Epoch: 091, Loss: -0.4464\n",
      "Graph 22: Epoch: 092, Loss: -0.1199\n",
      "Graph 22: Epoch: 093, Loss: -0.4463\n",
      "Graph 22: Epoch: 094, Loss: -0.4467\n",
      "Graph 22: Epoch: 095, Loss: -0.5524\n",
      "Graph 22: Epoch: 096, Loss: -0.5522\n",
      "Graph 22: Epoch: 097, Loss: -0.4475\n",
      "Graph 22: Epoch: 098, Loss: -0.5522\n",
      "Graph 22: Epoch: 099, Loss: -0.5525\n",
      "Graph 22: Epoch: 100, Loss: -0.5533\n",
      "Graph 23: Epoch: 001, Loss: -0.0225\n",
      "Graph 23: Epoch: 002, Loss: -0.1188\n",
      "Graph 23: Epoch: 003, Loss: -0.2476\n",
      "Graph 23: Epoch: 004, Loss: -0.2879\n",
      "Graph 23: Epoch: 005, Loss: -0.3323\n",
      "Graph 23: Epoch: 006, Loss: -0.3154\n",
      "Graph 23: Epoch: 007, Loss: -0.3500\n",
      "Graph 23: Epoch: 008, Loss: -0.4173\n",
      "Graph 23: Epoch: 009, Loss: -0.3515\n",
      "Graph 23: Epoch: 010, Loss: -0.3205\n",
      "Graph 23: Epoch: 011, Loss: -0.5497\n",
      "Graph 23: Epoch: 012, Loss: -0.4401\n",
      "Graph 23: Epoch: 013, Loss: -0.4048\n",
      "Graph 23: Epoch: 014, Loss: -0.4093\n",
      "Graph 23: Epoch: 015, Loss: -0.3818\n",
      "Graph 23: Epoch: 016, Loss: -0.5127\n",
      "Graph 23: Epoch: 017, Loss: -0.4160\n",
      "Graph 23: Epoch: 018, Loss: -0.4689\n",
      "Graph 23: Epoch: 019, Loss: -0.4235\n",
      "Graph 23: Epoch: 020, Loss: -0.3769\n",
      "Graph 23: Epoch: 021, Loss: -0.3822\n",
      "Graph 23: Epoch: 022, Loss: -0.4784\n",
      "Graph 23: Epoch: 023, Loss: -0.4820\n",
      "Graph 23: Epoch: 024, Loss: -0.4258\n",
      "Graph 23: Epoch: 025, Loss: -0.5892\n",
      "Graph 23: Epoch: 026, Loss: -0.3783\n",
      "Graph 23: Epoch: 027, Loss: -0.4875\n",
      "Graph 23: Epoch: 028, Loss: -0.4875\n",
      "Graph 23: Epoch: 029, Loss: -0.2789\n",
      "Graph 23: Epoch: 030, Loss: -0.5736\n",
      "Graph 23: Epoch: 031, Loss: -0.7575\n",
      "Graph 23: Epoch: 032, Loss: -0.3833\n",
      "Graph 23: Epoch: 033, Loss: -0.4909\n",
      "Graph 23: Epoch: 034, Loss: -0.3284\n",
      "Graph 23: Epoch: 035, Loss: -0.5988\n",
      "Graph 23: Epoch: 036, Loss: -0.5459\n",
      "Graph 23: Epoch: 037, Loss: -0.4378\n",
      "Graph 23: Epoch: 038, Loss: -0.4933\n",
      "Graph 23: Epoch: 039, Loss: -0.3853\n",
      "Graph 23: Epoch: 040, Loss: -0.4399\n",
      "Graph 23: Epoch: 041, Loss: -0.5495\n",
      "Graph 23: Epoch: 042, Loss: -0.6041\n",
      "Graph 23: Epoch: 043, Loss: -0.3853\n",
      "Graph 23: Epoch: 044, Loss: -0.5494\n",
      "Graph 23: Epoch: 045, Loss: -0.3848\n",
      "Graph 23: Epoch: 046, Loss: -0.5505\n",
      "Graph 23: Epoch: 047, Loss: -0.5510\n",
      "Graph 23: Epoch: 048, Loss: -0.4409\n",
      "Graph 23: Epoch: 049, Loss: -0.7157\n",
      "Graph 23: Epoch: 050, Loss: -0.4960\n",
      "Graph 23: Epoch: 051, Loss: -0.3857\n",
      "Graph 23: Epoch: 052, Loss: -0.2761\n",
      "Graph 23: Epoch: 053, Loss: -0.5516\n",
      "Graph 23: Epoch: 054, Loss: -0.8271\n",
      "Graph 23: Epoch: 055, Loss: -0.5514\n",
      "Graph 23: Epoch: 056, Loss: -0.3314\n",
      "Graph 23: Epoch: 057, Loss: -0.3863\n",
      "Graph 23: Epoch: 058, Loss: -0.3864\n",
      "Graph 23: Epoch: 059, Loss: -0.3867\n",
      "Graph 23: Epoch: 060, Loss: -0.5522\n",
      "Graph 23: Epoch: 061, Loss: -0.4415\n",
      "Graph 23: Epoch: 062, Loss: -0.4419\n",
      "Graph 23: Epoch: 063, Loss: -0.2766\n",
      "Graph 23: Epoch: 064, Loss: -0.3873\n",
      "Graph 23: Epoch: 065, Loss: -0.5524\n",
      "Graph 23: Epoch: 066, Loss: -0.4974\n",
      "Graph 23: Epoch: 067, Loss: -0.2765\n",
      "Graph 23: Epoch: 068, Loss: -0.4974\n",
      "Graph 23: Epoch: 069, Loss: -0.4422\n",
      "Graph 23: Epoch: 070, Loss: -0.5527\n",
      "Graph 23: Epoch: 071, Loss: -0.3871\n",
      "Graph 23: Epoch: 072, Loss: -0.4976\n",
      "Graph 23: Epoch: 073, Loss: -0.4422\n",
      "Graph 23: Epoch: 074, Loss: -0.3319\n",
      "Graph 23: Epoch: 075, Loss: -0.6082\n",
      "Graph 23: Epoch: 076, Loss: -0.5529\n",
      "Graph 23: Epoch: 077, Loss: -0.4424\n",
      "Graph 23: Epoch: 078, Loss: -0.6082\n",
      "Graph 23: Epoch: 079, Loss: -0.6080\n",
      "Graph 23: Epoch: 080, Loss: -0.6082\n",
      "Graph 23: Epoch: 081, Loss: -0.3320\n",
      "Graph 23: Epoch: 082, Loss: -0.4979\n",
      "Graph 23: Epoch: 083, Loss: -0.5531\n",
      "Graph 23: Epoch: 084, Loss: -0.3321\n",
      "Graph 23: Epoch: 085, Loss: -0.2769\n",
      "Graph 23: Epoch: 086, Loss: -0.5532\n",
      "Graph 23: Epoch: 087, Loss: -0.5533\n",
      "Graph 23: Epoch: 088, Loss: -0.3873\n",
      "Graph 23: Epoch: 089, Loss: -0.3874\n",
      "Graph 23: Epoch: 090, Loss: -0.3874\n",
      "Graph 23: Epoch: 091, Loss: -0.4428\n",
      "Graph 23: Epoch: 092, Loss: -0.4427\n",
      "Graph 23: Epoch: 093, Loss: -0.6640\n",
      "Graph 23: Epoch: 094, Loss: -0.3875\n",
      "Graph 23: Epoch: 095, Loss: -0.5534\n",
      "Graph 23: Epoch: 096, Loss: -0.4980\n",
      "Graph 23: Epoch: 097, Loss: -0.3322\n",
      "Graph 23: Epoch: 098, Loss: -0.3876\n",
      "Graph 23: Epoch: 099, Loss: -0.6641\n",
      "Graph 23: Epoch: 100, Loss: -0.6088\n",
      "Graph 24: Epoch: 001, Loss: -0.0852\n",
      "Graph 24: Epoch: 002, Loss: -0.3371\n",
      "Graph 24: Epoch: 003, Loss: -0.2899\n",
      "Graph 24: Epoch: 004, Loss: -0.2466\n",
      "Graph 24: Epoch: 005, Loss: -0.3961\n",
      "Graph 24: Epoch: 006, Loss: -0.3982\n",
      "Graph 24: Epoch: 007, Loss: -0.4166\n",
      "Graph 24: Epoch: 008, Loss: -0.5807\n",
      "Graph 24: Epoch: 009, Loss: -0.4385\n",
      "Graph 24: Epoch: 010, Loss: -0.4907\n",
      "Graph 24: Epoch: 011, Loss: -0.4990\n",
      "Graph 24: Epoch: 012, Loss: -0.5006\n",
      "Graph 24: Epoch: 013, Loss: -0.3014\n",
      "Graph 24: Epoch: 014, Loss: -0.4011\n",
      "Graph 24: Epoch: 015, Loss: -0.6060\n",
      "Graph 24: Epoch: 016, Loss: -0.4459\n",
      "Graph 24: Epoch: 017, Loss: -0.5061\n",
      "Graph 24: Epoch: 018, Loss: -0.3570\n",
      "Graph 24: Epoch: 019, Loss: -0.6068\n",
      "Graph 24: Epoch: 020, Loss: -0.5611\n",
      "Graph 24: Epoch: 021, Loss: -0.6634\n",
      "Graph 24: Epoch: 022, Loss: -0.5637\n",
      "Graph 24: Epoch: 023, Loss: -0.4644\n",
      "Graph 24: Epoch: 024, Loss: -0.5669\n",
      "Graph 24: Epoch: 025, Loss: -0.5145\n",
      "Graph 24: Epoch: 026, Loss: -0.3622\n",
      "Graph 24: Epoch: 027, Loss: -0.5691\n",
      "Graph 24: Epoch: 028, Loss: -0.4675\n",
      "Graph 24: Epoch: 029, Loss: -0.4656\n",
      "Graph 24: Epoch: 030, Loss: -0.3107\n",
      "Graph 24: Epoch: 031, Loss: -0.3631\n",
      "Graph 24: Epoch: 032, Loss: -0.4154\n",
      "Graph 24: Epoch: 033, Loss: -0.4663\n",
      "Graph 24: Epoch: 034, Loss: -0.5178\n",
      "Graph 24: Epoch: 035, Loss: -0.4154\n",
      "Graph 24: Epoch: 036, Loss: -0.5191\n",
      "Graph 24: Epoch: 037, Loss: -0.3635\n",
      "Graph 24: Epoch: 038, Loss: -0.5720\n",
      "Graph 24: Epoch: 039, Loss: -0.5194\n",
      "Graph 24: Epoch: 040, Loss: -0.5200\n",
      "Graph 24: Epoch: 041, Loss: -0.2603\n",
      "Graph 24: Epoch: 042, Loss: -0.4169\n",
      "Graph 24: Epoch: 043, Loss: -0.5732\n",
      "Graph 24: Epoch: 044, Loss: -0.7299\n",
      "Graph 24: Epoch: 045, Loss: -0.4695\n",
      "Graph 24: Epoch: 046, Loss: -0.4693\n",
      "Graph 24: Epoch: 047, Loss: -0.4696\n",
      "Graph 24: Epoch: 048, Loss: -0.4176\n",
      "Graph 24: Epoch: 049, Loss: -0.5219\n",
      "Graph 24: Epoch: 050, Loss: -0.4176\n",
      "Graph 24: Epoch: 051, Loss: -0.7831\n",
      "Graph 24: Epoch: 052, Loss: -0.5221\n",
      "Graph 24: Epoch: 053, Loss: -0.2613\n",
      "Graph 24: Epoch: 054, Loss: -0.3658\n",
      "Graph 24: Epoch: 055, Loss: -0.4701\n",
      "Graph 24: Epoch: 056, Loss: -0.5224\n",
      "Graph 24: Epoch: 057, Loss: -0.6270\n",
      "Graph 24: Epoch: 058, Loss: -0.5747\n",
      "Graph 24: Epoch: 059, Loss: -0.6793\n",
      "Graph 24: Epoch: 060, Loss: -0.5226\n",
      "Graph 24: Epoch: 061, Loss: -0.3658\n",
      "Graph 24: Epoch: 062, Loss: -0.4184\n",
      "Graph 24: Epoch: 063, Loss: -0.4707\n",
      "Graph 24: Epoch: 064, Loss: -0.4707\n",
      "Graph 24: Epoch: 065, Loss: -0.7319\n",
      "Graph 24: Epoch: 066, Loss: -0.3139\n",
      "Graph 24: Epoch: 067, Loss: -0.6276\n",
      "Graph 24: Epoch: 068, Loss: -0.6801\n",
      "Graph 24: Epoch: 069, Loss: -0.4709\n",
      "Graph 24: Epoch: 070, Loss: -0.4186\n",
      "Graph 24: Epoch: 071, Loss: -0.4188\n",
      "Graph 24: Epoch: 072, Loss: -0.4710\n",
      "Graph 24: Epoch: 073, Loss: -0.5235\n",
      "Graph 24: Epoch: 074, Loss: -0.4712\n",
      "Graph 24: Epoch: 075, Loss: -0.5759\n",
      "Graph 24: Epoch: 076, Loss: -0.4187\n",
      "Graph 24: Epoch: 077, Loss: -0.5235\n",
      "Graph 24: Epoch: 078, Loss: -0.5758\n",
      "Graph 24: Epoch: 079, Loss: -0.5761\n",
      "Graph 24: Epoch: 080, Loss: -0.4189\n",
      "Graph 24: Epoch: 081, Loss: -0.3143\n",
      "Graph 24: Epoch: 082, Loss: -0.4190\n",
      "Graph 24: Epoch: 083, Loss: -0.5762\n",
      "Graph 24: Epoch: 084, Loss: -0.4715\n",
      "Graph 24: Epoch: 085, Loss: -0.5762\n",
      "Graph 24: Epoch: 086, Loss: -0.3143\n",
      "Graph 24: Epoch: 087, Loss: -0.4715\n",
      "Graph 24: Epoch: 088, Loss: -0.5240\n",
      "Graph 24: Epoch: 089, Loss: -0.4715\n",
      "Graph 24: Epoch: 090, Loss: -0.5239\n",
      "Graph 24: Epoch: 091, Loss: -0.3150\n",
      "Graph 24: Epoch: 092, Loss: -0.6811\n",
      "Graph 24: Epoch: 093, Loss: -0.4716\n",
      "Graph 24: Epoch: 094, Loss: -0.5765\n",
      "Graph 24: Epoch: 095, Loss: -0.6813\n",
      "Graph 24: Epoch: 096, Loss: -0.5241\n",
      "Graph 24: Epoch: 097, Loss: -0.5765\n",
      "Graph 24: Epoch: 098, Loss: -0.5242\n",
      "Graph 24: Epoch: 099, Loss: -0.3670\n",
      "Graph 24: Epoch: 100, Loss: -0.4194\n",
      "Graph 25: Epoch: 001, Loss: -0.0147\n",
      "Graph 25: Epoch: 002, Loss: -0.0302\n",
      "Graph 25: Epoch: 003, Loss: -0.0903\n",
      "Graph 25: Epoch: 004, Loss: -0.1429\n",
      "Graph 25: Epoch: 005, Loss: -0.1814\n",
      "Graph 25: Epoch: 006, Loss: -0.2660\n",
      "Graph 25: Epoch: 007, Loss: -0.2735\n",
      "Graph 25: Epoch: 008, Loss: -0.3876\n",
      "Graph 25: Epoch: 009, Loss: -0.3262\n",
      "Graph 25: Epoch: 010, Loss: -0.3115\n",
      "Graph 25: Epoch: 011, Loss: -0.4225\n",
      "Graph 25: Epoch: 012, Loss: -0.3598\n",
      "Graph 25: Epoch: 013, Loss: -0.2780\n",
      "Graph 25: Epoch: 014, Loss: -0.3772\n",
      "Graph 25: Epoch: 015, Loss: -0.4236\n",
      "Graph 25: Epoch: 016, Loss: -0.3202\n",
      "Graph 25: Epoch: 017, Loss: -0.5196\n",
      "Graph 25: Epoch: 018, Loss: -0.2378\n",
      "Graph 25: Epoch: 019, Loss: -0.5280\n",
      "Graph 25: Epoch: 020, Loss: -0.4581\n",
      "Graph 25: Epoch: 021, Loss: -0.4549\n",
      "Graph 25: Epoch: 022, Loss: -0.3941\n",
      "Graph 25: Epoch: 023, Loss: -0.4619\n",
      "Graph 25: Epoch: 024, Loss: -0.4745\n",
      "Graph 25: Epoch: 025, Loss: -0.4719\n",
      "Graph 25: Epoch: 026, Loss: -0.4709\n",
      "Graph 25: Epoch: 027, Loss: -0.6260\n",
      "Graph 25: Epoch: 028, Loss: -0.4794\n",
      "Graph 25: Epoch: 029, Loss: -0.4056\n",
      "Graph 25: Epoch: 030, Loss: -0.5526\n",
      "Graph 25: Epoch: 031, Loss: -0.2459\n",
      "Graph 25: Epoch: 032, Loss: -0.4049\n",
      "Graph 25: Epoch: 033, Loss: -0.4010\n",
      "Graph 25: Epoch: 034, Loss: -0.3236\n",
      "Graph 25: Epoch: 035, Loss: -0.5672\n",
      "Graph 25: Epoch: 036, Loss: -0.4062\n",
      "Graph 25: Epoch: 037, Loss: -0.4870\n",
      "Graph 25: Epoch: 038, Loss: -0.4866\n",
      "Graph 25: Epoch: 039, Loss: -0.6483\n",
      "Graph 25: Epoch: 040, Loss: -0.5664\n",
      "Graph 25: Epoch: 041, Loss: -0.6505\n",
      "Graph 25: Epoch: 042, Loss: -0.4892\n",
      "Graph 25: Epoch: 043, Loss: -0.4889\n",
      "Graph 25: Epoch: 044, Loss: -0.3270\n",
      "Graph 25: Epoch: 045, Loss: -0.4899\n",
      "Graph 25: Epoch: 046, Loss: -0.5711\n",
      "Graph 25: Epoch: 047, Loss: -0.6544\n",
      "Graph 25: Epoch: 048, Loss: -0.5731\n",
      "Graph 25: Epoch: 049, Loss: -0.4845\n",
      "Graph 25: Epoch: 050, Loss: -0.7381\n",
      "Graph 25: Epoch: 051, Loss: -0.5743\n",
      "Graph 25: Epoch: 052, Loss: -0.4104\n",
      "Graph 25: Epoch: 053, Loss: -0.4903\n",
      "Graph 25: Epoch: 054, Loss: -0.4105\n",
      "Graph 25: Epoch: 055, Loss: -0.2606\n",
      "Graph 25: Epoch: 056, Loss: -0.2535\n",
      "Graph 25: Epoch: 057, Loss: -0.5744\n",
      "Graph 25: Epoch: 058, Loss: -0.4932\n",
      "Graph 25: Epoch: 059, Loss: -0.4924\n",
      "Graph 25: Epoch: 060, Loss: -0.3291\n",
      "Graph 25: Epoch: 061, Loss: -0.5742\n",
      "Graph 25: Epoch: 062, Loss: -0.5738\n",
      "Graph 25: Epoch: 063, Loss: -0.4119\n",
      "Graph 25: Epoch: 064, Loss: -0.3287\n",
      "Graph 25: Epoch: 065, Loss: -0.2481\n",
      "Graph 25: Epoch: 066, Loss: -0.2470\n",
      "Graph 25: Epoch: 067, Loss: -0.3296\n",
      "Graph 25: Epoch: 068, Loss: -0.3304\n",
      "Graph 25: Epoch: 069, Loss: -0.5768\n",
      "Graph 25: Epoch: 070, Loss: -0.5770\n",
      "Graph 25: Epoch: 071, Loss: -0.6593\n",
      "Graph 25: Epoch: 072, Loss: -0.4127\n",
      "Graph 25: Epoch: 073, Loss: -0.5772\n",
      "Graph 25: Epoch: 074, Loss: -0.4128\n",
      "Graph 25: Epoch: 075, Loss: -0.7420\n",
      "Graph 25: Epoch: 076, Loss: -0.4126\n",
      "Graph 25: Epoch: 077, Loss: -0.4133\n",
      "Graph 25: Epoch: 078, Loss: -0.4952\n",
      "Graph 25: Epoch: 079, Loss: -0.4947\n",
      "Graph 25: Epoch: 080, Loss: -0.5772\n",
      "Graph 25: Epoch: 081, Loss: -0.5777\n",
      "Graph 25: Epoch: 082, Loss: -0.6600\n",
      "Graph 25: Epoch: 083, Loss: -0.4950\n",
      "Graph 25: Epoch: 084, Loss: -0.3310\n",
      "Graph 25: Epoch: 085, Loss: -0.4132\n",
      "Graph 25: Epoch: 086, Loss: -0.5784\n",
      "Graph 25: Epoch: 087, Loss: -0.4137\n",
      "Graph 25: Epoch: 088, Loss: -0.4954\n",
      "Graph 25: Epoch: 089, Loss: -0.4961\n",
      "Graph 25: Epoch: 090, Loss: -0.4137\n",
      "Graph 25: Epoch: 091, Loss: -0.6613\n",
      "Graph 25: Epoch: 092, Loss: -0.4134\n",
      "Graph 25: Epoch: 093, Loss: -0.7440\n",
      "Graph 25: Epoch: 094, Loss: -0.5789\n",
      "Graph 25: Epoch: 095, Loss: -0.7443\n",
      "Graph 25: Epoch: 096, Loss: -0.4961\n",
      "Graph 25: Epoch: 097, Loss: -0.3313\n",
      "Graph 25: Epoch: 098, Loss: -0.4969\n",
      "Graph 25: Epoch: 099, Loss: -0.4140\n",
      "Graph 25: Epoch: 100, Loss: -0.2653\n",
      "Graph 26: Epoch: 001, Loss: -0.1232\n",
      "Graph 26: Epoch: 002, Loss: -0.1883\n",
      "Graph 26: Epoch: 003, Loss: -0.2461\n",
      "Graph 26: Epoch: 004, Loss: -0.0419\n",
      "Graph 26: Epoch: 005, Loss: -0.3203\n",
      "Graph 26: Epoch: 006, Loss: -0.4416\n",
      "Graph 26: Epoch: 007, Loss: -0.6049\n",
      "Graph 26: Epoch: 008, Loss: -0.6459\n",
      "Graph 26: Epoch: 009, Loss: -0.2724\n",
      "Graph 26: Epoch: 010, Loss: -0.2880\n",
      "Graph 26: Epoch: 011, Loss: -0.2667\n",
      "Graph 26: Epoch: 012, Loss: -0.4536\n",
      "Graph 26: Epoch: 013, Loss: -0.3341\n",
      "Graph 26: Epoch: 014, Loss: -0.2161\n",
      "Graph 26: Epoch: 015, Loss: -0.3255\n",
      "Graph 26: Epoch: 016, Loss: -0.7525\n",
      "Graph 26: Epoch: 017, Loss: -0.4295\n",
      "Graph 26: Epoch: 018, Loss: -0.2778\n",
      "Graph 26: Epoch: 019, Loss: -0.3972\n",
      "Graph 26: Epoch: 020, Loss: -0.5777\n",
      "Graph 26: Epoch: 021, Loss: -0.7486\n",
      "Graph 26: Epoch: 022, Loss: -0.4119\n",
      "Graph 26: Epoch: 023, Loss: -0.2903\n",
      "Graph 26: Epoch: 024, Loss: -0.5538\n",
      "Graph 26: Epoch: 025, Loss: -0.1775\n",
      "Graph 26: Epoch: 026, Loss: -0.2911\n",
      "Graph 26: Epoch: 027, Loss: -0.3420\n",
      "Graph 26: Epoch: 028, Loss: -0.2571\n",
      "Graph 26: Epoch: 029, Loss: -0.4605\n",
      "Graph 26: Epoch: 030, Loss: -0.6139\n",
      "Graph 26: Epoch: 031, Loss: -0.4474\n",
      "Graph 26: Epoch: 032, Loss: -0.0546\n",
      "Graph 26: Epoch: 033, Loss: -0.5230\n",
      "Graph 26: Epoch: 034, Loss: -0.8690\n",
      "Graph 26: Epoch: 035, Loss: -0.4509\n",
      "Graph 26: Epoch: 036, Loss: -0.6920\n",
      "Graph 26: Epoch: 037, Loss: -0.5892\n",
      "Graph 26: Epoch: 038, Loss: -0.4832\n",
      "Graph 26: Epoch: 039, Loss: -0.7346\n",
      "Graph 26: Epoch: 040, Loss: -0.7367\n",
      "Graph 26: Epoch: 041, Loss: -0.3482\n",
      "Graph 26: Epoch: 042, Loss: -0.6107\n",
      "Graph 26: Epoch: 043, Loss: -0.4746\n",
      "Graph 26: Epoch: 044, Loss: -0.5470\n",
      "Graph 26: Epoch: 045, Loss: -0.4818\n",
      "Graph 26: Epoch: 046, Loss: -0.1766\n",
      "Graph 26: Epoch: 047, Loss: -0.6136\n",
      "Graph 26: Epoch: 048, Loss: -0.4902\n",
      "Graph 26: Epoch: 049, Loss: -0.7552\n",
      "Graph 26: Epoch: 050, Loss: -0.1933\n",
      "Graph 26: Epoch: 051, Loss: -0.2044\n",
      "Graph 26: Epoch: 052, Loss: -0.3544\n",
      "Graph 26: Epoch: 053, Loss: -0.8998\n",
      "Graph 26: Epoch: 054, Loss: -0.0361\n",
      "Graph 26: Epoch: 055, Loss: -0.3642\n",
      "Graph 26: Epoch: 056, Loss: -0.1458\n",
      "Graph 26: Epoch: 057, Loss: -0.5878\n",
      "Graph 26: Epoch: 058, Loss: -0.3482\n",
      "Graph 26: Epoch: 059, Loss: -0.2100\n",
      "Graph 26: Epoch: 060, Loss: -0.9113\n",
      "Graph 26: Epoch: 061, Loss: -0.2850\n",
      "Graph 26: Epoch: 062, Loss: -0.3752\n",
      "Graph 26: Epoch: 063, Loss: -0.9156\n",
      "Graph 26: Epoch: 064, Loss: -0.6546\n",
      "Graph 26: Epoch: 065, Loss: -0.7704\n",
      "Graph 26: Epoch: 066, Loss: -0.1663\n",
      "Graph 26: Epoch: 067, Loss: -0.6622\n",
      "Graph 26: Epoch: 068, Loss: -0.7880\n",
      "Graph 26: Epoch: 069, Loss: -0.8955\n",
      "Graph 26: Epoch: 070, Loss: -0.4616\n",
      "Graph 26: Epoch: 071, Loss: -0.1605\n",
      "Graph 26: Epoch: 072, Loss: -0.1661\n",
      "Graph 26: Epoch: 073, Loss: -0.5959\n",
      "Graph 26: Epoch: 074, Loss: -0.9193\n",
      "Graph 26: Epoch: 075, Loss: -0.4676\n",
      "Graph 26: Epoch: 076, Loss: -0.1344\n",
      "Graph 26: Epoch: 077, Loss: -0.6545\n",
      "Graph 26: Epoch: 078, Loss: -0.6764\n",
      "Graph 26: Epoch: 079, Loss: -0.9211\n",
      "Graph 26: Epoch: 080, Loss: -0.4712\n",
      "Graph 26: Epoch: 081, Loss: -0.8009\n",
      "Graph 26: Epoch: 082, Loss: -0.0520\n",
      "Graph 26: Epoch: 083, Loss: -0.7860\n",
      "Graph 26: Epoch: 084, Loss: -0.5611\n",
      "Graph 26: Epoch: 085, Loss: -0.1621\n",
      "Graph 26: Epoch: 086, Loss: -0.3482\n",
      "Graph 26: Epoch: 087, Loss: -0.3491\n",
      "Graph 26: Epoch: 088, Loss: -0.9362\n",
      "Graph 26: Epoch: 089, Loss: -0.6920\n",
      "Graph 26: Epoch: 090, Loss: -0.5258\n",
      "Graph 26: Epoch: 091, Loss: -0.2409\n",
      "Graph 26: Epoch: 092, Loss: -0.6964\n",
      "Graph 26: Epoch: 093, Loss: -0.5985\n",
      "Graph 26: Epoch: 094, Loss: -0.4811\n",
      "Graph 26: Epoch: 095, Loss: -0.2591\n",
      "Graph 26: Epoch: 096, Loss: -0.4893\n",
      "Graph 26: Epoch: 097, Loss: -0.5959\n",
      "Graph 26: Epoch: 098, Loss: -0.4857\n",
      "Graph 26: Epoch: 099, Loss: -0.1410\n",
      "Graph 26: Epoch: 100, Loss: -0.5988\n",
      "Graph 27: Epoch: 001, Loss: -0.0745\n",
      "Graph 27: Epoch: 002, Loss: -0.1248\n",
      "Graph 27: Epoch: 003, Loss: -0.4271\n",
      "Graph 27: Epoch: 004, Loss: -0.2647\n",
      "Graph 27: Epoch: 005, Loss: -0.2582\n",
      "Graph 27: Epoch: 006, Loss: -0.4574\n",
      "Graph 27: Epoch: 007, Loss: -0.3996\n",
      "Graph 27: Epoch: 008, Loss: -0.4047\n",
      "Graph 27: Epoch: 009, Loss: -0.3878\n",
      "Graph 27: Epoch: 010, Loss: -0.3144\n",
      "Graph 27: Epoch: 011, Loss: -0.5230\n",
      "Graph 27: Epoch: 012, Loss: -0.5507\n",
      "Graph 27: Epoch: 013, Loss: -0.7931\n",
      "Graph 27: Epoch: 014, Loss: -0.3490\n",
      "Graph 27: Epoch: 015, Loss: -0.3448\n",
      "Graph 27: Epoch: 016, Loss: -0.5025\n",
      "Graph 27: Epoch: 017, Loss: -0.2914\n",
      "Graph 27: Epoch: 018, Loss: -0.4266\n",
      "Graph 27: Epoch: 019, Loss: -0.5738\n",
      "Graph 27: Epoch: 020, Loss: -0.4340\n",
      "Graph 27: Epoch: 021, Loss: -0.4390\n",
      "Graph 27: Epoch: 022, Loss: -0.2223\n",
      "Graph 27: Epoch: 023, Loss: -0.4391\n",
      "Graph 27: Epoch: 024, Loss: -0.4403\n",
      "Graph 27: Epoch: 025, Loss: -0.3615\n",
      "Graph 27: Epoch: 026, Loss: -0.5174\n",
      "Graph 27: Epoch: 027, Loss: -0.5137\n",
      "Graph 27: Epoch: 028, Loss: -0.5926\n",
      "Graph 27: Epoch: 029, Loss: -0.3721\n",
      "Graph 27: Epoch: 030, Loss: -0.3731\n",
      "Graph 27: Epoch: 031, Loss: -0.5225\n",
      "Graph 27: Epoch: 032, Loss: -0.5986\n",
      "Graph 27: Epoch: 033, Loss: -0.2259\n",
      "Graph 27: Epoch: 034, Loss: -0.2994\n",
      "Graph 27: Epoch: 035, Loss: -0.4505\n",
      "Graph 27: Epoch: 036, Loss: -0.5997\n",
      "Graph 27: Epoch: 037, Loss: -0.5262\n",
      "Graph 27: Epoch: 038, Loss: -0.3759\n",
      "Graph 27: Epoch: 039, Loss: -0.2259\n",
      "Graph 27: Epoch: 040, Loss: -0.4521\n",
      "Graph 27: Epoch: 041, Loss: -0.4517\n",
      "Graph 27: Epoch: 042, Loss: -0.4465\n",
      "Graph 27: Epoch: 043, Loss: -0.4513\n",
      "Graph 27: Epoch: 044, Loss: -0.6031\n",
      "Graph 27: Epoch: 045, Loss: -0.6788\n",
      "Graph 27: Epoch: 046, Loss: -0.4523\n",
      "Graph 27: Epoch: 047, Loss: -0.6044\n",
      "Graph 27: Epoch: 048, Loss: -0.7558\n",
      "Graph 27: Epoch: 049, Loss: -0.5293\n",
      "Graph 27: Epoch: 050, Loss: -0.5295\n",
      "Graph 27: Epoch: 051, Loss: -0.5298\n",
      "Graph 27: Epoch: 052, Loss: -0.8326\n",
      "Graph 27: Epoch: 053, Loss: -0.3786\n",
      "Graph 27: Epoch: 054, Loss: -0.3791\n",
      "Graph 27: Epoch: 055, Loss: -0.4538\n",
      "Graph 27: Epoch: 056, Loss: -0.4540\n",
      "Graph 27: Epoch: 057, Loss: -0.3795\n",
      "Graph 27: Epoch: 058, Loss: -0.3796\n",
      "Graph 27: Epoch: 059, Loss: -0.5312\n",
      "Graph 27: Epoch: 060, Loss: -0.6072\n",
      "Graph 27: Epoch: 061, Loss: -0.6053\n",
      "Graph 27: Epoch: 062, Loss: -0.4559\n",
      "Graph 27: Epoch: 063, Loss: -0.6074\n",
      "Graph 27: Epoch: 064, Loss: -0.6072\n",
      "Graph 27: Epoch: 065, Loss: -0.4561\n",
      "Graph 27: Epoch: 066, Loss: -0.4556\n",
      "Graph 27: Epoch: 067, Loss: -0.3804\n",
      "Graph 27: Epoch: 068, Loss: -0.3801\n",
      "Graph 27: Epoch: 069, Loss: -0.6834\n",
      "Graph 27: Epoch: 070, Loss: -0.3808\n",
      "Graph 27: Epoch: 071, Loss: -0.3804\n",
      "Graph 27: Epoch: 072, Loss: -0.3807\n",
      "Graph 27: Epoch: 073, Loss: -0.6842\n",
      "Graph 27: Epoch: 074, Loss: -0.6090\n",
      "Graph 27: Epoch: 075, Loss: -0.3045\n",
      "Graph 27: Epoch: 076, Loss: -0.6850\n",
      "Graph 27: Epoch: 077, Loss: -0.3051\n",
      "Graph 27: Epoch: 078, Loss: -0.6092\n",
      "Graph 27: Epoch: 079, Loss: -0.4572\n",
      "Graph 27: Epoch: 080, Loss: -0.5332\n",
      "Graph 27: Epoch: 081, Loss: -0.5333\n",
      "Graph 27: Epoch: 082, Loss: -0.7620\n",
      "Graph 27: Epoch: 083, Loss: -0.4574\n",
      "Graph 27: Epoch: 084, Loss: -0.5338\n",
      "Graph 27: Epoch: 085, Loss: -0.7619\n",
      "Graph 27: Epoch: 086, Loss: -0.3815\n",
      "Graph 27: Epoch: 087, Loss: -0.4577\n",
      "Graph 27: Epoch: 088, Loss: -0.3814\n",
      "Graph 27: Epoch: 089, Loss: -0.6100\n",
      "Graph 27: Epoch: 090, Loss: -0.3817\n",
      "Graph 27: Epoch: 091, Loss: -0.5342\n",
      "Graph 27: Epoch: 092, Loss: -0.3055\n",
      "Graph 27: Epoch: 093, Loss: -0.2293\n",
      "Graph 27: Epoch: 094, Loss: -0.4577\n",
      "Graph 27: Epoch: 095, Loss: -0.3055\n",
      "Graph 27: Epoch: 096, Loss: -0.5343\n",
      "Graph 27: Epoch: 097, Loss: -0.7626\n",
      "Graph 27: Epoch: 098, Loss: -0.5344\n",
      "Graph 27: Epoch: 099, Loss: -0.2295\n",
      "Graph 27: Epoch: 100, Loss: -0.4583\n",
      "Graph 28: Epoch: 001, Loss: -0.2500\n",
      "Graph 28: Epoch: 002, Loss: -0.4272\n",
      "Graph 28: Epoch: 003, Loss: -0.3200\n",
      "Graph 28: Epoch: 004, Loss: -0.3480\n",
      "Graph 28: Epoch: 005, Loss: -0.2565\n",
      "Graph 28: Epoch: 006, Loss: -0.2585\n",
      "Graph 28: Epoch: 007, Loss: -0.2006\n",
      "Graph 28: Epoch: 008, Loss: -0.2092\n",
      "Graph 28: Epoch: 009, Loss: -0.4175\n",
      "Graph 28: Epoch: 010, Loss: -0.2369\n",
      "Graph 28: Epoch: 011, Loss: -0.2487\n",
      "Graph 28: Epoch: 012, Loss: -0.2698\n",
      "Graph 28: Epoch: 013, Loss: -0.1814\n",
      "Graph 28: Epoch: 014, Loss: -0.4362\n",
      "Graph 28: Epoch: 015, Loss: -0.4292\n",
      "Graph 28: Epoch: 016, Loss: -0.2767\n",
      "Graph 28: Epoch: 017, Loss: -0.7794\n",
      "Graph 28: Epoch: 018, Loss: -0.6102\n",
      "Graph 28: Epoch: 019, Loss: -0.1797\n",
      "Graph 28: Epoch: 020, Loss: -0.3349\n",
      "Graph 28: Epoch: 021, Loss: -0.2848\n",
      "Graph 28: Epoch: 022, Loss: -0.3337\n",
      "Graph 28: Epoch: 023, Loss: -0.6861\n",
      "Graph 28: Epoch: 024, Loss: -0.3236\n",
      "Graph 28: Epoch: 025, Loss: -0.5950\n",
      "Graph 28: Epoch: 026, Loss: -0.1806\n",
      "Graph 28: Epoch: 027, Loss: -0.6265\n",
      "Graph 28: Epoch: 028, Loss: -0.3821\n",
      "Graph 28: Epoch: 029, Loss: -0.5622\n",
      "Graph 28: Epoch: 030, Loss: -0.2742\n",
      "Graph 28: Epoch: 031, Loss: -0.2967\n",
      "Graph 28: Epoch: 032, Loss: -0.1352\n",
      "Graph 28: Epoch: 033, Loss: -0.3171\n",
      "Graph 28: Epoch: 034, Loss: -0.4210\n",
      "Graph 28: Epoch: 035, Loss: -0.3103\n",
      "Graph 28: Epoch: 036, Loss: -0.3705\n",
      "Graph 28: Epoch: 037, Loss: -0.2495\n",
      "Graph 28: Epoch: 038, Loss: -0.0694\n",
      "Graph 28: Epoch: 039, Loss: -0.3116\n",
      "Graph 28: Epoch: 040, Loss: -0.3903\n",
      "Graph 28: Epoch: 041, Loss: -0.3891\n",
      "Graph 28: Epoch: 042, Loss: -0.0812\n",
      "Graph 28: Epoch: 043, Loss: -0.6383\n",
      "Graph 28: Epoch: 044, Loss: -0.4463\n",
      "Graph 28: Epoch: 045, Loss: -0.8276\n",
      "Graph 28: Epoch: 046, Loss: -0.4008\n",
      "Graph 28: Epoch: 047, Loss: -0.2264\n",
      "Graph 28: Epoch: 048, Loss: -0.4487\n",
      "Graph 28: Epoch: 049, Loss: -0.3184\n",
      "Graph 28: Epoch: 050, Loss: -0.4017\n",
      "Graph 28: Epoch: 051, Loss: -0.6509\n",
      "Graph 28: Epoch: 052, Loss: -0.3961\n",
      "Graph 28: Epoch: 053, Loss: -0.4608\n",
      "Graph 28: Epoch: 054, Loss: -0.2300\n",
      "Graph 28: Epoch: 055, Loss: -0.3707\n",
      "Graph 28: Epoch: 056, Loss: -0.2266\n",
      "Graph 28: Epoch: 057, Loss: -0.4495\n",
      "Graph 28: Epoch: 058, Loss: -0.6415\n",
      "Graph 28: Epoch: 059, Loss: -0.4102\n",
      "Graph 28: Epoch: 060, Loss: -0.5100\n",
      "Graph 28: Epoch: 061, Loss: -0.5522\n",
      "Graph 28: Epoch: 062, Loss: -0.4078\n",
      "Graph 28: Epoch: 063, Loss: -0.2893\n",
      "Graph 28: Epoch: 064, Loss: -0.6095\n",
      "Graph 28: Epoch: 065, Loss: -0.8693\n",
      "Graph 28: Epoch: 066, Loss: -0.5768\n",
      "Graph 28: Epoch: 067, Loss: -0.2659\n",
      "Graph 28: Epoch: 068, Loss: -0.4541\n",
      "Graph 28: Epoch: 069, Loss: -0.5625\n",
      "Graph 28: Epoch: 070, Loss: -0.3901\n",
      "Graph 28: Epoch: 071, Loss: -0.2529\n",
      "Graph 28: Epoch: 072, Loss: -0.4406\n",
      "Graph 28: Epoch: 073, Loss: -0.4684\n",
      "Graph 28: Epoch: 074, Loss: -0.3053\n",
      "Graph 28: Epoch: 075, Loss: -0.3924\n",
      "Graph 28: Epoch: 076, Loss: -0.6841\n",
      "Graph 28: Epoch: 077, Loss: -0.2148\n",
      "Graph 28: Epoch: 078, Loss: -0.6272\n",
      "Graph 28: Epoch: 079, Loss: -0.4644\n",
      "Graph 28: Epoch: 080, Loss: -0.8450\n",
      "Graph 28: Epoch: 081, Loss: -0.6914\n",
      "Graph 28: Epoch: 082, Loss: -0.5527\n",
      "Graph 28: Epoch: 083, Loss: -0.0472\n",
      "Graph 28: Epoch: 084, Loss: -0.4988\n",
      "Graph 28: Epoch: 085, Loss: -0.4544\n",
      "Graph 28: Epoch: 086, Loss: -0.5784\n",
      "Graph 28: Epoch: 087, Loss: -0.4556\n",
      "Graph 28: Epoch: 088, Loss: -0.4703\n",
      "Graph 28: Epoch: 089, Loss: -0.4757\n",
      "Graph 28: Epoch: 090, Loss: -0.2438\n",
      "Graph 28: Epoch: 091, Loss: -0.1600\n",
      "Graph 28: Epoch: 092, Loss: -0.2262\n",
      "Graph 28: Epoch: 093, Loss: -0.6179\n",
      "Graph 28: Epoch: 094, Loss: -0.6412\n",
      "Graph 28: Epoch: 095, Loss: -0.8056\n",
      "Graph 28: Epoch: 096, Loss: -0.4671\n",
      "Graph 28: Epoch: 097, Loss: -0.2685\n",
      "Graph 28: Epoch: 098, Loss: -0.1462\n",
      "Graph 28: Epoch: 099, Loss: -0.6387\n",
      "Graph 28: Epoch: 100, Loss: -0.4737\n",
      "Graph 29: Epoch: 001, Loss: -0.2371\n",
      "Graph 29: Epoch: 002, Loss: -0.1374\n",
      "Graph 29: Epoch: 003, Loss: -0.0942\n",
      "Graph 29: Epoch: 004, Loss: -0.0865\n",
      "Graph 29: Epoch: 005, Loss: -0.3450\n",
      "Graph 29: Epoch: 006, Loss: -0.1856\n",
      "Graph 29: Epoch: 007, Loss: -0.2102\n",
      "Graph 29: Epoch: 008, Loss: -0.0727\n",
      "Graph 29: Epoch: 009, Loss: -0.3028\n",
      "Graph 29: Epoch: 010, Loss: -0.1993\n",
      "Graph 29: Epoch: 011, Loss: -0.4322\n",
      "Graph 29: Epoch: 012, Loss: -0.2757\n",
      "Graph 29: Epoch: 013, Loss: -0.3047\n",
      "Graph 29: Epoch: 014, Loss: -0.1435\n",
      "Graph 29: Epoch: 015, Loss: -0.1688\n",
      "Graph 29: Epoch: 016, Loss: -0.3250\n",
      "Graph 29: Epoch: 017, Loss: -0.3972\n",
      "Graph 29: Epoch: 018, Loss: -0.3618\n",
      "Graph 29: Epoch: 019, Loss: -0.5951\n",
      "Graph 29: Epoch: 020, Loss: -0.5345\n",
      "Graph 29: Epoch: 021, Loss: -0.0360\n",
      "Graph 29: Epoch: 022, Loss: -0.4446\n",
      "Graph 29: Epoch: 023, Loss: -0.5622\n",
      "Graph 29: Epoch: 024, Loss: -0.3520\n",
      "Graph 29: Epoch: 025, Loss: -0.4690\n",
      "Graph 29: Epoch: 026, Loss: -0.3031\n",
      "Graph 29: Epoch: 027, Loss: -0.3769\n",
      "Graph 29: Epoch: 028, Loss: -0.2527\n",
      "Graph 29: Epoch: 029, Loss: -0.8062\n",
      "Graph 29: Epoch: 030, Loss: -0.3003\n",
      "Graph 29: Epoch: 031, Loss: -0.5998\n",
      "Graph 29: Epoch: 032, Loss: -0.1726\n",
      "Graph 29: Epoch: 033, Loss: -0.5628\n",
      "Graph 29: Epoch: 034, Loss: -0.3138\n",
      "Graph 29: Epoch: 035, Loss: -0.4885\n",
      "Graph 29: Epoch: 036, Loss: -0.2146\n",
      "Graph 29: Epoch: 037, Loss: -0.3747\n",
      "Graph 29: Epoch: 038, Loss: -0.4636\n",
      "Graph 29: Epoch: 039, Loss: -0.3968\n",
      "Graph 29: Epoch: 040, Loss: -0.6736\n",
      "Graph 29: Epoch: 041, Loss: -0.1660\n",
      "Graph 29: Epoch: 042, Loss: -0.7864\n",
      "Graph 29: Epoch: 043, Loss: -0.6665\n",
      "Graph 29: Epoch: 044, Loss: -0.3186\n",
      "Graph 29: Epoch: 045, Loss: -0.2940\n",
      "Graph 29: Epoch: 046, Loss: -0.4289\n",
      "Graph 29: Epoch: 047, Loss: -0.4766\n",
      "Graph 29: Epoch: 048, Loss: -0.1562\n",
      "Graph 29: Epoch: 049, Loss: -0.1732\n",
      "Graph 29: Epoch: 050, Loss: -0.3992\n",
      "Graph 29: Epoch: 051, Loss: -0.1224\n",
      "Graph 29: Epoch: 052, Loss: -0.1064\n",
      "Graph 29: Epoch: 053, Loss: -0.2320\n",
      "Graph 29: Epoch: 054, Loss: -0.3668\n",
      "Graph 29: Epoch: 055, Loss: -0.5866\n",
      "Graph 29: Epoch: 056, Loss: -0.2484\n",
      "Graph 29: Epoch: 057, Loss: -0.1409\n",
      "Graph 29: Epoch: 058, Loss: -0.6035\n",
      "Graph 29: Epoch: 059, Loss: -0.7906\n",
      "Graph 29: Epoch: 060, Loss: -0.2524\n",
      "Graph 29: Epoch: 061, Loss: -0.3894\n",
      "Graph 29: Epoch: 062, Loss: -0.7061\n",
      "Graph 29: Epoch: 063, Loss: -0.5760\n",
      "Graph 29: Epoch: 064, Loss: -0.7084\n",
      "Graph 29: Epoch: 065, Loss: -0.4200\n",
      "Graph 29: Epoch: 066, Loss: -0.3118\n",
      "Graph 29: Epoch: 067, Loss: -0.6297\n",
      "Graph 29: Epoch: 068, Loss: -0.6351\n",
      "Graph 29: Epoch: 069, Loss: -0.2064\n",
      "Graph 29: Epoch: 070, Loss: -0.3341\n",
      "Graph 29: Epoch: 071, Loss: -0.4621\n",
      "Graph 29: Epoch: 072, Loss: -0.4796\n",
      "Graph 29: Epoch: 073, Loss: -0.5976\n",
      "Graph 29: Epoch: 074, Loss: -0.5676\n",
      "Graph 29: Epoch: 075, Loss: -0.2947\n",
      "Graph 29: Epoch: 076, Loss: -0.6710\n",
      "Graph 29: Epoch: 077, Loss: -0.1456\n",
      "Graph 29: Epoch: 078, Loss: -0.6564\n",
      "Graph 29: Epoch: 079, Loss: -0.3447\n",
      "Graph 29: Epoch: 080, Loss: -0.6075\n",
      "Graph 29: Epoch: 081, Loss: -0.4396\n",
      "Graph 29: Epoch: 082, Loss: -0.5938\n",
      "Graph 29: Epoch: 083, Loss: -0.1262\n",
      "Graph 29: Epoch: 084, Loss: -0.3215\n",
      "Graph 29: Epoch: 085, Loss: -0.6243\n",
      "Graph 29: Epoch: 086, Loss: -0.4569\n",
      "Graph 29: Epoch: 087, Loss: -0.5184\n",
      "Graph 29: Epoch: 088, Loss: -0.4560\n",
      "Graph 29: Epoch: 089, Loss: -0.6486\n",
      "Graph 29: Epoch: 090, Loss: -0.1568\n",
      "Graph 29: Epoch: 091, Loss: -0.4069\n",
      "Graph 29: Epoch: 092, Loss: -0.1311\n",
      "Graph 29: Epoch: 093, Loss: -0.6119\n",
      "Graph 29: Epoch: 094, Loss: -0.2481\n",
      "Graph 29: Epoch: 095, Loss: -0.1420\n",
      "Graph 29: Epoch: 096, Loss: -0.6213\n",
      "Graph 29: Epoch: 097, Loss: -0.4207\n",
      "Graph 29: Epoch: 098, Loss: -0.6274\n",
      "Graph 29: Epoch: 099, Loss: -0.6649\n",
      "Graph 29: Epoch: 100, Loss: -0.2509\n",
      "Graph 30: Epoch: 001, Loss: -0.0104\n",
      "Graph 30: Epoch: 002, Loss: -0.0720\n",
      "Graph 30: Epoch: 003, Loss: -0.2464\n",
      "Graph 30: Epoch: 004, Loss: -0.3469\n",
      "Graph 30: Epoch: 005, Loss: -0.2796\n",
      "Graph 30: Epoch: 006, Loss: -0.4399\n",
      "Graph 30: Epoch: 007, Loss: -0.5636\n",
      "Graph 30: Epoch: 008, Loss: -0.3465\n",
      "Graph 30: Epoch: 009, Loss: -0.2584\n",
      "Graph 30: Epoch: 010, Loss: -0.4622\n",
      "Graph 30: Epoch: 011, Loss: -0.4099\n",
      "Graph 30: Epoch: 012, Loss: -0.6283\n",
      "Graph 30: Epoch: 013, Loss: -0.3653\n",
      "Graph 30: Epoch: 014, Loss: -0.4727\n",
      "Graph 30: Epoch: 015, Loss: -0.4241\n",
      "Graph 30: Epoch: 016, Loss: -0.6896\n",
      "Graph 30: Epoch: 017, Loss: -0.5281\n",
      "Graph 30: Epoch: 018, Loss: -0.4292\n",
      "Graph 30: Epoch: 019, Loss: -0.4298\n",
      "Graph 30: Epoch: 020, Loss: -0.5376\n",
      "Graph 30: Epoch: 021, Loss: -0.5943\n",
      "Graph 30: Epoch: 022, Loss: -0.6474\n",
      "Graph 30: Epoch: 023, Loss: -0.3788\n",
      "Graph 30: Epoch: 024, Loss: -0.4338\n",
      "Graph 30: Epoch: 025, Loss: -0.2721\n",
      "Graph 30: Epoch: 026, Loss: -0.5974\n",
      "Graph 30: Epoch: 027, Loss: -0.5963\n",
      "Graph 30: Epoch: 028, Loss: -0.4354\n",
      "Graph 30: Epoch: 029, Loss: -0.3809\n",
      "Graph 30: Epoch: 030, Loss: -0.5440\n",
      "Graph 30: Epoch: 031, Loss: -0.4909\n",
      "Graph 30: Epoch: 032, Loss: -0.4361\n",
      "Graph 30: Epoch: 033, Loss: -0.3824\n",
      "Graph 30: Epoch: 034, Loss: -0.6553\n",
      "Graph 30: Epoch: 035, Loss: -0.3821\n",
      "Graph 30: Epoch: 036, Loss: -0.3835\n",
      "Graph 30: Epoch: 037, Loss: -0.4920\n",
      "Graph 30: Epoch: 038, Loss: -0.7121\n",
      "Graph 30: Epoch: 039, Loss: -0.2741\n",
      "Graph 30: Epoch: 040, Loss: -0.7106\n",
      "Graph 30: Epoch: 041, Loss: -0.4942\n",
      "Graph 30: Epoch: 042, Loss: -0.5491\n",
      "Graph 30: Epoch: 043, Loss: -0.5493\n",
      "Graph 30: Epoch: 044, Loss: -0.4398\n",
      "Graph 30: Epoch: 045, Loss: -0.4398\n",
      "Graph 30: Epoch: 046, Loss: -0.4400\n",
      "Graph 30: Epoch: 047, Loss: -0.4948\n",
      "Graph 30: Epoch: 048, Loss: -0.6051\n",
      "Graph 30: Epoch: 049, Loss: -0.4402\n",
      "Graph 30: Epoch: 050, Loss: -0.6604\n",
      "Graph 30: Epoch: 051, Loss: -0.4404\n",
      "Graph 30: Epoch: 052, Loss: -0.4955\n",
      "Graph 30: Epoch: 053, Loss: -0.8252\n",
      "Graph 30: Epoch: 054, Loss: -0.4955\n",
      "Graph 30: Epoch: 055, Loss: -0.4407\n",
      "Graph 30: Epoch: 056, Loss: -0.3306\n",
      "Graph 30: Epoch: 057, Loss: -0.4960\n",
      "Graph 30: Epoch: 058, Loss: -0.5512\n",
      "Graph 30: Epoch: 059, Loss: -0.4412\n",
      "Graph 30: Epoch: 060, Loss: -0.4958\n",
      "Graph 30: Epoch: 061, Loss: -0.4964\n",
      "Graph 30: Epoch: 062, Loss: -0.3862\n",
      "Graph 30: Epoch: 063, Loss: -0.6068\n",
      "Graph 30: Epoch: 064, Loss: -0.5514\n",
      "Graph 30: Epoch: 065, Loss: -0.5517\n",
      "Graph 30: Epoch: 066, Loss: -0.5517\n",
      "Graph 30: Epoch: 067, Loss: -0.3864\n",
      "Graph 30: Epoch: 068, Loss: -0.5518\n",
      "Graph 30: Epoch: 069, Loss: -0.5520\n",
      "Graph 30: Epoch: 070, Loss: -0.3865\n",
      "Graph 30: Epoch: 071, Loss: -0.4418\n",
      "Graph 30: Epoch: 072, Loss: -0.6625\n",
      "Graph 30: Epoch: 073, Loss: -0.5521\n",
      "Graph 30: Epoch: 074, Loss: -0.7175\n",
      "Graph 30: Epoch: 075, Loss: -0.3866\n",
      "Graph 30: Epoch: 076, Loss: -0.4419\n",
      "Graph 30: Epoch: 077, Loss: -0.7180\n",
      "Graph 30: Epoch: 078, Loss: -0.5523\n",
      "Graph 30: Epoch: 079, Loss: -0.5524\n",
      "Graph 30: Epoch: 080, Loss: -0.4973\n",
      "Graph 30: Epoch: 081, Loss: -0.4420\n",
      "Graph 30: Epoch: 082, Loss: -0.5526\n",
      "Graph 30: Epoch: 083, Loss: -0.3317\n",
      "Graph 30: Epoch: 084, Loss: -0.6079\n",
      "Graph 30: Epoch: 085, Loss: -0.8289\n",
      "Graph 30: Epoch: 086, Loss: -0.7185\n",
      "Graph 30: Epoch: 087, Loss: -0.6633\n",
      "Graph 30: Epoch: 088, Loss: -0.3871\n",
      "Graph 30: Epoch: 089, Loss: -0.6081\n",
      "Graph 30: Epoch: 090, Loss: -0.3871\n",
      "Graph 30: Epoch: 091, Loss: -0.4977\n",
      "Graph 30: Epoch: 092, Loss: -0.4424\n",
      "Graph 30: Epoch: 093, Loss: -0.6083\n",
      "Graph 30: Epoch: 094, Loss: -0.6083\n",
      "Graph 30: Epoch: 095, Loss: -0.3871\n",
      "Graph 30: Epoch: 096, Loss: -0.3319\n",
      "Graph 30: Epoch: 097, Loss: -0.4425\n",
      "Graph 30: Epoch: 098, Loss: -0.4978\n",
      "Graph 30: Epoch: 099, Loss: -0.4979\n",
      "Graph 30: Epoch: 100, Loss: -0.5532\n",
      "Graph 31: Epoch: 001, Loss: -0.0496\n",
      "Graph 31: Epoch: 002, Loss: -0.0294\n",
      "Graph 31: Epoch: 003, Loss: -0.0388\n",
      "Graph 31: Epoch: 004, Loss: -0.0853\n",
      "Graph 31: Epoch: 005, Loss: -0.2348\n",
      "Graph 31: Epoch: 006, Loss: -0.1039\n",
      "Graph 31: Epoch: 007, Loss: -0.1237\n",
      "Graph 31: Epoch: 008, Loss: -0.1823\n",
      "Graph 31: Epoch: 009, Loss: -0.2436\n",
      "Graph 31: Epoch: 010, Loss: -0.3380\n",
      "Graph 31: Epoch: 011, Loss: -0.1854\n",
      "Graph 31: Epoch: 012, Loss: -0.4283\n",
      "Graph 31: Epoch: 013, Loss: -0.4797\n",
      "Graph 31: Epoch: 014, Loss: -0.2342\n",
      "Graph 31: Epoch: 015, Loss: -0.2225\n",
      "Graph 31: Epoch: 016, Loss: -0.1083\n",
      "Graph 31: Epoch: 017, Loss: -0.3160\n",
      "Graph 31: Epoch: 018, Loss: -0.3020\n",
      "Graph 31: Epoch: 019, Loss: -0.4691\n",
      "Graph 31: Epoch: 020, Loss: -0.3714\n",
      "Graph 31: Epoch: 021, Loss: -0.3830\n",
      "Graph 31: Epoch: 022, Loss: -0.5006\n",
      "Graph 31: Epoch: 023, Loss: -0.4342\n",
      "Graph 31: Epoch: 024, Loss: -0.1908\n",
      "Graph 31: Epoch: 025, Loss: -0.6205\n",
      "Graph 31: Epoch: 026, Loss: -0.6641\n",
      "Graph 31: Epoch: 027, Loss: -0.2561\n",
      "Graph 31: Epoch: 028, Loss: -0.4487\n",
      "Graph 31: Epoch: 029, Loss: -0.6213\n",
      "Graph 31: Epoch: 030, Loss: -0.2538\n",
      "Graph 31: Epoch: 031, Loss: -0.3290\n",
      "Graph 31: Epoch: 032, Loss: -0.6541\n",
      "Graph 31: Epoch: 033, Loss: -0.4240\n",
      "Graph 31: Epoch: 034, Loss: -0.4264\n",
      "Graph 31: Epoch: 035, Loss: -0.6535\n",
      "Graph 31: Epoch: 036, Loss: -0.8239\n",
      "Graph 31: Epoch: 037, Loss: -0.1222\n",
      "Graph 31: Epoch: 038, Loss: -0.4477\n",
      "Graph 31: Epoch: 039, Loss: -0.4654\n",
      "Graph 31: Epoch: 040, Loss: -0.6537\n",
      "Graph 31: Epoch: 041, Loss: -0.3741\n",
      "Graph 31: Epoch: 042, Loss: -0.3099\n",
      "Graph 31: Epoch: 043, Loss: -0.3197\n",
      "Graph 31: Epoch: 044, Loss: -0.4668\n",
      "Graph 31: Epoch: 045, Loss: -0.4798\n",
      "Graph 31: Epoch: 046, Loss: -0.4635\n",
      "Graph 31: Epoch: 047, Loss: -0.3743\n",
      "Graph 31: Epoch: 048, Loss: -0.3874\n",
      "Graph 31: Epoch: 049, Loss: -0.4712\n",
      "Graph 31: Epoch: 050, Loss: -0.2517\n",
      "Graph 31: Epoch: 051, Loss: -0.0287\n",
      "Graph 31: Epoch: 052, Loss: -0.0308\n",
      "Graph 31: Epoch: 053, Loss: -0.6949\n",
      "Graph 31: Epoch: 054, Loss: -0.4252\n",
      "Graph 31: Epoch: 055, Loss: -0.3511\n",
      "Graph 31: Epoch: 056, Loss: -0.5122\n",
      "Graph 31: Epoch: 057, Loss: -0.3783\n",
      "Graph 31: Epoch: 058, Loss: -0.6745\n",
      "Graph 31: Epoch: 059, Loss: -0.3112\n",
      "Graph 31: Epoch: 060, Loss: -0.6076\n",
      "Graph 31: Epoch: 061, Loss: -0.7049\n",
      "Graph 31: Epoch: 062, Loss: -0.5338\n",
      "Graph 31: Epoch: 063, Loss: -0.2435\n",
      "Graph 31: Epoch: 064, Loss: -0.8399\n",
      "Graph 31: Epoch: 065, Loss: -0.4010\n",
      "Graph 31: Epoch: 066, Loss: -0.2503\n",
      "Graph 31: Epoch: 067, Loss: -0.0255\n",
      "Graph 31: Epoch: 068, Loss: -0.3549\n",
      "Graph 31: Epoch: 069, Loss: -0.5262\n",
      "Graph 31: Epoch: 070, Loss: -0.2705\n",
      "Graph 31: Epoch: 071, Loss: -0.5295\n",
      "Graph 31: Epoch: 072, Loss: -0.0801\n",
      "Graph 31: Epoch: 073, Loss: -0.2540\n",
      "Graph 31: Epoch: 074, Loss: -0.2996\n",
      "Graph 31: Epoch: 075, Loss: -0.9460\n",
      "Graph 31: Epoch: 076, Loss: -0.4598\n",
      "Graph 31: Epoch: 077, Loss: -0.3044\n",
      "Graph 31: Epoch: 078, Loss: -0.2418\n",
      "Graph 31: Epoch: 079, Loss: -0.6932\n",
      "Graph 31: Epoch: 080, Loss: -0.4415\n",
      "Graph 31: Epoch: 081, Loss: -0.3826\n",
      "Graph 31: Epoch: 082, Loss: -0.7168\n",
      "Graph 31: Epoch: 083, Loss: -0.2534\n",
      "Graph 31: Epoch: 084, Loss: -0.3068\n",
      "Graph 31: Epoch: 085, Loss: -0.2769\n",
      "Graph 31: Epoch: 086, Loss: -0.2414\n",
      "Graph 31: Epoch: 087, Loss: -0.2817\n",
      "Graph 31: Epoch: 088, Loss: -0.2555\n",
      "Graph 31: Epoch: 089, Loss: -0.4968\n",
      "Graph 31: Epoch: 090, Loss: -0.4939\n",
      "Graph 31: Epoch: 091, Loss: -0.3616\n",
      "Graph 31: Epoch: 092, Loss: -0.4075\n",
      "Graph 31: Epoch: 093, Loss: -0.4884\n",
      "Graph 31: Epoch: 094, Loss: -0.7562\n",
      "Graph 31: Epoch: 095, Loss: -0.5345\n",
      "Graph 31: Epoch: 096, Loss: -0.3683\n",
      "Graph 31: Epoch: 097, Loss: -0.7154\n",
      "Graph 31: Epoch: 098, Loss: -0.7119\n",
      "Graph 31: Epoch: 099, Loss: -0.5300\n",
      "Graph 31: Epoch: 100, Loss: -0.6498\n",
      "Graph 32: Epoch: 001, Loss: -0.0193\n",
      "Graph 32: Epoch: 002, Loss: -0.0500\n",
      "Graph 32: Epoch: 003, Loss: -0.1787\n",
      "Graph 32: Epoch: 004, Loss: -0.2595\n",
      "Graph 32: Epoch: 005, Loss: -0.2645\n",
      "Graph 32: Epoch: 006, Loss: -0.3388\n",
      "Graph 32: Epoch: 007, Loss: -0.4136\n",
      "Graph 32: Epoch: 008, Loss: -0.3713\n",
      "Graph 32: Epoch: 009, Loss: -0.3585\n",
      "Graph 32: Epoch: 010, Loss: -0.4355\n",
      "Graph 32: Epoch: 011, Loss: -0.3112\n",
      "Graph 32: Epoch: 012, Loss: -0.4027\n",
      "Graph 32: Epoch: 013, Loss: -0.3693\n",
      "Graph 32: Epoch: 014, Loss: -0.4148\n",
      "Graph 32: Epoch: 015, Loss: -0.4241\n",
      "Graph 32: Epoch: 016, Loss: -0.3624\n",
      "Graph 32: Epoch: 017, Loss: -0.4542\n",
      "Graph 32: Epoch: 018, Loss: -0.4741\n",
      "Graph 32: Epoch: 019, Loss: -0.4013\n",
      "Graph 32: Epoch: 020, Loss: -0.5110\n",
      "Graph 32: Epoch: 021, Loss: -0.5795\n",
      "Graph 32: Epoch: 022, Loss: -0.5557\n",
      "Graph 32: Epoch: 023, Loss: -0.5185\n",
      "Graph 32: Epoch: 024, Loss: -0.4662\n",
      "Graph 32: Epoch: 025, Loss: -0.4731\n",
      "Graph 32: Epoch: 026, Loss: -0.7089\n",
      "Graph 32: Epoch: 027, Loss: -0.4838\n",
      "Graph 32: Epoch: 028, Loss: -0.4270\n",
      "Graph 32: Epoch: 029, Loss: -0.3079\n",
      "Graph 32: Epoch: 030, Loss: -0.4288\n",
      "Graph 32: Epoch: 031, Loss: -0.5483\n",
      "Graph 32: Epoch: 032, Loss: -0.5485\n",
      "Graph 32: Epoch: 033, Loss: -0.4901\n",
      "Graph 32: Epoch: 034, Loss: -0.4905\n",
      "Graph 32: Epoch: 035, Loss: -0.4913\n",
      "Graph 32: Epoch: 036, Loss: -0.4939\n",
      "Graph 32: Epoch: 037, Loss: -0.3720\n",
      "Graph 32: Epoch: 038, Loss: -0.6160\n",
      "Graph 32: Epoch: 039, Loss: -0.5541\n",
      "Graph 32: Epoch: 040, Loss: -0.4934\n",
      "Graph 32: Epoch: 041, Loss: -0.3097\n",
      "Graph 32: Epoch: 042, Loss: -0.6171\n",
      "Graph 32: Epoch: 043, Loss: -0.4332\n",
      "Graph 32: Epoch: 044, Loss: -0.4329\n",
      "Graph 32: Epoch: 045, Loss: -0.6181\n",
      "Graph 32: Epoch: 046, Loss: -0.2480\n",
      "Graph 32: Epoch: 047, Loss: -0.6178\n",
      "Graph 32: Epoch: 048, Loss: -0.4331\n",
      "Graph 32: Epoch: 049, Loss: -0.3718\n",
      "Graph 32: Epoch: 050, Loss: -0.5567\n",
      "Graph 32: Epoch: 051, Loss: -0.5574\n",
      "Graph 32: Epoch: 052, Loss: -0.3100\n",
      "Graph 32: Epoch: 053, Loss: -0.8057\n",
      "Graph 32: Epoch: 054, Loss: -0.4335\n",
      "Graph 32: Epoch: 055, Loss: -0.3718\n",
      "Graph 32: Epoch: 056, Loss: -0.4339\n",
      "Graph 32: Epoch: 057, Loss: -0.5582\n",
      "Graph 32: Epoch: 058, Loss: -0.6198\n",
      "Graph 32: Epoch: 059, Loss: -0.4962\n",
      "Graph 32: Epoch: 060, Loss: -0.3103\n",
      "Graph 32: Epoch: 061, Loss: -0.6276\n",
      "Graph 32: Epoch: 062, Loss: -0.5582\n",
      "Graph 32: Epoch: 063, Loss: -0.4343\n",
      "Graph 32: Epoch: 064, Loss: -0.6206\n",
      "Graph 32: Epoch: 065, Loss: -0.3719\n",
      "Graph 32: Epoch: 066, Loss: -0.4965\n",
      "Graph 32: Epoch: 067, Loss: -0.4344\n",
      "Graph 32: Epoch: 068, Loss: -0.4346\n",
      "Graph 32: Epoch: 069, Loss: -0.5565\n",
      "Graph 32: Epoch: 070, Loss: -0.4343\n",
      "Graph 32: Epoch: 071, Loss: -0.5575\n",
      "Graph 32: Epoch: 072, Loss: -0.5581\n",
      "Graph 32: Epoch: 073, Loss: -0.5585\n",
      "Graph 32: Epoch: 074, Loss: -0.7449\n",
      "Graph 32: Epoch: 075, Loss: -0.6827\n",
      "Graph 32: Epoch: 076, Loss: -0.3727\n",
      "Graph 32: Epoch: 077, Loss: -0.6212\n",
      "Graph 32: Epoch: 078, Loss: -0.6210\n",
      "Graph 32: Epoch: 079, Loss: -0.4967\n",
      "Graph 32: Epoch: 080, Loss: -0.4971\n",
      "Graph 32: Epoch: 081, Loss: -0.5594\n",
      "Graph 32: Epoch: 082, Loss: -0.4349\n",
      "Graph 32: Epoch: 083, Loss: -0.4970\n",
      "Graph 32: Epoch: 084, Loss: -0.5594\n",
      "Graph 32: Epoch: 085, Loss: -0.5592\n",
      "Graph 32: Epoch: 086, Loss: -0.3731\n",
      "Graph 32: Epoch: 087, Loss: -0.7458\n",
      "Graph 32: Epoch: 088, Loss: -0.3110\n",
      "Graph 32: Epoch: 089, Loss: -0.4973\n",
      "Graph 32: Epoch: 090, Loss: -0.4353\n",
      "Graph 32: Epoch: 091, Loss: -0.6216\n",
      "Graph 32: Epoch: 092, Loss: -0.6838\n",
      "Graph 32: Epoch: 093, Loss: -0.3734\n",
      "Graph 32: Epoch: 094, Loss: -0.4973\n",
      "Graph 32: Epoch: 095, Loss: -0.5596\n",
      "Graph 32: Epoch: 096, Loss: -0.5596\n",
      "Graph 32: Epoch: 097, Loss: -0.3733\n",
      "Graph 32: Epoch: 098, Loss: -0.4353\n",
      "Graph 32: Epoch: 099, Loss: -0.5598\n",
      "Graph 32: Epoch: 100, Loss: -0.4977\n",
      "Graph 33: Epoch: 001, Loss: -0.0363\n",
      "Graph 33: Epoch: 002, Loss: -0.1767\n",
      "Graph 33: Epoch: 003, Loss: -0.2112\n",
      "Graph 33: Epoch: 004, Loss: -0.3396\n",
      "Graph 33: Epoch: 005, Loss: -0.3211\n",
      "Graph 33: Epoch: 006, Loss: -0.3715\n",
      "Graph 33: Epoch: 007, Loss: -0.3765\n",
      "Graph 33: Epoch: 008, Loss: -0.4060\n",
      "Graph 33: Epoch: 009, Loss: -0.4121\n",
      "Graph 33: Epoch: 010, Loss: -0.5003\n",
      "Graph 33: Epoch: 011, Loss: -0.3522\n",
      "Graph 33: Epoch: 012, Loss: -0.3788\n",
      "Graph 33: Epoch: 013, Loss: -0.4977\n",
      "Graph 33: Epoch: 014, Loss: -0.3112\n",
      "Graph 33: Epoch: 015, Loss: -0.4280\n",
      "Graph 33: Epoch: 016, Loss: -0.4833\n",
      "Graph 33: Epoch: 017, Loss: -0.4849\n",
      "Graph 33: Epoch: 018, Loss: -0.3620\n",
      "Graph 33: Epoch: 019, Loss: -0.4778\n",
      "Graph 33: Epoch: 020, Loss: -0.4055\n",
      "Graph 33: Epoch: 021, Loss: -0.5585\n",
      "Graph 33: Epoch: 022, Loss: -0.4542\n",
      "Graph 33: Epoch: 023, Loss: -0.4519\n",
      "Graph 33: Epoch: 024, Loss: -0.4076\n",
      "Graph 33: Epoch: 025, Loss: -0.5639\n",
      "Graph 33: Epoch: 026, Loss: -0.4110\n",
      "Graph 33: Epoch: 027, Loss: -0.3137\n",
      "Graph 33: Epoch: 028, Loss: -0.4647\n",
      "Graph 33: Epoch: 029, Loss: -0.5635\n",
      "Graph 33: Epoch: 030, Loss: -0.4137\n",
      "Graph 33: Epoch: 031, Loss: -0.5174\n",
      "Graph 33: Epoch: 032, Loss: -0.3623\n",
      "Graph 33: Epoch: 033, Loss: -0.5188\n",
      "Graph 33: Epoch: 034, Loss: -0.5712\n",
      "Graph 33: Epoch: 035, Loss: -0.4157\n",
      "Graph 33: Epoch: 036, Loss: -0.4162\n",
      "Graph 33: Epoch: 037, Loss: -0.5200\n",
      "Graph 33: Epoch: 038, Loss: -0.4163\n",
      "Graph 33: Epoch: 039, Loss: -0.4168\n",
      "Graph 33: Epoch: 040, Loss: -0.5198\n",
      "Graph 33: Epoch: 041, Loss: -0.5721\n",
      "Graph 33: Epoch: 042, Loss: -0.4166\n",
      "Graph 33: Epoch: 043, Loss: -0.4694\n",
      "Graph 33: Epoch: 044, Loss: -0.4691\n",
      "Graph 33: Epoch: 045, Loss: -0.4692\n",
      "Graph 33: Epoch: 046, Loss: -0.5740\n",
      "Graph 33: Epoch: 047, Loss: -0.5737\n",
      "Graph 33: Epoch: 048, Loss: -0.4176\n",
      "Graph 33: Epoch: 049, Loss: -0.3136\n",
      "Graph 33: Epoch: 050, Loss: -0.3135\n",
      "Graph 33: Epoch: 051, Loss: -0.5740\n",
      "Graph 33: Epoch: 052, Loss: -0.5226\n",
      "Graph 33: Epoch: 053, Loss: -0.5225\n",
      "Graph 33: Epoch: 054, Loss: -0.4182\n",
      "Graph 33: Epoch: 055, Loss: -0.3661\n",
      "Graph 33: Epoch: 056, Loss: -0.5748\n",
      "Graph 33: Epoch: 057, Loss: -0.6794\n",
      "Graph 33: Epoch: 058, Loss: -0.5229\n",
      "Graph 33: Epoch: 059, Loss: -0.5224\n",
      "Graph 33: Epoch: 060, Loss: -0.4187\n",
      "Graph 33: Epoch: 061, Loss: -0.3663\n",
      "Graph 33: Epoch: 062, Loss: -0.5233\n",
      "Graph 33: Epoch: 063, Loss: -0.4710\n",
      "Graph 33: Epoch: 064, Loss: -0.4707\n",
      "Graph 33: Epoch: 065, Loss: -0.5755\n",
      "Graph 33: Epoch: 066, Loss: -0.4710\n",
      "Graph 33: Epoch: 067, Loss: -0.4712\n",
      "Graph 33: Epoch: 068, Loss: -0.3143\n",
      "Graph 33: Epoch: 069, Loss: -0.4189\n",
      "Graph 33: Epoch: 070, Loss: -0.2097\n",
      "Graph 33: Epoch: 071, Loss: -0.6283\n",
      "Graph 33: Epoch: 072, Loss: -0.4189\n",
      "Graph 33: Epoch: 073, Loss: -0.5758\n",
      "Graph 33: Epoch: 074, Loss: -0.6282\n",
      "Graph 33: Epoch: 075, Loss: -0.5761\n",
      "Graph 33: Epoch: 076, Loss: -0.3144\n",
      "Graph 33: Epoch: 077, Loss: -0.4190\n",
      "Graph 33: Epoch: 078, Loss: -0.7333\n",
      "Graph 33: Epoch: 079, Loss: -0.3144\n",
      "Graph 33: Epoch: 080, Loss: -0.4714\n",
      "Graph 33: Epoch: 081, Loss: -0.3668\n",
      "Graph 33: Epoch: 082, Loss: -0.5239\n",
      "Graph 33: Epoch: 083, Loss: -0.6810\n",
      "Graph 33: Epoch: 084, Loss: -0.7334\n",
      "Graph 33: Epoch: 085, Loss: -0.3669\n",
      "Graph 33: Epoch: 086, Loss: -0.6287\n",
      "Graph 33: Epoch: 087, Loss: -0.5764\n",
      "Graph 33: Epoch: 088, Loss: -0.4192\n",
      "Graph 33: Epoch: 089, Loss: -0.5240\n",
      "Graph 33: Epoch: 090, Loss: -0.4718\n",
      "Graph 33: Epoch: 091, Loss: -0.5766\n",
      "Graph 33: Epoch: 092, Loss: -0.6289\n",
      "Graph 33: Epoch: 093, Loss: -0.4194\n",
      "Graph 33: Epoch: 094, Loss: -0.5766\n",
      "Graph 33: Epoch: 095, Loss: -0.5242\n",
      "Graph 33: Epoch: 096, Loss: -0.3147\n",
      "Graph 33: Epoch: 097, Loss: -0.5764\n",
      "Graph 33: Epoch: 098, Loss: -0.3671\n",
      "Graph 33: Epoch: 099, Loss: -0.4720\n",
      "Graph 33: Epoch: 100, Loss: -0.5242\n",
      "Graph 34: Epoch: 001, Loss: -0.1087\n",
      "Graph 34: Epoch: 002, Loss: -0.0753\n",
      "Graph 34: Epoch: 003, Loss: -0.2094\n",
      "Graph 34: Epoch: 004, Loss: -0.2166\n",
      "Graph 34: Epoch: 005, Loss: -0.3453\n",
      "Graph 34: Epoch: 006, Loss: -0.3755\n",
      "Graph 34: Epoch: 007, Loss: -0.2847\n",
      "Graph 34: Epoch: 008, Loss: -0.4734\n",
      "Graph 34: Epoch: 009, Loss: -0.4621\n",
      "Graph 34: Epoch: 010, Loss: -0.2498\n",
      "Graph 34: Epoch: 011, Loss: -0.3124\n",
      "Graph 34: Epoch: 012, Loss: -0.3164\n",
      "Graph 34: Epoch: 013, Loss: -0.2491\n",
      "Graph 34: Epoch: 014, Loss: -0.0962\n",
      "Graph 34: Epoch: 015, Loss: -0.5722\n",
      "Graph 34: Epoch: 016, Loss: -0.3998\n",
      "Graph 34: Epoch: 017, Loss: -0.4847\n",
      "Graph 34: Epoch: 018, Loss: -0.4052\n",
      "Graph 34: Epoch: 019, Loss: -0.4182\n",
      "Graph 34: Epoch: 020, Loss: -0.5662\n",
      "Graph 34: Epoch: 021, Loss: -0.5969\n",
      "Graph 34: Epoch: 022, Loss: -0.5111\n",
      "Graph 34: Epoch: 023, Loss: -0.3423\n",
      "Graph 34: Epoch: 024, Loss: -0.4033\n",
      "Graph 34: Epoch: 025, Loss: -0.3434\n",
      "Graph 34: Epoch: 026, Loss: -0.4336\n",
      "Graph 34: Epoch: 027, Loss: -0.4306\n",
      "Graph 34: Epoch: 028, Loss: -0.6045\n",
      "Graph 34: Epoch: 029, Loss: -0.2834\n",
      "Graph 34: Epoch: 030, Loss: -0.6078\n",
      "Graph 34: Epoch: 031, Loss: -0.6911\n",
      "Graph 34: Epoch: 032, Loss: -0.2640\n",
      "Graph 34: Epoch: 033, Loss: -0.6828\n",
      "Graph 34: Epoch: 034, Loss: -0.4401\n",
      "Graph 34: Epoch: 035, Loss: -0.3517\n",
      "Graph 34: Epoch: 036, Loss: -0.4382\n",
      "Graph 34: Epoch: 037, Loss: -0.4412\n",
      "Graph 34: Epoch: 038, Loss: -0.5266\n",
      "Graph 34: Epoch: 039, Loss: -0.5226\n",
      "Graph 34: Epoch: 040, Loss: -0.7936\n",
      "Graph 34: Epoch: 041, Loss: -0.5315\n",
      "Graph 34: Epoch: 042, Loss: -0.4419\n",
      "Graph 34: Epoch: 043, Loss: -0.7055\n",
      "Graph 34: Epoch: 044, Loss: -0.6205\n",
      "Graph 34: Epoch: 045, Loss: -0.5312\n",
      "Graph 34: Epoch: 046, Loss: -0.4577\n",
      "Graph 34: Epoch: 047, Loss: -0.2672\n",
      "Graph 34: Epoch: 048, Loss: -0.8002\n",
      "Graph 34: Epoch: 049, Loss: -0.6891\n",
      "Graph 34: Epoch: 050, Loss: -0.3573\n",
      "Graph 34: Epoch: 051, Loss: -0.8902\n",
      "Graph 34: Epoch: 052, Loss: -0.5312\n",
      "Graph 34: Epoch: 053, Loss: -0.4465\n",
      "Graph 34: Epoch: 054, Loss: -0.5296\n",
      "Graph 34: Epoch: 055, Loss: -0.7138\n",
      "Graph 34: Epoch: 056, Loss: -0.5361\n",
      "Graph 34: Epoch: 057, Loss: -0.2688\n",
      "Graph 34: Epoch: 058, Loss: -0.5368\n",
      "Graph 34: Epoch: 059, Loss: -0.5212\n",
      "Graph 34: Epoch: 060, Loss: -0.3583\n",
      "Graph 34: Epoch: 061, Loss: -0.6263\n",
      "Graph 34: Epoch: 062, Loss: -0.7152\n",
      "Graph 34: Epoch: 063, Loss: -0.5376\n",
      "Graph 34: Epoch: 064, Loss: -0.3588\n",
      "Graph 34: Epoch: 065, Loss: -0.3586\n",
      "Graph 34: Epoch: 066, Loss: -0.4484\n",
      "Graph 34: Epoch: 067, Loss: -0.7152\n",
      "Graph 34: Epoch: 068, Loss: -0.7158\n",
      "Graph 34: Epoch: 069, Loss: -0.4399\n",
      "Graph 34: Epoch: 070, Loss: -0.5373\n",
      "Graph 34: Epoch: 071, Loss: -0.1835\n",
      "Graph 34: Epoch: 072, Loss: -0.6281\n",
      "Graph 34: Epoch: 073, Loss: -0.3604\n",
      "Graph 34: Epoch: 074, Loss: -0.5384\n",
      "Graph 34: Epoch: 075, Loss: -0.3538\n",
      "Graph 34: Epoch: 076, Loss: -0.7171\n",
      "Graph 34: Epoch: 077, Loss: -0.5396\n",
      "Graph 34: Epoch: 078, Loss: -0.6298\n",
      "Graph 34: Epoch: 079, Loss: -0.8092\n",
      "Graph 34: Epoch: 080, Loss: -0.4498\n",
      "Graph 34: Epoch: 081, Loss: -0.4498\n",
      "Graph 34: Epoch: 082, Loss: -0.3602\n",
      "Graph 34: Epoch: 083, Loss: -0.8384\n",
      "Graph 34: Epoch: 084, Loss: -0.5398\n",
      "Graph 34: Epoch: 085, Loss: -0.2706\n",
      "Graph 34: Epoch: 086, Loss: -0.4496\n",
      "Graph 34: Epoch: 087, Loss: -0.5400\n",
      "Graph 34: Epoch: 088, Loss: -0.3591\n",
      "Graph 34: Epoch: 089, Loss: -0.4505\n",
      "Graph 34: Epoch: 090, Loss: -0.5407\n",
      "Graph 34: Epoch: 091, Loss: -0.7144\n",
      "Graph 34: Epoch: 092, Loss: -0.6305\n",
      "Graph 34: Epoch: 093, Loss: -0.4505\n",
      "Graph 34: Epoch: 094, Loss: -0.7188\n",
      "Graph 34: Epoch: 095, Loss: -0.4400\n",
      "Graph 34: Epoch: 096, Loss: -0.4509\n",
      "Graph 34: Epoch: 097, Loss: -0.4511\n",
      "Graph 34: Epoch: 098, Loss: -0.5760\n",
      "Graph 34: Epoch: 099, Loss: -0.5411\n",
      "Graph 34: Epoch: 100, Loss: -0.5409\n",
      "Graph 35: Epoch: 001, Loss: -0.2098\n",
      "Graph 35: Epoch: 002, Loss: -0.2046\n",
      "Graph 35: Epoch: 003, Loss: -0.3898\n",
      "Graph 35: Epoch: 004, Loss: -0.0265\n",
      "Graph 35: Epoch: 005, Loss: -0.4437\n",
      "Graph 35: Epoch: 006, Loss: -0.2761\n",
      "Graph 35: Epoch: 007, Loss: -0.4426\n",
      "Graph 35: Epoch: 008, Loss: -0.1951\n",
      "Graph 35: Epoch: 009, Loss: -0.2783\n",
      "Graph 35: Epoch: 010, Loss: -0.4904\n",
      "Graph 35: Epoch: 011, Loss: -0.4798\n",
      "Graph 35: Epoch: 012, Loss: -0.5159\n",
      "Graph 35: Epoch: 013, Loss: -0.6098\n",
      "Graph 35: Epoch: 014, Loss: -0.6372\n",
      "Graph 35: Epoch: 015, Loss: -0.3322\n",
      "Graph 35: Epoch: 016, Loss: -0.2285\n",
      "Graph 35: Epoch: 017, Loss: -0.4923\n",
      "Graph 35: Epoch: 018, Loss: -0.5233\n",
      "Graph 35: Epoch: 019, Loss: -0.2173\n",
      "Graph 35: Epoch: 020, Loss: -0.4986\n",
      "Graph 35: Epoch: 021, Loss: -0.4228\n",
      "Graph 35: Epoch: 022, Loss: -0.4374\n",
      "Graph 35: Epoch: 023, Loss: -0.3907\n",
      "Graph 35: Epoch: 024, Loss: -0.1930\n",
      "Graph 35: Epoch: 025, Loss: -0.5281\n",
      "Graph 35: Epoch: 026, Loss: -0.3711\n",
      "Graph 35: Epoch: 027, Loss: -0.5406\n",
      "Graph 35: Epoch: 028, Loss: -0.3744\n",
      "Graph 35: Epoch: 029, Loss: -0.4226\n",
      "Graph 35: Epoch: 030, Loss: -0.2003\n",
      "Graph 35: Epoch: 031, Loss: -0.6204\n",
      "Graph 35: Epoch: 032, Loss: -0.4096\n",
      "Graph 35: Epoch: 033, Loss: -0.3749\n",
      "Graph 35: Epoch: 034, Loss: -0.7319\n",
      "Graph 35: Epoch: 035, Loss: -0.3693\n",
      "Graph 35: Epoch: 036, Loss: -0.5528\n",
      "Graph 35: Epoch: 037, Loss: -0.4165\n",
      "Graph 35: Epoch: 038, Loss: -0.3794\n",
      "Graph 35: Epoch: 039, Loss: -0.4269\n",
      "Graph 35: Epoch: 040, Loss: -0.3011\n",
      "Graph 35: Epoch: 041, Loss: -0.2022\n",
      "Graph 35: Epoch: 042, Loss: -0.7338\n",
      "Graph 35: Epoch: 043, Loss: -0.3844\n",
      "Graph 35: Epoch: 044, Loss: -0.1957\n",
      "Graph 35: Epoch: 045, Loss: -0.8110\n",
      "Graph 35: Epoch: 046, Loss: -0.4590\n",
      "Graph 35: Epoch: 047, Loss: -0.7495\n",
      "Graph 35: Epoch: 048, Loss: -0.5939\n",
      "Graph 35: Epoch: 049, Loss: -0.2776\n",
      "Graph 35: Epoch: 050, Loss: -0.7549\n",
      "Graph 35: Epoch: 051, Loss: -0.3874\n",
      "Graph 35: Epoch: 052, Loss: -0.3842\n",
      "Graph 35: Epoch: 053, Loss: -0.5709\n",
      "Graph 35: Epoch: 054, Loss: -0.1685\n",
      "Graph 35: Epoch: 055, Loss: -0.5705\n",
      "Graph 35: Epoch: 056, Loss: -0.3882\n",
      "Graph 35: Epoch: 057, Loss: -0.4944\n",
      "Graph 35: Epoch: 058, Loss: -0.3939\n",
      "Graph 35: Epoch: 059, Loss: -0.3803\n",
      "Graph 35: Epoch: 060, Loss: -0.6073\n",
      "Graph 35: Epoch: 061, Loss: -0.2087\n",
      "Graph 35: Epoch: 062, Loss: -0.2583\n",
      "Graph 35: Epoch: 063, Loss: -0.5743\n",
      "Graph 35: Epoch: 064, Loss: -0.4218\n",
      "Graph 35: Epoch: 065, Loss: -0.2073\n",
      "Graph 35: Epoch: 066, Loss: -0.4915\n",
      "Graph 35: Epoch: 067, Loss: -0.1943\n",
      "Graph 35: Epoch: 068, Loss: -0.7668\n",
      "Graph 35: Epoch: 069, Loss: -0.3924\n",
      "Graph 35: Epoch: 070, Loss: -0.1918\n",
      "Graph 35: Epoch: 071, Loss: -0.1998\n",
      "Graph 35: Epoch: 072, Loss: -0.5042\n",
      "Graph 35: Epoch: 073, Loss: -0.9521\n",
      "Graph 35: Epoch: 074, Loss: -0.3935\n",
      "Graph 35: Epoch: 075, Loss: -0.7740\n",
      "Graph 35: Epoch: 076, Loss: -0.3921\n",
      "Graph 35: Epoch: 077, Loss: -0.3924\n",
      "Graph 35: Epoch: 078, Loss: -0.3891\n",
      "Graph 35: Epoch: 079, Loss: -0.3903\n",
      "Graph 35: Epoch: 080, Loss: -0.2012\n",
      "Graph 35: Epoch: 081, Loss: -0.5264\n",
      "Graph 35: Epoch: 082, Loss: -0.1986\n",
      "Graph 35: Epoch: 083, Loss: -0.7546\n",
      "Graph 35: Epoch: 084, Loss: -0.4880\n",
      "Graph 35: Epoch: 085, Loss: -0.5861\n",
      "Graph 35: Epoch: 086, Loss: -0.3925\n",
      "Graph 35: Epoch: 087, Loss: -0.6198\n",
      "Graph 35: Epoch: 088, Loss: -0.5829\n",
      "Graph 35: Epoch: 089, Loss: -0.2068\n",
      "Graph 35: Epoch: 090, Loss: -0.3910\n",
      "Graph 35: Epoch: 091, Loss: -0.0834\n",
      "Graph 35: Epoch: 092, Loss: -0.3474\n",
      "Graph 35: Epoch: 093, Loss: -0.3735\n",
      "Graph 35: Epoch: 094, Loss: -0.5818\n",
      "Graph 35: Epoch: 095, Loss: -0.3820\n",
      "Graph 35: Epoch: 096, Loss: -0.5853\n",
      "Graph 35: Epoch: 097, Loss: -0.2013\n",
      "Graph 35: Epoch: 098, Loss: -0.7771\n",
      "Graph 35: Epoch: 099, Loss: -0.2379\n",
      "Graph 35: Epoch: 100, Loss: -0.3862\n",
      "Graph 36: Epoch: 001, Loss: -0.0263\n",
      "Graph 36: Epoch: 002, Loss: -0.0877\n",
      "Graph 36: Epoch: 003, Loss: -0.2348\n",
      "Graph 36: Epoch: 004, Loss: -0.2772\n",
      "Graph 36: Epoch: 005, Loss: -0.3647\n",
      "Graph 36: Epoch: 006, Loss: -0.3647\n",
      "Graph 36: Epoch: 007, Loss: -0.4190\n",
      "Graph 36: Epoch: 008, Loss: -0.2515\n",
      "Graph 36: Epoch: 009, Loss: -0.2824\n",
      "Graph 36: Epoch: 010, Loss: -0.4503\n",
      "Graph 36: Epoch: 011, Loss: -0.4564\n",
      "Graph 36: Epoch: 012, Loss: -0.7087\n",
      "Graph 36: Epoch: 013, Loss: -0.3938\n",
      "Graph 36: Epoch: 014, Loss: -0.3378\n",
      "Graph 36: Epoch: 015, Loss: -0.6316\n",
      "Graph 36: Epoch: 016, Loss: -0.6430\n",
      "Graph 36: Epoch: 017, Loss: -0.5066\n",
      "Graph 36: Epoch: 018, Loss: -0.5802\n",
      "Graph 36: Epoch: 019, Loss: -0.3485\n",
      "Graph 36: Epoch: 020, Loss: -0.6445\n",
      "Graph 36: Epoch: 021, Loss: -0.2885\n",
      "Graph 36: Epoch: 022, Loss: -0.5906\n",
      "Graph 36: Epoch: 023, Loss: -0.3693\n",
      "Graph 36: Epoch: 024, Loss: -0.5924\n",
      "Graph 36: Epoch: 025, Loss: -0.4455\n",
      "Graph 36: Epoch: 026, Loss: -0.2898\n",
      "Graph 36: Epoch: 027, Loss: -0.4455\n",
      "Graph 36: Epoch: 028, Loss: -0.2978\n",
      "Graph 36: Epoch: 029, Loss: -0.2985\n",
      "Graph 36: Epoch: 030, Loss: -0.7443\n",
      "Graph 36: Epoch: 031, Loss: -0.5211\n",
      "Graph 36: Epoch: 032, Loss: -0.5990\n",
      "Graph 36: Epoch: 033, Loss: -0.4492\n",
      "Graph 36: Epoch: 034, Loss: -0.2252\n",
      "Graph 36: Epoch: 035, Loss: -0.5257\n",
      "Graph 36: Epoch: 036, Loss: -0.4513\n",
      "Graph 36: Epoch: 037, Loss: -0.3724\n",
      "Graph 36: Epoch: 038, Loss: -0.6018\n",
      "Graph 36: Epoch: 039, Loss: -0.5283\n",
      "Graph 36: Epoch: 040, Loss: -0.4519\n",
      "Graph 36: Epoch: 041, Loss: -0.6039\n",
      "Graph 36: Epoch: 042, Loss: -0.4530\n",
      "Graph 36: Epoch: 043, Loss: -0.4531\n",
      "Graph 36: Epoch: 044, Loss: -0.3788\n",
      "Graph 36: Epoch: 045, Loss: -0.4537\n",
      "Graph 36: Epoch: 046, Loss: -0.4538\n",
      "Graph 36: Epoch: 047, Loss: -0.4541\n",
      "Graph 36: Epoch: 048, Loss: -0.6054\n",
      "Graph 36: Epoch: 049, Loss: -0.3787\n",
      "Graph 36: Epoch: 050, Loss: -0.5293\n",
      "Graph 36: Epoch: 051, Loss: -0.2273\n",
      "Graph 36: Epoch: 052, Loss: -0.3035\n",
      "Graph 36: Epoch: 053, Loss: -0.5305\n",
      "Graph 36: Epoch: 054, Loss: -0.2265\n",
      "Graph 36: Epoch: 055, Loss: -0.6073\n",
      "Graph 36: Epoch: 056, Loss: -0.5314\n",
      "Graph 36: Epoch: 057, Loss: -0.6063\n",
      "Graph 36: Epoch: 058, Loss: -0.5317\n",
      "Graph 36: Epoch: 059, Loss: -0.6834\n",
      "Graph 36: Epoch: 060, Loss: -0.4560\n",
      "Graph 36: Epoch: 061, Loss: -0.6077\n",
      "Graph 36: Epoch: 062, Loss: -0.6839\n",
      "Graph 36: Epoch: 063, Loss: -0.5315\n",
      "Graph 36: Epoch: 064, Loss: -0.6079\n",
      "Graph 36: Epoch: 065, Loss: -0.4563\n",
      "Graph 36: Epoch: 066, Loss: -0.3048\n",
      "Graph 36: Epoch: 067, Loss: -0.2287\n",
      "Graph 36: Epoch: 068, Loss: -0.6848\n",
      "Graph 36: Epoch: 069, Loss: -0.3806\n",
      "Graph 36: Epoch: 070, Loss: -0.5330\n",
      "Graph 36: Epoch: 071, Loss: -0.2287\n",
      "Graph 36: Epoch: 072, Loss: -0.7613\n",
      "Graph 36: Epoch: 073, Loss: -0.3050\n",
      "Graph 36: Epoch: 074, Loss: -0.6091\n",
      "Graph 36: Epoch: 075, Loss: -0.6094\n",
      "Graph 36: Epoch: 076, Loss: -0.3812\n",
      "Graph 36: Epoch: 077, Loss: -0.4574\n",
      "Graph 36: Epoch: 078, Loss: -0.6098\n",
      "Graph 36: Epoch: 079, Loss: -0.6096\n",
      "Graph 36: Epoch: 080, Loss: -0.4575\n",
      "Graph 36: Epoch: 081, Loss: -0.6095\n",
      "Graph 36: Epoch: 082, Loss: -0.3814\n",
      "Graph 36: Epoch: 083, Loss: -0.4577\n",
      "Graph 36: Epoch: 084, Loss: -0.7610\n",
      "Graph 36: Epoch: 085, Loss: -0.3817\n",
      "Graph 36: Epoch: 086, Loss: -0.2292\n",
      "Graph 36: Epoch: 087, Loss: -0.5342\n",
      "Graph 36: Epoch: 088, Loss: -0.6103\n",
      "Graph 36: Epoch: 089, Loss: -0.6100\n",
      "Graph 36: Epoch: 090, Loss: -0.6869\n",
      "Graph 36: Epoch: 091, Loss: -0.4580\n",
      "Graph 36: Epoch: 092, Loss: -0.4582\n",
      "Graph 36: Epoch: 093, Loss: -0.6105\n",
      "Graph 36: Epoch: 094, Loss: -0.6107\n",
      "Graph 36: Epoch: 095, Loss: -0.6872\n",
      "Graph 36: Epoch: 096, Loss: -0.8382\n",
      "Graph 36: Epoch: 097, Loss: -0.6109\n",
      "Graph 36: Epoch: 098, Loss: -0.6874\n",
      "Graph 36: Epoch: 099, Loss: -0.6870\n",
      "Graph 36: Epoch: 100, Loss: -0.6112\n",
      "Graph 37: Epoch: 001, Loss: -0.0595\n",
      "Graph 37: Epoch: 002, Loss: -0.2226\n",
      "Graph 37: Epoch: 003, Loss: -0.3089\n",
      "Graph 37: Epoch: 004, Loss: -0.2810\n",
      "Graph 37: Epoch: 005, Loss: -0.4193\n",
      "Graph 37: Epoch: 006, Loss: -0.6028\n",
      "Graph 37: Epoch: 007, Loss: -0.4588\n",
      "Graph 37: Epoch: 008, Loss: -0.4222\n",
      "Graph 37: Epoch: 009, Loss: -0.6422\n",
      "Graph 37: Epoch: 010, Loss: -0.4987\n",
      "Graph 37: Epoch: 011, Loss: -0.3908\n",
      "Graph 37: Epoch: 012, Loss: -0.5757\n",
      "Graph 37: Epoch: 013, Loss: -0.5981\n",
      "Graph 37: Epoch: 014, Loss: -0.5964\n",
      "Graph 37: Epoch: 015, Loss: -0.4714\n",
      "Graph 37: Epoch: 016, Loss: -0.4043\n",
      "Graph 37: Epoch: 017, Loss: -0.5343\n",
      "Graph 37: Epoch: 018, Loss: -0.2681\n",
      "Graph 37: Epoch: 019, Loss: -0.5412\n",
      "Graph 37: Epoch: 020, Loss: -0.3411\n",
      "Graph 37: Epoch: 021, Loss: -0.2020\n",
      "Graph 37: Epoch: 022, Loss: -0.4052\n",
      "Graph 37: Epoch: 023, Loss: -0.5561\n",
      "Graph 37: Epoch: 024, Loss: -0.4822\n",
      "Graph 37: Epoch: 025, Loss: -0.3430\n",
      "Graph 37: Epoch: 026, Loss: -0.6209\n",
      "Graph 37: Epoch: 027, Loss: -0.5543\n",
      "Graph 37: Epoch: 028, Loss: -0.5537\n",
      "Graph 37: Epoch: 029, Loss: -0.6240\n",
      "Graph 37: Epoch: 030, Loss: -0.4176\n",
      "Graph 37: Epoch: 031, Loss: -0.3486\n",
      "Graph 37: Epoch: 032, Loss: -0.4878\n",
      "Graph 37: Epoch: 033, Loss: -0.3490\n",
      "Graph 37: Epoch: 034, Loss: -0.6278\n",
      "Graph 37: Epoch: 035, Loss: -0.4173\n",
      "Graph 37: Epoch: 036, Loss: -0.4890\n",
      "Graph 37: Epoch: 037, Loss: -0.3502\n",
      "Graph 37: Epoch: 038, Loss: -0.4901\n",
      "Graph 37: Epoch: 039, Loss: -0.6287\n",
      "Graph 37: Epoch: 040, Loss: -0.4207\n",
      "Graph 37: Epoch: 041, Loss: -0.4907\n",
      "Graph 37: Epoch: 042, Loss: -0.4210\n",
      "Graph 37: Epoch: 043, Loss: -0.4916\n",
      "Graph 37: Epoch: 044, Loss: -0.6311\n",
      "Graph 37: Epoch: 045, Loss: -0.4218\n",
      "Graph 37: Epoch: 046, Loss: -0.7021\n",
      "Graph 37: Epoch: 047, Loss: -0.6315\n",
      "Graph 37: Epoch: 048, Loss: -0.7725\n",
      "Graph 37: Epoch: 049, Loss: -0.6327\n",
      "Graph 37: Epoch: 050, Loss: -0.4226\n",
      "Graph 37: Epoch: 051, Loss: -0.3528\n",
      "Graph 37: Epoch: 052, Loss: -0.7748\n",
      "Graph 37: Epoch: 053, Loss: -0.4232\n",
      "Graph 37: Epoch: 054, Loss: -0.5639\n",
      "Graph 37: Epoch: 055, Loss: -0.4234\n",
      "Graph 37: Epoch: 056, Loss: -0.4941\n",
      "Graph 37: Epoch: 057, Loss: -0.2828\n",
      "Graph 37: Epoch: 058, Loss: -0.3528\n",
      "Graph 37: Epoch: 059, Loss: -0.3533\n",
      "Graph 37: Epoch: 060, Loss: -0.5649\n",
      "Graph 37: Epoch: 061, Loss: -0.2829\n",
      "Graph 37: Epoch: 062, Loss: -0.4946\n",
      "Graph 37: Epoch: 063, Loss: -0.3521\n",
      "Graph 37: Epoch: 064, Loss: -0.4946\n",
      "Graph 37: Epoch: 065, Loss: -0.3501\n",
      "Graph 37: Epoch: 066, Loss: -0.5652\n",
      "Graph 37: Epoch: 067, Loss: -0.4241\n",
      "Graph 37: Epoch: 068, Loss: -0.5657\n",
      "Graph 37: Epoch: 069, Loss: -0.5654\n",
      "Graph 37: Epoch: 070, Loss: -0.3540\n",
      "Graph 37: Epoch: 071, Loss: -0.6366\n",
      "Graph 37: Epoch: 072, Loss: -0.4243\n",
      "Graph 37: Epoch: 073, Loss: -0.5655\n",
      "Graph 37: Epoch: 074, Loss: -0.4954\n",
      "Graph 37: Epoch: 075, Loss: -0.6369\n",
      "Graph 37: Epoch: 076, Loss: -0.4956\n",
      "Graph 37: Epoch: 077, Loss: -0.3541\n",
      "Graph 37: Epoch: 078, Loss: -0.7075\n",
      "Graph 37: Epoch: 079, Loss: -0.4248\n",
      "Graph 37: Epoch: 080, Loss: -0.4952\n",
      "Graph 37: Epoch: 081, Loss: -0.5660\n",
      "Graph 37: Epoch: 082, Loss: -0.5666\n",
      "Graph 37: Epoch: 083, Loss: -0.4251\n",
      "Graph 37: Epoch: 084, Loss: -0.5664\n",
      "Graph 37: Epoch: 085, Loss: -0.3545\n",
      "Graph 37: Epoch: 086, Loss: -0.7083\n",
      "Graph 37: Epoch: 087, Loss: -0.4254\n",
      "Graph 37: Epoch: 088, Loss: -0.4253\n",
      "Graph 37: Epoch: 089, Loss: -0.5670\n",
      "Graph 37: Epoch: 090, Loss: -0.2838\n",
      "Graph 37: Epoch: 091, Loss: -0.4964\n",
      "Graph 37: Epoch: 092, Loss: -0.3548\n",
      "Graph 37: Epoch: 093, Loss: -0.4965\n",
      "Graph 37: Epoch: 094, Loss: -0.4965\n",
      "Graph 37: Epoch: 095, Loss: -0.4963\n",
      "Graph 37: Epoch: 096, Loss: -0.4965\n",
      "Graph 37: Epoch: 097, Loss: -0.6383\n",
      "Graph 37: Epoch: 098, Loss: -0.4965\n",
      "Graph 37: Epoch: 099, Loss: -0.6385\n",
      "Graph 37: Epoch: 100, Loss: -0.5676\n",
      "Graph 38: Epoch: 001, Loss: -0.1336\n",
      "Graph 38: Epoch: 002, Loss: -0.1348\n",
      "Graph 38: Epoch: 003, Loss: -0.2483\n",
      "Graph 38: Epoch: 004, Loss: -0.2768\n",
      "Graph 38: Epoch: 005, Loss: -0.3832\n",
      "Graph 38: Epoch: 006, Loss: -0.3860\n",
      "Graph 38: Epoch: 007, Loss: -0.5112\n",
      "Graph 38: Epoch: 008, Loss: -0.4424\n",
      "Graph 38: Epoch: 009, Loss: -0.5901\n",
      "Graph 38: Epoch: 010, Loss: -0.5516\n",
      "Graph 38: Epoch: 011, Loss: -0.4068\n",
      "Graph 38: Epoch: 012, Loss: -0.4811\n",
      "Graph 38: Epoch: 013, Loss: -0.2965\n",
      "Graph 38: Epoch: 014, Loss: -0.3245\n",
      "Graph 38: Epoch: 015, Loss: -0.3017\n",
      "Graph 38: Epoch: 016, Loss: -0.6900\n",
      "Graph 38: Epoch: 017, Loss: -0.4425\n",
      "Graph 38: Epoch: 018, Loss: -0.4418\n",
      "Graph 38: Epoch: 019, Loss: -0.2894\n",
      "Graph 38: Epoch: 020, Loss: -0.1877\n",
      "Graph 38: Epoch: 021, Loss: -0.4885\n",
      "Graph 38: Epoch: 022, Loss: -0.6422\n",
      "Graph 38: Epoch: 023, Loss: -0.0342\n",
      "Graph 38: Epoch: 024, Loss: -0.4533\n",
      "Graph 38: Epoch: 025, Loss: -0.3903\n",
      "Graph 38: Epoch: 026, Loss: -0.5040\n",
      "Graph 38: Epoch: 027, Loss: -0.2650\n",
      "Graph 38: Epoch: 028, Loss: -0.3286\n",
      "Graph 38: Epoch: 029, Loss: -0.6147\n",
      "Graph 38: Epoch: 030, Loss: -0.3155\n",
      "Graph 38: Epoch: 031, Loss: -0.1685\n",
      "Graph 38: Epoch: 032, Loss: -0.3539\n",
      "Graph 38: Epoch: 033, Loss: -0.4790\n",
      "Graph 38: Epoch: 034, Loss: -0.6172\n",
      "Graph 38: Epoch: 035, Loss: -0.4705\n",
      "Graph 38: Epoch: 036, Loss: -0.5794\n",
      "Graph 38: Epoch: 037, Loss: -0.1696\n",
      "Graph 38: Epoch: 038, Loss: -0.3184\n",
      "Graph 38: Epoch: 039, Loss: -0.3348\n",
      "Graph 38: Epoch: 040, Loss: -0.7804\n",
      "Graph 38: Epoch: 041, Loss: -0.6282\n",
      "Graph 38: Epoch: 042, Loss: -0.3578\n",
      "Graph 38: Epoch: 043, Loss: -0.3489\n",
      "Graph 38: Epoch: 044, Loss: -0.4591\n",
      "Graph 38: Epoch: 045, Loss: -0.3189\n",
      "Graph 38: Epoch: 046, Loss: -0.7667\n",
      "Graph 38: Epoch: 047, Loss: -0.0101\n",
      "Graph 38: Epoch: 048, Loss: -0.3200\n",
      "Graph 38: Epoch: 049, Loss: -0.3210\n",
      "Graph 38: Epoch: 050, Loss: -0.6120\n",
      "Graph 38: Epoch: 051, Loss: -0.5718\n",
      "Graph 38: Epoch: 052, Loss: -0.3870\n",
      "Graph 38: Epoch: 053, Loss: -0.6405\n",
      "Graph 38: Epoch: 054, Loss: -0.7977\n",
      "Graph 38: Epoch: 055, Loss: -0.4737\n",
      "Graph 38: Epoch: 056, Loss: -0.4874\n",
      "Graph 38: Epoch: 057, Loss: -0.9613\n",
      "Graph 38: Epoch: 058, Loss: -0.3257\n",
      "Graph 38: Epoch: 059, Loss: -0.4815\n",
      "Graph 38: Epoch: 060, Loss: -0.3775\n",
      "Graph 38: Epoch: 061, Loss: -0.3267\n",
      "Graph 38: Epoch: 062, Loss: -0.1704\n",
      "Graph 38: Epoch: 063, Loss: -0.3257\n",
      "Graph 38: Epoch: 064, Loss: -0.3292\n",
      "Graph 38: Epoch: 065, Loss: -0.3623\n",
      "Graph 38: Epoch: 066, Loss: -0.6470\n",
      "Graph 38: Epoch: 067, Loss: -0.0958\n",
      "Graph 38: Epoch: 068, Loss: -0.5673\n",
      "Graph 38: Epoch: 069, Loss: -0.8048\n",
      "Graph 38: Epoch: 070, Loss: -0.6490\n",
      "Graph 38: Epoch: 071, Loss: -0.2742\n",
      "Graph 38: Epoch: 072, Loss: -0.7999\n",
      "Graph 38: Epoch: 073, Loss: -0.3293\n",
      "Graph 38: Epoch: 074, Loss: -0.2033\n",
      "Graph 38: Epoch: 075, Loss: -0.3284\n",
      "Graph 38: Epoch: 076, Loss: -0.3186\n",
      "Graph 38: Epoch: 077, Loss: -0.3290\n",
      "Graph 38: Epoch: 078, Loss: -0.4855\n",
      "Graph 38: Epoch: 079, Loss: -0.6292\n",
      "Graph 38: Epoch: 080, Loss: -0.4891\n",
      "Graph 38: Epoch: 081, Loss: -0.4317\n",
      "Graph 38: Epoch: 082, Loss: -0.4875\n",
      "Graph 38: Epoch: 083, Loss: -0.1637\n",
      "Graph 38: Epoch: 084, Loss: -0.4839\n",
      "Graph 38: Epoch: 085, Loss: -0.6892\n",
      "Graph 38: Epoch: 086, Loss: -0.6470\n",
      "Graph 38: Epoch: 087, Loss: -0.5329\n",
      "Graph 38: Epoch: 088, Loss: -0.6521\n",
      "Graph 38: Epoch: 089, Loss: -0.3270\n",
      "Graph 38: Epoch: 090, Loss: -0.0073\n",
      "Graph 38: Epoch: 091, Loss: -0.4911\n",
      "Graph 38: Epoch: 092, Loss: -0.4906\n",
      "Graph 38: Epoch: 093, Loss: -0.5275\n",
      "Graph 38: Epoch: 094, Loss: -0.1686\n",
      "Graph 38: Epoch: 095, Loss: -0.5259\n",
      "Graph 38: Epoch: 096, Loss: -0.4931\n",
      "Graph 38: Epoch: 097, Loss: -0.1677\n",
      "Graph 38: Epoch: 098, Loss: -0.4849\n",
      "Graph 38: Epoch: 099, Loss: -0.5280\n",
      "Graph 38: Epoch: 100, Loss: -0.6547\n",
      "Graph 39: Epoch: 001, Loss: -0.0077\n",
      "Graph 39: Epoch: 002, Loss: -0.0238\n",
      "Graph 39: Epoch: 003, Loss: -0.0555\n",
      "Graph 39: Epoch: 004, Loss: -0.1038\n",
      "Graph 39: Epoch: 005, Loss: -0.1504\n",
      "Graph 39: Epoch: 006, Loss: -0.2280\n",
      "Graph 39: Epoch: 007, Loss: -0.2216\n",
      "Graph 39: Epoch: 008, Loss: -0.2753\n",
      "Graph 39: Epoch: 009, Loss: -0.3249\n",
      "Graph 39: Epoch: 010, Loss: -0.3254\n",
      "Graph 39: Epoch: 011, Loss: -0.3063\n",
      "Graph 39: Epoch: 012, Loss: -0.3588\n",
      "Graph 39: Epoch: 013, Loss: -0.3747\n",
      "Graph 39: Epoch: 014, Loss: -0.3891\n",
      "Graph 39: Epoch: 015, Loss: -0.3643\n",
      "Graph 39: Epoch: 016, Loss: -0.4414\n",
      "Graph 39: Epoch: 017, Loss: -0.4091\n",
      "Graph 39: Epoch: 018, Loss: -0.3539\n",
      "Graph 39: Epoch: 019, Loss: -0.4411\n",
      "Graph 39: Epoch: 020, Loss: -0.3912\n",
      "Graph 39: Epoch: 021, Loss: -0.2904\n",
      "Graph 39: Epoch: 022, Loss: -0.5267\n",
      "Graph 39: Epoch: 023, Loss: -0.3350\n",
      "Graph 39: Epoch: 024, Loss: -0.4379\n",
      "Graph 39: Epoch: 025, Loss: -0.4140\n",
      "Graph 39: Epoch: 026, Loss: -0.3554\n",
      "Graph 39: Epoch: 027, Loss: -0.5504\n",
      "Graph 39: Epoch: 028, Loss: -0.4725\n",
      "Graph 39: Epoch: 029, Loss: -0.4860\n",
      "Graph 39: Epoch: 030, Loss: -0.3635\n",
      "Graph 39: Epoch: 031, Loss: -0.5687\n",
      "Graph 39: Epoch: 032, Loss: -0.4402\n",
      "Graph 39: Epoch: 033, Loss: -0.2892\n",
      "Graph 39: Epoch: 034, Loss: -0.5067\n",
      "Graph 39: Epoch: 035, Loss: -0.3591\n",
      "Graph 39: Epoch: 036, Loss: -0.3740\n",
      "Graph 39: Epoch: 037, Loss: -0.6731\n",
      "Graph 39: Epoch: 038, Loss: -0.5685\n",
      "Graph 39: Epoch: 039, Loss: -0.4373\n",
      "Graph 39: Epoch: 040, Loss: -0.4411\n",
      "Graph 39: Epoch: 041, Loss: -0.2834\n",
      "Graph 39: Epoch: 042, Loss: -0.3600\n",
      "Graph 39: Epoch: 043, Loss: -0.3643\n",
      "Graph 39: Epoch: 044, Loss: -0.4423\n",
      "Graph 39: Epoch: 045, Loss: -0.2134\n",
      "Graph 39: Epoch: 046, Loss: -0.4415\n",
      "Graph 39: Epoch: 047, Loss: -0.5254\n",
      "Graph 39: Epoch: 048, Loss: -0.6928\n",
      "Graph 39: Epoch: 049, Loss: -0.6075\n",
      "Graph 39: Epoch: 050, Loss: -0.6054\n",
      "Graph 39: Epoch: 051, Loss: -0.3578\n",
      "Graph 39: Epoch: 052, Loss: -0.5300\n",
      "Graph 39: Epoch: 053, Loss: -0.4350\n",
      "Graph 39: Epoch: 054, Loss: -0.5206\n",
      "Graph 39: Epoch: 055, Loss: -0.2766\n",
      "Graph 39: Epoch: 056, Loss: -0.4720\n",
      "Graph 39: Epoch: 057, Loss: -0.5296\n",
      "Graph 39: Epoch: 058, Loss: -0.5256\n",
      "Graph 39: Epoch: 059, Loss: -0.2762\n",
      "Graph 39: Epoch: 060, Loss: -0.4432\n",
      "Graph 39: Epoch: 061, Loss: -0.4437\n",
      "Graph 39: Epoch: 062, Loss: -0.1850\n",
      "Graph 39: Epoch: 063, Loss: -0.6764\n",
      "Graph 39: Epoch: 064, Loss: -0.2740\n",
      "Graph 39: Epoch: 065, Loss: -0.6178\n",
      "Graph 39: Epoch: 066, Loss: -0.2716\n",
      "Graph 39: Epoch: 067, Loss: -0.6252\n",
      "Graph 39: Epoch: 068, Loss: -0.6255\n",
      "Graph 39: Epoch: 069, Loss: -0.4496\n",
      "Graph 39: Epoch: 070, Loss: -0.5429\n",
      "Graph 39: Epoch: 071, Loss: -0.4549\n",
      "Graph 39: Epoch: 072, Loss: -0.5369\n",
      "Graph 39: Epoch: 073, Loss: -0.1877\n",
      "Graph 39: Epoch: 074, Loss: -0.3626\n",
      "Graph 39: Epoch: 075, Loss: -0.5314\n",
      "Graph 39: Epoch: 076, Loss: -0.6371\n",
      "Graph 39: Epoch: 077, Loss: -0.7109\n",
      "Graph 39: Epoch: 078, Loss: -0.5982\n",
      "Graph 39: Epoch: 079, Loss: -0.2726\n",
      "Graph 39: Epoch: 080, Loss: -0.4797\n",
      "Graph 39: Epoch: 081, Loss: -0.6242\n",
      "Graph 39: Epoch: 082, Loss: -0.7118\n",
      "Graph 39: Epoch: 083, Loss: -0.5342\n",
      "Graph 39: Epoch: 084, Loss: -0.6280\n",
      "Graph 39: Epoch: 085, Loss: -0.5402\n",
      "Graph 39: Epoch: 086, Loss: -0.5390\n",
      "Graph 39: Epoch: 087, Loss: -0.3621\n",
      "Graph 39: Epoch: 088, Loss: -0.2714\n",
      "Graph 39: Epoch: 089, Loss: -0.6248\n",
      "Graph 39: Epoch: 090, Loss: -0.7179\n",
      "Graph 39: Epoch: 091, Loss: -0.4501\n",
      "Graph 39: Epoch: 092, Loss: -0.6297\n",
      "Graph 39: Epoch: 093, Loss: -0.7177\n",
      "Graph 39: Epoch: 094, Loss: -0.6299\n",
      "Graph 39: Epoch: 095, Loss: -0.4506\n",
      "Graph 39: Epoch: 096, Loss: -0.6275\n",
      "Graph 39: Epoch: 097, Loss: -0.4540\n",
      "Graph 39: Epoch: 098, Loss: -0.7203\n",
      "Graph 39: Epoch: 099, Loss: -0.3607\n",
      "Graph 39: Epoch: 100, Loss: -0.5405\n",
      "Graph 40: Epoch: 001, Loss: -0.0774\n",
      "Graph 40: Epoch: 002, Loss: -0.1553\n",
      "Graph 40: Epoch: 003, Loss: -0.4032\n",
      "Graph 40: Epoch: 004, Loss: -0.3090\n",
      "Graph 40: Epoch: 005, Loss: -0.4860\n",
      "Graph 40: Epoch: 006, Loss: -0.4209\n",
      "Graph 40: Epoch: 007, Loss: -0.6269\n",
      "Graph 40: Epoch: 008, Loss: -0.2751\n",
      "Graph 40: Epoch: 009, Loss: -0.3885\n",
      "Graph 40: Epoch: 010, Loss: -0.4859\n",
      "Graph 40: Epoch: 011, Loss: -0.5440\n",
      "Graph 40: Epoch: 012, Loss: -0.4840\n",
      "Graph 40: Epoch: 013, Loss: -0.5584\n",
      "Graph 40: Epoch: 014, Loss: -0.4292\n",
      "Graph 40: Epoch: 015, Loss: -0.1360\n",
      "Graph 40: Epoch: 016, Loss: -0.4258\n",
      "Graph 40: Epoch: 017, Loss: -0.4326\n",
      "Graph 40: Epoch: 018, Loss: -0.2192\n",
      "Graph 40: Epoch: 019, Loss: -0.6317\n",
      "Graph 40: Epoch: 020, Loss: -0.4309\n",
      "Graph 40: Epoch: 021, Loss: -0.5850\n",
      "Graph 40: Epoch: 022, Loss: -0.5039\n",
      "Graph 40: Epoch: 023, Loss: -0.2954\n",
      "Graph 40: Epoch: 024, Loss: -0.4436\n",
      "Graph 40: Epoch: 025, Loss: -0.5181\n",
      "Graph 40: Epoch: 026, Loss: -0.6001\n",
      "Graph 40: Epoch: 027, Loss: -0.8155\n",
      "Graph 40: Epoch: 028, Loss: -0.5948\n",
      "Graph 40: Epoch: 029, Loss: -0.3714\n",
      "Graph 40: Epoch: 030, Loss: -0.4477\n",
      "Graph 40: Epoch: 031, Loss: -0.5948\n",
      "Graph 40: Epoch: 032, Loss: -0.4444\n",
      "Graph 40: Epoch: 033, Loss: -0.5243\n",
      "Graph 40: Epoch: 034, Loss: -0.4501\n",
      "Graph 40: Epoch: 035, Loss: -0.3006\n",
      "Graph 40: Epoch: 036, Loss: -0.5248\n",
      "Graph 40: Epoch: 037, Loss: -0.6008\n",
      "Graph 40: Epoch: 038, Loss: -0.4522\n",
      "Graph 40: Epoch: 039, Loss: -0.3019\n",
      "Graph 40: Epoch: 040, Loss: -0.5279\n",
      "Graph 40: Epoch: 041, Loss: -0.4529\n",
      "Graph 40: Epoch: 042, Loss: -0.3778\n",
      "Graph 40: Epoch: 043, Loss: -0.5290\n",
      "Graph 40: Epoch: 044, Loss: -0.6043\n",
      "Graph 40: Epoch: 045, Loss: -0.3031\n",
      "Graph 40: Epoch: 046, Loss: -0.5267\n",
      "Graph 40: Epoch: 047, Loss: -0.8314\n",
      "Graph 40: Epoch: 048, Loss: -0.5300\n",
      "Graph 40: Epoch: 049, Loss: -0.4545\n",
      "Graph 40: Epoch: 050, Loss: -0.3793\n",
      "Graph 40: Epoch: 051, Loss: -0.3790\n",
      "Graph 40: Epoch: 052, Loss: -0.4541\n",
      "Graph 40: Epoch: 053, Loss: -0.6065\n",
      "Graph 40: Epoch: 054, Loss: -0.6052\n",
      "Graph 40: Epoch: 055, Loss: -0.2283\n",
      "Graph 40: Epoch: 056, Loss: -0.4556\n",
      "Graph 40: Epoch: 057, Loss: -0.4527\n",
      "Graph 40: Epoch: 058, Loss: -0.3043\n",
      "Graph 40: Epoch: 059, Loss: -0.6079\n",
      "Graph 40: Epoch: 060, Loss: -0.3803\n",
      "Graph 40: Epoch: 061, Loss: -0.6079\n",
      "Graph 40: Epoch: 062, Loss: -0.3803\n",
      "Graph 40: Epoch: 063, Loss: -0.3046\n",
      "Graph 40: Epoch: 064, Loss: -0.2286\n",
      "Graph 40: Epoch: 065, Loss: -0.4566\n",
      "Graph 40: Epoch: 066, Loss: -0.4565\n",
      "Graph 40: Epoch: 067, Loss: -0.5327\n",
      "Graph 40: Epoch: 068, Loss: -0.5321\n",
      "Graph 40: Epoch: 069, Loss: -0.6089\n",
      "Graph 40: Epoch: 070, Loss: -0.6845\n",
      "Graph 40: Epoch: 071, Loss: -0.5325\n",
      "Graph 40: Epoch: 072, Loss: -0.3049\n",
      "Graph 40: Epoch: 073, Loss: -0.3811\n",
      "Graph 40: Epoch: 074, Loss: -0.3813\n",
      "Graph 40: Epoch: 075, Loss: -0.4573\n",
      "Graph 40: Epoch: 076, Loss: -0.3052\n",
      "Graph 40: Epoch: 077, Loss: -0.4574\n",
      "Graph 40: Epoch: 078, Loss: -0.6097\n",
      "Graph 40: Epoch: 079, Loss: -0.6097\n",
      "Graph 40: Epoch: 080, Loss: -0.3814\n",
      "Graph 40: Epoch: 081, Loss: -0.6860\n",
      "Graph 40: Epoch: 082, Loss: -0.3816\n",
      "Graph 40: Epoch: 083, Loss: -0.7625\n",
      "Graph 40: Epoch: 084, Loss: -0.3816\n",
      "Graph 40: Epoch: 085, Loss: -0.2293\n",
      "Graph 40: Epoch: 086, Loss: -0.6102\n",
      "Graph 40: Epoch: 087, Loss: -0.3053\n",
      "Graph 40: Epoch: 088, Loss: -0.6866\n",
      "Graph 40: Epoch: 089, Loss: -0.3818\n",
      "Graph 40: Epoch: 090, Loss: -0.3056\n",
      "Graph 40: Epoch: 091, Loss: -0.6107\n",
      "Graph 40: Epoch: 092, Loss: -0.3817\n",
      "Graph 40: Epoch: 093, Loss: -0.6104\n",
      "Graph 40: Epoch: 094, Loss: -0.5339\n",
      "Graph 40: Epoch: 095, Loss: -0.3054\n",
      "Graph 40: Epoch: 096, Loss: -0.6108\n",
      "Graph 40: Epoch: 097, Loss: -0.5346\n",
      "Graph 40: Epoch: 098, Loss: -0.4580\n",
      "Graph 40: Epoch: 099, Loss: -0.5348\n",
      "Graph 40: Epoch: 100, Loss: -0.4582\n",
      "Graph 41: Epoch: 001, Loss: -0.0237\n",
      "Graph 41: Epoch: 002, Loss: -0.0268\n",
      "Graph 41: Epoch: 003, Loss: -0.0510\n",
      "Graph 41: Epoch: 004, Loss: -0.0749\n",
      "Graph 41: Epoch: 005, Loss: -0.1689\n",
      "Graph 41: Epoch: 006, Loss: -0.0960\n",
      "Graph 41: Epoch: 007, Loss: -0.0966\n",
      "Graph 41: Epoch: 008, Loss: -0.3987\n",
      "Graph 41: Epoch: 009, Loss: -0.4741\n",
      "Graph 41: Epoch: 010, Loss: -0.2066\n",
      "Graph 41: Epoch: 011, Loss: -0.2009\n",
      "Graph 41: Epoch: 012, Loss: -0.2502\n",
      "Graph 41: Epoch: 013, Loss: -0.2710\n",
      "Graph 41: Epoch: 014, Loss: -0.3391\n",
      "Graph 41: Epoch: 015, Loss: -0.6412\n",
      "Graph 41: Epoch: 016, Loss: -0.4132\n",
      "Graph 41: Epoch: 017, Loss: -0.2416\n",
      "Graph 41: Epoch: 018, Loss: -0.4038\n",
      "Graph 41: Epoch: 019, Loss: -0.0924\n",
      "Graph 41: Epoch: 020, Loss: -0.2604\n",
      "Graph 41: Epoch: 021, Loss: -0.2045\n",
      "Graph 41: Epoch: 022, Loss: -0.2876\n",
      "Graph 41: Epoch: 023, Loss: -0.3457\n",
      "Graph 41: Epoch: 024, Loss: -0.3246\n",
      "Graph 41: Epoch: 025, Loss: -0.6204\n",
      "Graph 41: Epoch: 026, Loss: -0.4352\n",
      "Graph 41: Epoch: 027, Loss: -0.4132\n",
      "Graph 41: Epoch: 028, Loss: -0.2643\n",
      "Graph 41: Epoch: 029, Loss: -0.2883\n",
      "Graph 41: Epoch: 030, Loss: -0.5590\n",
      "Graph 41: Epoch: 031, Loss: -0.6471\n",
      "Graph 41: Epoch: 032, Loss: -0.2495\n",
      "Graph 41: Epoch: 033, Loss: -0.3086\n",
      "Graph 41: Epoch: 034, Loss: -0.3693\n",
      "Graph 41: Epoch: 035, Loss: -0.4460\n",
      "Graph 41: Epoch: 036, Loss: -0.4229\n",
      "Graph 41: Epoch: 037, Loss: -0.8363\n",
      "Graph 41: Epoch: 038, Loss: -0.5140\n",
      "Graph 41: Epoch: 039, Loss: -0.1851\n",
      "Graph 41: Epoch: 040, Loss: -0.3090\n",
      "Graph 41: Epoch: 041, Loss: -0.5035\n",
      "Graph 41: Epoch: 042, Loss: -0.6654\n",
      "Graph 41: Epoch: 043, Loss: -0.6136\n",
      "Graph 41: Epoch: 044, Loss: -0.4711\n",
      "Graph 41: Epoch: 045, Loss: -0.3443\n",
      "Graph 41: Epoch: 046, Loss: -0.4790\n",
      "Graph 41: Epoch: 047, Loss: -0.4051\n",
      "Graph 41: Epoch: 048, Loss: -0.6451\n",
      "Graph 41: Epoch: 049, Loss: -0.4768\n",
      "Graph 41: Epoch: 050, Loss: -0.3626\n",
      "Graph 41: Epoch: 051, Loss: -0.1484\n",
      "Graph 41: Epoch: 052, Loss: -0.3564\n",
      "Graph 41: Epoch: 053, Loss: -0.4726\n",
      "Graph 41: Epoch: 054, Loss: -0.6584\n",
      "Graph 41: Epoch: 055, Loss: -0.8381\n",
      "Graph 41: Epoch: 056, Loss: -0.6791\n",
      "Graph 41: Epoch: 057, Loss: -0.1406\n",
      "Graph 41: Epoch: 058, Loss: -0.7050\n",
      "Graph 41: Epoch: 059, Loss: -0.3482\n",
      "Graph 41: Epoch: 060, Loss: -0.3128\n",
      "Graph 41: Epoch: 061, Loss: -0.5369\n",
      "Graph 41: Epoch: 062, Loss: -0.4766\n",
      "Graph 41: Epoch: 063, Loss: -0.3976\n",
      "Graph 41: Epoch: 064, Loss: -0.2298\n",
      "Graph 41: Epoch: 065, Loss: -0.4783\n",
      "Graph 41: Epoch: 066, Loss: -0.5470\n",
      "Graph 41: Epoch: 067, Loss: -0.3628\n",
      "Graph 41: Epoch: 068, Loss: -0.3618\n",
      "Graph 41: Epoch: 069, Loss: -0.7128\n",
      "Graph 41: Epoch: 070, Loss: -0.4425\n",
      "Graph 41: Epoch: 071, Loss: -0.4819\n",
      "Graph 41: Epoch: 072, Loss: -0.9476\n",
      "Graph 41: Epoch: 073, Loss: -0.7557\n",
      "Graph 41: Epoch: 074, Loss: -0.2500\n",
      "Graph 41: Epoch: 075, Loss: -0.3770\n",
      "Graph 41: Epoch: 076, Loss: -0.6125\n",
      "Graph 41: Epoch: 077, Loss: -0.5489\n",
      "Graph 41: Epoch: 078, Loss: -0.4041\n",
      "Graph 41: Epoch: 079, Loss: -0.4791\n",
      "Graph 41: Epoch: 080, Loss: -0.4208\n",
      "Graph 41: Epoch: 081, Loss: -0.2638\n",
      "Graph 41: Epoch: 082, Loss: -0.1486\n",
      "Graph 41: Epoch: 083, Loss: -0.2450\n",
      "Graph 41: Epoch: 084, Loss: -0.9301\n",
      "Graph 41: Epoch: 085, Loss: -0.6531\n",
      "Graph 41: Epoch: 086, Loss: -0.3324\n",
      "Graph 41: Epoch: 087, Loss: -0.4142\n",
      "Graph 41: Epoch: 088, Loss: -0.0425\n",
      "Graph 41: Epoch: 089, Loss: -0.5894\n",
      "Graph 41: Epoch: 090, Loss: -0.3769\n",
      "Graph 41: Epoch: 091, Loss: -0.0734\n",
      "Graph 41: Epoch: 092, Loss: -0.8270\n",
      "Graph 41: Epoch: 093, Loss: -0.5429\n",
      "Graph 41: Epoch: 094, Loss: -0.5762\n",
      "Graph 41: Epoch: 095, Loss: -0.3582\n",
      "Graph 41: Epoch: 096, Loss: -0.8392\n",
      "Graph 41: Epoch: 097, Loss: -0.4502\n",
      "Graph 41: Epoch: 098, Loss: -0.3238\n",
      "Graph 41: Epoch: 099, Loss: -0.1104\n",
      "Graph 41: Epoch: 100, Loss: -0.2811\n",
      "Graph 42: Epoch: 001, Loss: -0.1007\n",
      "Graph 42: Epoch: 002, Loss: -0.1719\n",
      "Graph 42: Epoch: 003, Loss: -0.2622\n",
      "Graph 42: Epoch: 004, Loss: -0.2915\n",
      "Graph 42: Epoch: 005, Loss: -0.3442\n",
      "Graph 42: Epoch: 006, Loss: -0.3709\n",
      "Graph 42: Epoch: 007, Loss: -0.3879\n",
      "Graph 42: Epoch: 008, Loss: -0.5158\n",
      "Graph 42: Epoch: 009, Loss: -0.3397\n",
      "Graph 42: Epoch: 010, Loss: -0.3713\n",
      "Graph 42: Epoch: 011, Loss: -0.4968\n",
      "Graph 42: Epoch: 012, Loss: -0.3527\n",
      "Graph 42: Epoch: 013, Loss: -0.4027\n",
      "Graph 42: Epoch: 014, Loss: -0.2941\n",
      "Graph 42: Epoch: 015, Loss: -0.3949\n",
      "Graph 42: Epoch: 016, Loss: -0.6031\n",
      "Graph 42: Epoch: 017, Loss: -0.4196\n",
      "Graph 42: Epoch: 018, Loss: -0.5031\n",
      "Graph 42: Epoch: 019, Loss: -0.3632\n",
      "Graph 42: Epoch: 020, Loss: -0.6448\n",
      "Graph 42: Epoch: 021, Loss: -0.3647\n",
      "Graph 42: Epoch: 022, Loss: -0.5765\n",
      "Graph 42: Epoch: 023, Loss: -0.5840\n",
      "Graph 42: Epoch: 024, Loss: -0.5130\n",
      "Graph 42: Epoch: 025, Loss: -0.6621\n",
      "Graph 42: Epoch: 026, Loss: -0.1536\n",
      "Graph 42: Epoch: 027, Loss: -0.5177\n",
      "Graph 42: Epoch: 028, Loss: -0.2263\n",
      "Graph 42: Epoch: 029, Loss: -0.3001\n",
      "Graph 42: Epoch: 030, Loss: -0.3741\n",
      "Graph 42: Epoch: 031, Loss: -0.5219\n",
      "Graph 42: Epoch: 032, Loss: -0.4489\n",
      "Graph 42: Epoch: 033, Loss: -0.3710\n",
      "Graph 42: Epoch: 034, Loss: -0.3003\n",
      "Graph 42: Epoch: 035, Loss: -0.5959\n",
      "Graph 42: Epoch: 036, Loss: -0.3754\n",
      "Graph 42: Epoch: 037, Loss: -0.2969\n",
      "Graph 42: Epoch: 038, Loss: -0.6747\n",
      "Graph 42: Epoch: 039, Loss: -0.4508\n",
      "Graph 42: Epoch: 040, Loss: -0.5998\n",
      "Graph 42: Epoch: 041, Loss: -0.5267\n",
      "Graph 42: Epoch: 042, Loss: -0.1522\n",
      "Graph 42: Epoch: 043, Loss: -0.3772\n",
      "Graph 42: Epoch: 044, Loss: -0.4533\n",
      "Graph 42: Epoch: 045, Loss: -0.4527\n",
      "Graph 42: Epoch: 046, Loss: -0.3034\n",
      "Graph 42: Epoch: 047, Loss: -0.6029\n",
      "Graph 42: Epoch: 048, Loss: -0.4535\n",
      "Graph 42: Epoch: 049, Loss: -0.5294\n",
      "Graph 42: Epoch: 050, Loss: -0.7535\n",
      "Graph 42: Epoch: 051, Loss: -0.2276\n",
      "Graph 42: Epoch: 052, Loss: -0.3783\n",
      "Graph 42: Epoch: 053, Loss: -0.4598\n",
      "Graph 42: Epoch: 054, Loss: -0.6052\n",
      "Graph 42: Epoch: 055, Loss: -0.5314\n",
      "Graph 42: Epoch: 056, Loss: -0.5303\n",
      "Graph 42: Epoch: 057, Loss: -0.6814\n",
      "Graph 42: Epoch: 058, Loss: -0.5306\n",
      "Graph 42: Epoch: 059, Loss: -0.7572\n",
      "Graph 42: Epoch: 060, Loss: -0.6817\n",
      "Graph 42: Epoch: 061, Loss: -0.4547\n",
      "Graph 42: Epoch: 062, Loss: -0.3797\n",
      "Graph 42: Epoch: 063, Loss: -0.3794\n",
      "Graph 42: Epoch: 064, Loss: -0.5312\n",
      "Graph 42: Epoch: 065, Loss: -0.8328\n",
      "Graph 42: Epoch: 066, Loss: -0.4557\n",
      "Graph 42: Epoch: 067, Loss: -0.5317\n",
      "Graph 42: Epoch: 068, Loss: -0.4562\n",
      "Graph 42: Epoch: 069, Loss: -0.6069\n",
      "Graph 42: Epoch: 070, Loss: -0.6081\n",
      "Graph 42: Epoch: 071, Loss: -0.3049\n",
      "Graph 42: Epoch: 072, Loss: -0.3047\n",
      "Graph 42: Epoch: 073, Loss: -0.6084\n",
      "Graph 42: Epoch: 074, Loss: -0.6822\n",
      "Graph 42: Epoch: 075, Loss: -0.5328\n",
      "Graph 42: Epoch: 076, Loss: -0.3049\n",
      "Graph 42: Epoch: 077, Loss: -0.2291\n",
      "Graph 42: Epoch: 078, Loss: -0.4568\n",
      "Graph 42: Epoch: 079, Loss: -0.3811\n",
      "Graph 42: Epoch: 080, Loss: -0.6848\n",
      "Graph 42: Epoch: 081, Loss: -0.3810\n",
      "Graph 42: Epoch: 082, Loss: -0.5328\n",
      "Graph 42: Epoch: 083, Loss: -0.6854\n",
      "Graph 42: Epoch: 084, Loss: -0.3806\n",
      "Graph 42: Epoch: 085, Loss: -0.5333\n",
      "Graph 42: Epoch: 086, Loss: -0.3052\n",
      "Graph 42: Epoch: 087, Loss: -0.6096\n",
      "Graph 42: Epoch: 088, Loss: -0.6090\n",
      "Graph 42: Epoch: 089, Loss: -0.6859\n",
      "Graph 42: Epoch: 090, Loss: -0.3053\n",
      "Graph 42: Epoch: 091, Loss: -0.4578\n",
      "Graph 42: Epoch: 092, Loss: -0.4574\n",
      "Graph 42: Epoch: 093, Loss: -0.5339\n",
      "Graph 42: Epoch: 094, Loss: -0.4581\n",
      "Graph 42: Epoch: 095, Loss: -0.2293\n",
      "Graph 42: Epoch: 096, Loss: -0.6103\n",
      "Graph 42: Epoch: 097, Loss: -0.5336\n",
      "Graph 42: Epoch: 098, Loss: -0.0769\n",
      "Graph 42: Epoch: 099, Loss: -0.4581\n",
      "Graph 42: Epoch: 100, Loss: -0.6862\n",
      "Graph 43: Epoch: 001, Loss: -0.0385\n",
      "Graph 43: Epoch: 002, Loss: -0.2082\n",
      "Graph 43: Epoch: 003, Loss: -0.3325\n",
      "Graph 43: Epoch: 004, Loss: -0.3186\n",
      "Graph 43: Epoch: 005, Loss: -0.3698\n",
      "Graph 43: Epoch: 006, Loss: -0.3064\n",
      "Graph 43: Epoch: 007, Loss: -0.3688\n",
      "Graph 43: Epoch: 008, Loss: -0.4258\n",
      "Graph 43: Epoch: 009, Loss: -0.3277\n",
      "Graph 43: Epoch: 010, Loss: -0.6181\n",
      "Graph 43: Epoch: 011, Loss: -0.4516\n",
      "Graph 43: Epoch: 012, Loss: -0.6153\n",
      "Graph 43: Epoch: 013, Loss: -0.2858\n",
      "Graph 43: Epoch: 014, Loss: -0.4761\n",
      "Graph 43: Epoch: 015, Loss: -0.5880\n",
      "Graph 43: Epoch: 016, Loss: -0.2902\n",
      "Graph 43: Epoch: 017, Loss: -0.6388\n",
      "Graph 43: Epoch: 018, Loss: -0.4801\n",
      "Graph 43: Epoch: 019, Loss: -0.3615\n",
      "Graph 43: Epoch: 020, Loss: -0.4211\n",
      "Graph 43: Epoch: 021, Loss: -0.2973\n",
      "Graph 43: Epoch: 022, Loss: -0.3633\n",
      "Graph 43: Epoch: 023, Loss: -0.5458\n",
      "Graph 43: Epoch: 024, Loss: -0.4839\n",
      "Graph 43: Epoch: 025, Loss: -0.4245\n",
      "Graph 43: Epoch: 026, Loss: -0.2443\n",
      "Graph 43: Epoch: 027, Loss: -0.6720\n",
      "Graph 43: Epoch: 028, Loss: -0.4892\n",
      "Graph 43: Epoch: 029, Loss: -0.4291\n",
      "Graph 43: Epoch: 030, Loss: -0.6123\n",
      "Graph 43: Epoch: 031, Loss: -0.3680\n",
      "Graph 43: Epoch: 032, Loss: -0.4905\n",
      "Graph 43: Epoch: 033, Loss: -0.4294\n",
      "Graph 43: Epoch: 034, Loss: -0.5497\n",
      "Graph 43: Epoch: 035, Loss: -0.6142\n",
      "Graph 43: Epoch: 036, Loss: -0.6758\n",
      "Graph 43: Epoch: 037, Loss: -0.5537\n",
      "Graph 43: Epoch: 038, Loss: -0.4307\n",
      "Graph 43: Epoch: 039, Loss: -0.6155\n",
      "Graph 43: Epoch: 040, Loss: -0.4314\n",
      "Graph 43: Epoch: 041, Loss: -0.3085\n",
      "Graph 43: Epoch: 042, Loss: -0.6162\n",
      "Graph 43: Epoch: 043, Loss: -0.4938\n",
      "Graph 43: Epoch: 044, Loss: -0.7404\n",
      "Graph 43: Epoch: 045, Loss: -0.4321\n",
      "Graph 43: Epoch: 046, Loss: -0.6177\n",
      "Graph 43: Epoch: 047, Loss: -0.4941\n",
      "Graph 43: Epoch: 048, Loss: -0.4327\n",
      "Graph 43: Epoch: 049, Loss: -0.6180\n",
      "Graph 43: Epoch: 050, Loss: -0.5559\n",
      "Graph 43: Epoch: 051, Loss: -0.5568\n",
      "Graph 43: Epoch: 052, Loss: -0.4330\n",
      "Graph 43: Epoch: 053, Loss: -0.4948\n",
      "Graph 43: Epoch: 054, Loss: -0.3714\n",
      "Graph 43: Epoch: 055, Loss: -0.3095\n",
      "Graph 43: Epoch: 056, Loss: -0.3094\n",
      "Graph 43: Epoch: 057, Loss: -0.4954\n",
      "Graph 43: Epoch: 058, Loss: -0.3097\n",
      "Graph 43: Epoch: 059, Loss: -0.5574\n",
      "Graph 43: Epoch: 060, Loss: -0.5573\n",
      "Graph 43: Epoch: 061, Loss: -0.7431\n",
      "Graph 43: Epoch: 062, Loss: -0.6195\n",
      "Graph 43: Epoch: 063, Loss: -0.4338\n",
      "Graph 43: Epoch: 064, Loss: -0.6197\n",
      "Graph 43: Epoch: 065, Loss: -0.6817\n",
      "Graph 43: Epoch: 066, Loss: -0.3721\n",
      "Graph 43: Epoch: 067, Loss: -0.3100\n",
      "Graph 43: Epoch: 068, Loss: -0.4341\n",
      "Graph 43: Epoch: 069, Loss: -0.3721\n",
      "Graph 43: Epoch: 070, Loss: -0.4342\n",
      "Graph 43: Epoch: 071, Loss: -0.6191\n",
      "Graph 43: Epoch: 072, Loss: -0.3723\n",
      "Graph 43: Epoch: 073, Loss: -0.3103\n",
      "Graph 43: Epoch: 074, Loss: -0.5585\n",
      "Graph 43: Epoch: 075, Loss: -0.6204\n",
      "Graph 43: Epoch: 076, Loss: -0.3104\n",
      "Graph 43: Epoch: 077, Loss: -0.4346\n",
      "Graph 43: Epoch: 078, Loss: -0.6206\n",
      "Graph 43: Epoch: 079, Loss: -0.2485\n",
      "Graph 43: Epoch: 080, Loss: -0.3106\n",
      "Graph 43: Epoch: 081, Loss: -0.5587\n",
      "Graph 43: Epoch: 082, Loss: -0.4968\n",
      "Graph 43: Epoch: 083, Loss: -0.4968\n",
      "Graph 43: Epoch: 084, Loss: -0.5590\n",
      "Graph 43: Epoch: 085, Loss: -0.5590\n",
      "Graph 43: Epoch: 086, Loss: -0.4969\n",
      "Graph 43: Epoch: 087, Loss: -0.3727\n",
      "Graph 43: Epoch: 088, Loss: -0.4349\n",
      "Graph 43: Epoch: 089, Loss: -0.5591\n",
      "Graph 43: Epoch: 090, Loss: -0.5592\n",
      "Graph 43: Epoch: 091, Loss: -0.4972\n",
      "Graph 43: Epoch: 092, Loss: -0.6835\n",
      "Graph 43: Epoch: 093, Loss: -0.5593\n",
      "Graph 43: Epoch: 094, Loss: -0.4351\n",
      "Graph 43: Epoch: 095, Loss: -0.4351\n",
      "Graph 43: Epoch: 096, Loss: -0.4351\n",
      "Graph 43: Epoch: 097, Loss: -0.5595\n",
      "Graph 43: Epoch: 098, Loss: -0.6835\n",
      "Graph 43: Epoch: 099, Loss: -0.5596\n",
      "Graph 43: Epoch: 100, Loss: -0.3732\n",
      "Graph 44: Epoch: 001, Loss: -0.0705\n",
      "Graph 44: Epoch: 002, Loss: -0.5665\n",
      "Graph 44: Epoch: 003, Loss: -0.5345\n",
      "Graph 44: Epoch: 004, Loss: -0.4349\n",
      "Graph 44: Epoch: 005, Loss: -0.2681\n",
      "Graph 44: Epoch: 006, Loss: -0.0716\n",
      "Graph 44: Epoch: 007, Loss: -0.2579\n",
      "Graph 44: Epoch: 008, Loss: -0.6929\n",
      "Graph 44: Epoch: 009, Loss: -0.4091\n",
      "Graph 44: Epoch: 010, Loss: -0.4480\n",
      "Graph 44: Epoch: 011, Loss: -0.4664\n",
      "Graph 44: Epoch: 012, Loss: -0.4213\n",
      "Graph 44: Epoch: 013, Loss: -0.5871\n",
      "Graph 44: Epoch: 014, Loss: -0.5255\n",
      "Graph 44: Epoch: 015, Loss: -0.5225\n",
      "Graph 44: Epoch: 016, Loss: -0.1256\n",
      "Graph 44: Epoch: 017, Loss: -0.8050\n",
      "Graph 44: Epoch: 018, Loss: -0.1735\n",
      "Graph 44: Epoch: 019, Loss: -0.6237\n",
      "Graph 44: Epoch: 020, Loss: -0.5465\n",
      "Graph 44: Epoch: 021, Loss: -0.2788\n",
      "Graph 44: Epoch: 022, Loss: -0.2665\n",
      "Graph 44: Epoch: 023, Loss: -0.5046\n",
      "Graph 44: Epoch: 024, Loss: -0.8204\n",
      "Graph 44: Epoch: 025, Loss: -0.4354\n",
      "Graph 44: Epoch: 026, Loss: -0.2803\n",
      "Graph 44: Epoch: 027, Loss: -0.3635\n",
      "Graph 44: Epoch: 028, Loss: -0.6685\n",
      "Graph 44: Epoch: 029, Loss: -0.2689\n",
      "Graph 44: Epoch: 030, Loss: -0.5433\n",
      "Graph 44: Epoch: 031, Loss: -0.2558\n",
      "Graph 44: Epoch: 032, Loss: -0.4636\n",
      "Graph 44: Epoch: 033, Loss: -0.6732\n",
      "Graph 44: Epoch: 034, Loss: -0.8449\n",
      "Graph 44: Epoch: 035, Loss: -0.4647\n",
      "Graph 44: Epoch: 036, Loss: -0.4702\n",
      "Graph 44: Epoch: 037, Loss: -0.2657\n",
      "Graph 44: Epoch: 038, Loss: -0.1876\n",
      "Graph 44: Epoch: 039, Loss: -0.3651\n",
      "Graph 44: Epoch: 040, Loss: -0.3038\n",
      "Graph 44: Epoch: 041, Loss: -0.8425\n",
      "Graph 44: Epoch: 042, Loss: -0.2960\n",
      "Graph 44: Epoch: 043, Loss: -0.4087\n",
      "Graph 44: Epoch: 044, Loss: -0.5047\n",
      "Graph 44: Epoch: 045, Loss: -0.4723\n",
      "Graph 44: Epoch: 046, Loss: -0.4618\n",
      "Graph 44: Epoch: 047, Loss: -0.8592\n",
      "Graph 44: Epoch: 048, Loss: -0.6395\n",
      "Graph 44: Epoch: 049, Loss: -0.3712\n",
      "Graph 44: Epoch: 050, Loss: -0.0779\n",
      "Graph 44: Epoch: 051, Loss: -0.2457\n",
      "Graph 44: Epoch: 052, Loss: -0.4773\n",
      "Graph 44: Epoch: 053, Loss: -0.5996\n",
      "Graph 44: Epoch: 054, Loss: -0.6603\n",
      "Graph 44: Epoch: 055, Loss: -0.5553\n",
      "Graph 44: Epoch: 056, Loss: -0.2503\n",
      "Graph 44: Epoch: 057, Loss: -0.3219\n",
      "Graph 44: Epoch: 058, Loss: -0.6607\n",
      "Graph 44: Epoch: 059, Loss: -0.4767\n",
      "Graph 44: Epoch: 060, Loss: -0.1882\n",
      "Graph 44: Epoch: 061, Loss: -0.5823\n",
      "Graph 44: Epoch: 062, Loss: -0.6740\n",
      "Graph 44: Epoch: 063, Loss: -0.5568\n",
      "Graph 44: Epoch: 064, Loss: -0.3017\n",
      "Graph 44: Epoch: 065, Loss: -0.4614\n",
      "Graph 44: Epoch: 066, Loss: -0.4659\n",
      "Graph 44: Epoch: 067, Loss: -0.5934\n",
      "Graph 44: Epoch: 068, Loss: -0.4159\n",
      "Graph 44: Epoch: 069, Loss: -0.4765\n",
      "Graph 44: Epoch: 070, Loss: -0.6783\n",
      "Graph 44: Epoch: 071, Loss: -0.2617\n",
      "Graph 44: Epoch: 072, Loss: -0.1099\n",
      "Graph 44: Epoch: 073, Loss: -0.0769\n",
      "Graph 44: Epoch: 074, Loss: -0.2908\n",
      "Graph 44: Epoch: 075, Loss: -0.4181\n",
      "Graph 44: Epoch: 076, Loss: -0.4881\n",
      "Graph 44: Epoch: 077, Loss: -0.5728\n",
      "Graph 44: Epoch: 078, Loss: -0.1569\n",
      "Graph 44: Epoch: 079, Loss: -0.4989\n",
      "Graph 44: Epoch: 080, Loss: -0.4121\n",
      "Graph 44: Epoch: 081, Loss: -0.3495\n",
      "Graph 44: Epoch: 082, Loss: -0.4852\n",
      "Graph 44: Epoch: 083, Loss: -0.4565\n",
      "Graph 44: Epoch: 084, Loss: -0.4765\n",
      "Graph 44: Epoch: 085, Loss: -0.6502\n",
      "Graph 44: Epoch: 086, Loss: -0.6182\n",
      "Graph 44: Epoch: 087, Loss: -0.4163\n",
      "Graph 44: Epoch: 088, Loss: -0.1354\n",
      "Graph 44: Epoch: 089, Loss: -0.5527\n",
      "Graph 44: Epoch: 090, Loss: -0.3569\n",
      "Graph 44: Epoch: 091, Loss: -0.6872\n",
      "Graph 44: Epoch: 092, Loss: -0.2144\n",
      "Graph 44: Epoch: 093, Loss: -0.6123\n",
      "Graph 44: Epoch: 094, Loss: -0.4201\n",
      "Graph 44: Epoch: 095, Loss: -0.4795\n",
      "Graph 44: Epoch: 096, Loss: -0.3384\n",
      "Graph 44: Epoch: 097, Loss: -0.5973\n",
      "Graph 44: Epoch: 098, Loss: -0.4340\n",
      "Graph 44: Epoch: 099, Loss: -0.2808\n",
      "Graph 44: Epoch: 100, Loss: -0.1554\n",
      "Graph 45: Epoch: 001, Loss: -0.0117\n",
      "Graph 45: Epoch: 002, Loss: -0.0561\n",
      "Graph 45: Epoch: 003, Loss: -0.1811\n",
      "Graph 45: Epoch: 004, Loss: -0.2479\n",
      "Graph 45: Epoch: 005, Loss: -0.2674\n",
      "Graph 45: Epoch: 006, Loss: -0.3615\n",
      "Graph 45: Epoch: 007, Loss: -0.3801\n",
      "Graph 45: Epoch: 008, Loss: -0.3710\n",
      "Graph 45: Epoch: 009, Loss: -0.2901\n",
      "Graph 45: Epoch: 010, Loss: -0.3436\n",
      "Graph 45: Epoch: 011, Loss: -0.2992\n",
      "Graph 45: Epoch: 012, Loss: -0.6061\n",
      "Graph 45: Epoch: 013, Loss: -0.4279\n",
      "Graph 45: Epoch: 014, Loss: -0.7371\n",
      "Graph 45: Epoch: 015, Loss: -0.5226\n",
      "Graph 45: Epoch: 016, Loss: -0.6052\n",
      "Graph 45: Epoch: 017, Loss: -0.3859\n",
      "Graph 45: Epoch: 018, Loss: -0.6160\n",
      "Graph 45: Epoch: 019, Loss: -0.4287\n",
      "Graph 45: Epoch: 020, Loss: -0.3815\n",
      "Graph 45: Epoch: 021, Loss: -0.3090\n",
      "Graph 45: Epoch: 022, Loss: -0.3145\n",
      "Graph 45: Epoch: 023, Loss: -0.2360\n",
      "Graph 45: Epoch: 024, Loss: -0.3966\n",
      "Graph 45: Epoch: 025, Loss: -0.3989\n",
      "Graph 45: Epoch: 026, Loss: -0.7079\n",
      "Graph 45: Epoch: 027, Loss: -0.5442\n",
      "Graph 45: Epoch: 028, Loss: -0.5601\n",
      "Graph 45: Epoch: 029, Loss: -0.4732\n",
      "Graph 45: Epoch: 030, Loss: -0.5604\n",
      "Graph 45: Epoch: 031, Loss: -0.6446\n",
      "Graph 45: Epoch: 032, Loss: -0.3237\n",
      "Graph 45: Epoch: 033, Loss: -0.4052\n",
      "Graph 45: Epoch: 034, Loss: -0.4053\n",
      "Graph 45: Epoch: 035, Loss: -0.7240\n",
      "Graph 45: Epoch: 036, Loss: -0.4047\n",
      "Graph 45: Epoch: 037, Loss: -0.4864\n",
      "Graph 45: Epoch: 038, Loss: -0.3254\n",
      "Graph 45: Epoch: 039, Loss: -0.4068\n",
      "Graph 45: Epoch: 040, Loss: -0.5676\n",
      "Graph 45: Epoch: 041, Loss: -0.6478\n",
      "Graph 45: Epoch: 042, Loss: -0.4875\n",
      "Graph 45: Epoch: 043, Loss: -0.3269\n",
      "Graph 45: Epoch: 044, Loss: -0.4902\n",
      "Graph 45: Epoch: 045, Loss: -0.4884\n",
      "Graph 45: Epoch: 046, Loss: -0.5686\n",
      "Graph 45: Epoch: 047, Loss: -0.2467\n",
      "Graph 45: Epoch: 048, Loss: -0.4092\n",
      "Graph 45: Epoch: 049, Loss: -0.5710\n",
      "Graph 45: Epoch: 050, Loss: -0.4912\n",
      "Graph 45: Epoch: 051, Loss: -0.4102\n",
      "Graph 45: Epoch: 052, Loss: -0.4102\n",
      "Graph 45: Epoch: 053, Loss: -0.2470\n",
      "Graph 45: Epoch: 054, Loss: -0.4104\n",
      "Graph 45: Epoch: 055, Loss: -0.4105\n",
      "Graph 45: Epoch: 056, Loss: -0.5745\n",
      "Graph 45: Epoch: 057, Loss: -0.4103\n",
      "Graph 45: Epoch: 058, Loss: -0.4925\n",
      "Graph 45: Epoch: 059, Loss: -0.4921\n",
      "Graph 45: Epoch: 060, Loss: -0.6677\n",
      "Graph 45: Epoch: 061, Loss: -0.7389\n",
      "Graph 45: Epoch: 062, Loss: -0.4110\n",
      "Graph 45: Epoch: 063, Loss: -0.2473\n",
      "Graph 45: Epoch: 064, Loss: -0.4103\n",
      "Graph 45: Epoch: 065, Loss: -0.4107\n",
      "Graph 45: Epoch: 066, Loss: -0.4113\n",
      "Graph 45: Epoch: 067, Loss: -0.3300\n",
      "Graph 45: Epoch: 068, Loss: -0.4929\n",
      "Graph 45: Epoch: 069, Loss: -0.4105\n",
      "Graph 45: Epoch: 070, Loss: -0.4938\n",
      "Graph 45: Epoch: 071, Loss: -0.4113\n",
      "Graph 45: Epoch: 072, Loss: -0.2470\n",
      "Graph 45: Epoch: 073, Loss: -0.5760\n",
      "Graph 45: Epoch: 074, Loss: -0.3300\n",
      "Graph 45: Epoch: 075, Loss: -0.7397\n",
      "Graph 45: Epoch: 076, Loss: -0.2488\n",
      "Graph 45: Epoch: 077, Loss: -0.5764\n",
      "Graph 45: Epoch: 078, Loss: -0.4922\n",
      "Graph 45: Epoch: 079, Loss: -0.5766\n",
      "Graph 45: Epoch: 080, Loss: -0.4119\n",
      "Graph 45: Epoch: 081, Loss: -0.6591\n",
      "Graph 45: Epoch: 082, Loss: -0.5770\n",
      "Graph 45: Epoch: 083, Loss: -0.6598\n",
      "Graph 45: Epoch: 084, Loss: -0.5770\n",
      "Graph 45: Epoch: 085, Loss: -0.2480\n",
      "Graph 45: Epoch: 086, Loss: -0.4116\n",
      "Graph 45: Epoch: 087, Loss: -0.4129\n",
      "Graph 45: Epoch: 088, Loss: -0.5777\n",
      "Graph 45: Epoch: 089, Loss: -0.2839\n",
      "Graph 45: Epoch: 090, Loss: -0.5778\n",
      "Graph 45: Epoch: 091, Loss: -0.6592\n",
      "Graph 45: Epoch: 092, Loss: -0.2482\n",
      "Graph 45: Epoch: 093, Loss: -0.7422\n",
      "Graph 45: Epoch: 094, Loss: -0.6519\n",
      "Graph 45: Epoch: 095, Loss: -0.4950\n",
      "Graph 45: Epoch: 096, Loss: -0.4940\n",
      "Graph 45: Epoch: 097, Loss: -0.3302\n",
      "Graph 45: Epoch: 098, Loss: -0.4124\n",
      "Graph 45: Epoch: 099, Loss: -0.4118\n",
      "Graph 45: Epoch: 100, Loss: -0.4938\n",
      "Graph 46: Epoch: 001, Loss: -0.0688\n",
      "Graph 46: Epoch: 002, Loss: -0.0591\n",
      "Graph 46: Epoch: 003, Loss: -0.1348\n",
      "Graph 46: Epoch: 004, Loss: -0.3210\n",
      "Graph 46: Epoch: 005, Loss: -0.3236\n",
      "Graph 46: Epoch: 006, Loss: -0.4148\n",
      "Graph 46: Epoch: 007, Loss: -0.3596\n",
      "Graph 46: Epoch: 008, Loss: -0.2878\n",
      "Graph 46: Epoch: 009, Loss: -0.2738\n",
      "Graph 46: Epoch: 010, Loss: -0.4613\n",
      "Graph 46: Epoch: 011, Loss: -0.3552\n",
      "Graph 46: Epoch: 012, Loss: -0.5095\n",
      "Graph 46: Epoch: 013, Loss: -0.4777\n",
      "Graph 46: Epoch: 014, Loss: -0.4716\n",
      "Graph 46: Epoch: 015, Loss: -0.5588\n",
      "Graph 46: Epoch: 016, Loss: -0.5425\n",
      "Graph 46: Epoch: 017, Loss: -0.2627\n",
      "Graph 46: Epoch: 018, Loss: -0.4081\n",
      "Graph 46: Epoch: 019, Loss: -0.3267\n",
      "Graph 46: Epoch: 020, Loss: -0.7186\n",
      "Graph 46: Epoch: 021, Loss: -0.7039\n",
      "Graph 46: Epoch: 022, Loss: -0.6130\n",
      "Graph 46: Epoch: 023, Loss: -0.5880\n",
      "Graph 46: Epoch: 024, Loss: -0.3986\n",
      "Graph 46: Epoch: 025, Loss: -0.3177\n",
      "Graph 46: Epoch: 026, Loss: -0.4181\n",
      "Graph 46: Epoch: 027, Loss: -0.5213\n",
      "Graph 46: Epoch: 028, Loss: -0.3867\n",
      "Graph 46: Epoch: 029, Loss: -0.6258\n",
      "Graph 46: Epoch: 030, Loss: -0.3188\n",
      "Graph 46: Epoch: 031, Loss: -0.6361\n",
      "Graph 46: Epoch: 032, Loss: -0.3203\n",
      "Graph 46: Epoch: 033, Loss: -0.6341\n",
      "Graph 46: Epoch: 034, Loss: -0.3205\n",
      "Graph 46: Epoch: 035, Loss: -0.6336\n",
      "Graph 46: Epoch: 036, Loss: -0.6416\n",
      "Graph 46: Epoch: 037, Loss: -0.4287\n",
      "Graph 46: Epoch: 038, Loss: -0.4406\n",
      "Graph 46: Epoch: 039, Loss: -0.5281\n",
      "Graph 46: Epoch: 040, Loss: -0.5358\n",
      "Graph 46: Epoch: 041, Loss: -0.3243\n",
      "Graph 46: Epoch: 042, Loss: -0.5386\n",
      "Graph 46: Epoch: 043, Loss: -0.3244\n",
      "Graph 46: Epoch: 044, Loss: -0.4323\n",
      "Graph 46: Epoch: 045, Loss: -0.5409\n",
      "Graph 46: Epoch: 046, Loss: -0.7549\n",
      "Graph 46: Epoch: 047, Loss: -0.5386\n",
      "Graph 46: Epoch: 048, Loss: -0.5420\n",
      "Graph 46: Epoch: 049, Loss: -0.5425\n",
      "Graph 46: Epoch: 050, Loss: -0.3263\n",
      "Graph 46: Epoch: 051, Loss: -0.5436\n",
      "Graph 46: Epoch: 052, Loss: -0.4359\n",
      "Graph 46: Epoch: 053, Loss: -0.5444\n",
      "Graph 46: Epoch: 054, Loss: -0.4354\n",
      "Graph 46: Epoch: 055, Loss: -0.6521\n",
      "Graph 46: Epoch: 056, Loss: -0.7610\n",
      "Graph 46: Epoch: 057, Loss: -0.4289\n",
      "Graph 46: Epoch: 058, Loss: -0.5631\n",
      "Graph 46: Epoch: 059, Loss: -0.4364\n",
      "Graph 46: Epoch: 060, Loss: -0.6545\n",
      "Graph 46: Epoch: 061, Loss: -0.5448\n",
      "Graph 46: Epoch: 062, Loss: -0.5470\n",
      "Graph 46: Epoch: 063, Loss: -0.4819\n",
      "Graph 46: Epoch: 064, Loss: -0.6554\n",
      "Graph 46: Epoch: 065, Loss: -0.4366\n",
      "Graph 46: Epoch: 066, Loss: -0.2194\n",
      "Graph 46: Epoch: 067, Loss: -0.8733\n",
      "Graph 46: Epoch: 068, Loss: -0.5470\n",
      "Graph 46: Epoch: 069, Loss: -0.4346\n",
      "Graph 46: Epoch: 070, Loss: -0.5367\n",
      "Graph 46: Epoch: 071, Loss: -0.5465\n",
      "Graph 46: Epoch: 072, Loss: -0.4389\n",
      "Graph 46: Epoch: 073, Loss: -0.4573\n",
      "Graph 46: Epoch: 074, Loss: -0.4350\n",
      "Graph 46: Epoch: 075, Loss: -0.6565\n",
      "Graph 46: Epoch: 076, Loss: -0.7643\n",
      "Graph 46: Epoch: 077, Loss: -0.4390\n",
      "Graph 46: Epoch: 078, Loss: -0.4387\n",
      "Graph 46: Epoch: 079, Loss: -0.4389\n",
      "Graph 46: Epoch: 080, Loss: -0.6583\n",
      "Graph 46: Epoch: 081, Loss: -0.4394\n",
      "Graph 46: Epoch: 082, Loss: -0.6578\n",
      "Graph 46: Epoch: 083, Loss: -0.4395\n",
      "Graph 46: Epoch: 084, Loss: -0.2203\n",
      "Graph 46: Epoch: 085, Loss: -0.6581\n",
      "Graph 46: Epoch: 086, Loss: -0.1112\n",
      "Graph 46: Epoch: 087, Loss: -0.4399\n",
      "Graph 46: Epoch: 088, Loss: -0.5492\n",
      "Graph 46: Epoch: 089, Loss: -0.5486\n",
      "Graph 46: Epoch: 090, Loss: -0.5489\n",
      "Graph 46: Epoch: 091, Loss: -0.4399\n",
      "Graph 46: Epoch: 092, Loss: -0.3305\n",
      "Graph 46: Epoch: 093, Loss: -0.5504\n",
      "Graph 46: Epoch: 094, Loss: -0.2209\n",
      "Graph 46: Epoch: 095, Loss: -0.5499\n",
      "Graph 46: Epoch: 096, Loss: -0.6601\n",
      "Graph 46: Epoch: 097, Loss: -0.5498\n",
      "Graph 46: Epoch: 098, Loss: -0.4364\n",
      "Graph 46: Epoch: 099, Loss: -0.5503\n",
      "Graph 46: Epoch: 100, Loss: -0.4406\n",
      "Graph 47: Epoch: 001, Loss: -0.1213\n",
      "Graph 47: Epoch: 002, Loss: -0.1555\n",
      "Graph 47: Epoch: 003, Loss: -0.2777\n",
      "Graph 47: Epoch: 004, Loss: -0.4114\n",
      "Graph 47: Epoch: 005, Loss: -0.0947\n",
      "Graph 47: Epoch: 006, Loss: -0.2049\n",
      "Graph 47: Epoch: 007, Loss: -0.3344\n",
      "Graph 47: Epoch: 008, Loss: -0.3449\n",
      "Graph 47: Epoch: 009, Loss: -0.1954\n",
      "Graph 47: Epoch: 010, Loss: -0.5356\n",
      "Graph 47: Epoch: 011, Loss: -0.3931\n",
      "Graph 47: Epoch: 012, Loss: -0.2957\n",
      "Graph 47: Epoch: 013, Loss: -0.3823\n",
      "Graph 47: Epoch: 014, Loss: -0.3975\n",
      "Graph 47: Epoch: 015, Loss: -0.3771\n",
      "Graph 47: Epoch: 016, Loss: -0.4700\n",
      "Graph 47: Epoch: 017, Loss: -0.7870\n",
      "Graph 47: Epoch: 018, Loss: -0.2777\n",
      "Graph 47: Epoch: 019, Loss: -0.5927\n",
      "Graph 47: Epoch: 020, Loss: -0.5112\n",
      "Graph 47: Epoch: 021, Loss: -0.4069\n",
      "Graph 47: Epoch: 022, Loss: -0.5205\n",
      "Graph 47: Epoch: 023, Loss: -0.8242\n",
      "Graph 47: Epoch: 024, Loss: -0.4200\n",
      "Graph 47: Epoch: 025, Loss: -0.3982\n",
      "Graph 47: Epoch: 026, Loss: -0.6132\n",
      "Graph 47: Epoch: 027, Loss: -0.3181\n",
      "Graph 47: Epoch: 028, Loss: -0.5232\n",
      "Graph 47: Epoch: 029, Loss: -0.3361\n",
      "Graph 47: Epoch: 030, Loss: -0.8436\n",
      "Graph 47: Epoch: 031, Loss: -0.3149\n",
      "Graph 47: Epoch: 032, Loss: -0.4476\n",
      "Graph 47: Epoch: 033, Loss: -0.3339\n",
      "Graph 47: Epoch: 034, Loss: -0.7449\n",
      "Graph 47: Epoch: 035, Loss: -0.6362\n",
      "Graph 47: Epoch: 036, Loss: -0.2153\n",
      "Graph 47: Epoch: 037, Loss: -0.4452\n",
      "Graph 47: Epoch: 038, Loss: -0.6426\n",
      "Graph 47: Epoch: 039, Loss: -0.5357\n",
      "Graph 47: Epoch: 040, Loss: -0.4214\n",
      "Graph 47: Epoch: 041, Loss: -0.5377\n",
      "Graph 47: Epoch: 042, Loss: -0.3228\n",
      "Graph 47: Epoch: 043, Loss: -0.2171\n",
      "Graph 47: Epoch: 044, Loss: -0.7532\n",
      "Graph 47: Epoch: 045, Loss: -0.4329\n",
      "Graph 47: Epoch: 046, Loss: -0.5395\n",
      "Graph 47: Epoch: 047, Loss: -0.5406\n",
      "Graph 47: Epoch: 048, Loss: -0.5384\n",
      "Graph 47: Epoch: 049, Loss: -0.4303\n",
      "Graph 47: Epoch: 050, Loss: -0.5408\n",
      "Graph 47: Epoch: 051, Loss: -0.5415\n",
      "Graph 47: Epoch: 052, Loss: -0.4345\n",
      "Graph 47: Epoch: 053, Loss: -0.5383\n",
      "Graph 47: Epoch: 054, Loss: -0.2190\n",
      "Graph 47: Epoch: 055, Loss: -0.4356\n",
      "Graph 47: Epoch: 056, Loss: -0.5433\n",
      "Graph 47: Epoch: 057, Loss: -0.5411\n",
      "Graph 47: Epoch: 058, Loss: -0.2197\n",
      "Graph 47: Epoch: 059, Loss: -0.4346\n",
      "Graph 47: Epoch: 060, Loss: -0.3275\n",
      "Graph 47: Epoch: 061, Loss: -0.5447\n",
      "Graph 47: Epoch: 062, Loss: -0.1113\n",
      "Graph 47: Epoch: 063, Loss: -0.7611\n",
      "Graph 47: Epoch: 064, Loss: -0.5460\n",
      "Graph 47: Epoch: 065, Loss: -0.5461\n",
      "Graph 47: Epoch: 066, Loss: -0.4306\n",
      "Graph 47: Epoch: 067, Loss: -0.6525\n",
      "Graph 47: Epoch: 068, Loss: -0.6553\n",
      "Graph 47: Epoch: 069, Loss: -0.6551\n",
      "Graph 47: Epoch: 070, Loss: -0.7630\n",
      "Graph 47: Epoch: 071, Loss: -0.7649\n",
      "Graph 47: Epoch: 072, Loss: -0.5472\n",
      "Graph 47: Epoch: 073, Loss: -0.5474\n",
      "Graph 47: Epoch: 074, Loss: -0.6562\n",
      "Graph 47: Epoch: 075, Loss: -0.5465\n",
      "Graph 47: Epoch: 076, Loss: -0.5476\n",
      "Graph 47: Epoch: 077, Loss: -0.3295\n",
      "Graph 47: Epoch: 078, Loss: -0.5472\n",
      "Graph 47: Epoch: 079, Loss: -0.7661\n",
      "Graph 47: Epoch: 080, Loss: -0.5466\n",
      "Graph 47: Epoch: 081, Loss: -0.6565\n",
      "Graph 47: Epoch: 082, Loss: -0.5477\n",
      "Graph 47: Epoch: 083, Loss: -0.6571\n",
      "Graph 47: Epoch: 084, Loss: -0.4863\n",
      "Graph 47: Epoch: 085, Loss: -0.5488\n",
      "Graph 47: Epoch: 086, Loss: -0.4395\n",
      "Graph 47: Epoch: 087, Loss: -0.6583\n",
      "Graph 47: Epoch: 088, Loss: -0.1113\n",
      "Graph 47: Epoch: 089, Loss: -0.3289\n",
      "Graph 47: Epoch: 090, Loss: -0.5480\n",
      "Graph 47: Epoch: 091, Loss: -0.3570\n",
      "Graph 47: Epoch: 092, Loss: -0.8768\n",
      "Graph 47: Epoch: 093, Loss: -0.7664\n",
      "Graph 47: Epoch: 094, Loss: -0.6590\n",
      "Graph 47: Epoch: 095, Loss: -0.4393\n",
      "Graph 47: Epoch: 096, Loss: -0.4397\n",
      "Graph 47: Epoch: 097, Loss: -0.5486\n",
      "Graph 47: Epoch: 098, Loss: -0.3305\n",
      "Graph 47: Epoch: 099, Loss: -0.4353\n",
      "Graph 47: Epoch: 100, Loss: -0.4384\n",
      "Graph 48: Epoch: 001, Loss: -0.5000\n",
      "Graph 48: Epoch: 002, Loss: -0.4950\n",
      "Graph 48: Epoch: 003, Loss: -0.8788\n",
      "Graph 48: Epoch: 004, Loss: -0.8806\n",
      "Graph 48: Epoch: 005, Loss: -0.4981\n",
      "Graph 48: Epoch: 006, Loss: -0.5007\n",
      "Graph 48: Epoch: 007, Loss: -0.4960\n",
      "Graph 48: Epoch: 008, Loss: -0.5055\n",
      "Graph 48: Epoch: 009, Loss: -0.5077\n",
      "Graph 48: Epoch: 010, Loss: -0.5104\n",
      "Graph 48: Epoch: 011, Loss: -0.4865\n",
      "Graph 48: Epoch: 012, Loss: -0.4847\n",
      "Graph 48: Epoch: 013, Loss: -0.4839\n",
      "Graph 48: Epoch: 014, Loss: -0.4839\n",
      "Graph 48: Epoch: 015, Loss: -0.5154\n",
      "Graph 48: Epoch: 016, Loss: -0.4845\n",
      "Graph 48: Epoch: 017, Loss: -0.4851\n",
      "Graph 48: Epoch: 018, Loss: -0.5138\n",
      "Graph 48: Epoch: 019, Loss: -0.5134\n",
      "Graph 48: Epoch: 020, Loss: -0.5136\n",
      "Graph 48: Epoch: 021, Loss: -0.5144\n",
      "Graph 48: Epoch: 022, Loss: -0.1004\n",
      "Graph 48: Epoch: 023, Loss: -0.5167\n",
      "Graph 48: Epoch: 024, Loss: -0.4818\n",
      "Graph 48: Epoch: 025, Loss: -0.4811\n",
      "Graph 48: Epoch: 026, Loss: -0.4811\n",
      "Graph 48: Epoch: 027, Loss: -0.5184\n",
      "Graph 48: Epoch: 028, Loss: -0.8990\n",
      "Graph 48: Epoch: 029, Loss: -0.4812\n",
      "Graph 48: Epoch: 030, Loss: -0.8996\n",
      "Graph 48: Epoch: 031, Loss: -0.4815\n",
      "Graph 48: Epoch: 032, Loss: -0.5179\n",
      "Graph 48: Epoch: 033, Loss: -0.4821\n",
      "Graph 48: Epoch: 034, Loss: -0.5174\n",
      "Graph 48: Epoch: 035, Loss: -0.5174\n",
      "Graph 48: Epoch: 036, Loss: -0.5180\n",
      "Graph 48: Epoch: 037, Loss: -0.5191\n",
      "Graph 48: Epoch: 038, Loss: -0.4794\n",
      "Graph 48: Epoch: 039, Loss: -0.4786\n",
      "Graph 48: Epoch: 040, Loss: -0.4784\n",
      "Graph 48: Epoch: 041, Loss: -0.5213\n",
      "Graph 48: Epoch: 042, Loss: -0.4785\n",
      "Graph 48: Epoch: 043, Loss: -0.4789\n",
      "Graph 48: Epoch: 044, Loss: -0.5203\n",
      "Graph 48: Epoch: 045, Loss: -0.5201\n",
      "Graph 48: Epoch: 046, Loss: -0.5204\n",
      "Graph 48: Epoch: 047, Loss: -0.5212\n",
      "Graph 48: Epoch: 048, Loss: -0.4775\n",
      "Graph 48: Epoch: 049, Loss: -0.5231\n",
      "Graph 48: Epoch: 050, Loss: -0.5242\n",
      "Graph 48: Epoch: 051, Loss: -0.0918\n",
      "Graph 48: Epoch: 052, Loss: -0.4732\n",
      "Graph 48: Epoch: 053, Loss: -0.5274\n",
      "Graph 48: Epoch: 054, Loss: -0.0919\n",
      "Graph 48: Epoch: 055, Loss: -0.4709\n",
      "Graph 48: Epoch: 056, Loss: -0.4708\n",
      "Graph 48: Epoch: 057, Loss: -0.4712\n",
      "Graph 48: Epoch: 058, Loss: -0.4721\n",
      "Graph 48: Epoch: 059, Loss: -0.5266\n",
      "Graph 48: Epoch: 060, Loss: -0.5259\n",
      "Graph 48: Epoch: 061, Loss: -0.4742\n",
      "Graph 48: Epoch: 062, Loss: -0.0969\n",
      "Graph 48: Epoch: 063, Loss: -0.5245\n",
      "Graph 48: Epoch: 064, Loss: -0.5244\n",
      "Graph 48: Epoch: 065, Loss: -0.4752\n",
      "Graph 48: Epoch: 066, Loss: -0.5246\n",
      "Graph 48: Epoch: 067, Loss: -0.5250\n",
      "Graph 48: Epoch: 068, Loss: -0.5259\n",
      "Graph 48: Epoch: 069, Loss: -0.4728\n",
      "Graph 48: Epoch: 070, Loss: -0.5279\n",
      "Graph 48: Epoch: 071, Loss: -0.8995\n",
      "Graph 48: Epoch: 072, Loss: -0.4698\n",
      "Graph 48: Epoch: 073, Loss: -0.0995\n",
      "Graph 48: Epoch: 074, Loss: -0.0997\n",
      "Graph 48: Epoch: 075, Loss: -0.4689\n",
      "Graph 48: Epoch: 076, Loss: -0.5307\n",
      "Graph 48: Epoch: 077, Loss: -0.4692\n",
      "Graph 48: Epoch: 078, Loss: -0.5304\n",
      "Graph 48: Epoch: 079, Loss: -0.4695\n",
      "Graph 48: Epoch: 080, Loss: -0.4699\n",
      "Graph 48: Epoch: 081, Loss: -0.8956\n",
      "Graph 48: Epoch: 082, Loss: -0.8955\n",
      "Graph 48: Epoch: 083, Loss: -0.4717\n",
      "Graph 48: Epoch: 084, Loss: -0.5275\n",
      "Graph 48: Epoch: 085, Loss: -0.4728\n",
      "Graph 48: Epoch: 086, Loss: -0.5265\n",
      "Graph 48: Epoch: 087, Loss: -0.4736\n",
      "Graph 48: Epoch: 088, Loss: -0.4743\n",
      "Graph 48: Epoch: 089, Loss: -0.4754\n",
      "Graph 48: Epoch: 090, Loss: -0.5231\n",
      "Graph 48: Epoch: 091, Loss: -0.5222\n",
      "Graph 48: Epoch: 092, Loss: -0.4780\n",
      "Graph 48: Epoch: 093, Loss: -0.4787\n",
      "Graph 48: Epoch: 094, Loss: -0.5201\n",
      "Graph 48: Epoch: 095, Loss: -0.5195\n",
      "Graph 48: Epoch: 096, Loss: -0.4804\n",
      "Graph 48: Epoch: 097, Loss: -0.5191\n",
      "Graph 48: Epoch: 098, Loss: -0.8969\n",
      "Graph 48: Epoch: 099, Loss: -0.5194\n",
      "Graph 48: Epoch: 100, Loss: -0.5202\n",
      "Graph 49: Epoch: 001, Loss: -0.0322\n",
      "Graph 49: Epoch: 002, Loss: -0.0890\n",
      "Graph 49: Epoch: 003, Loss: -0.3300\n",
      "Graph 49: Epoch: 004, Loss: -0.3673\n",
      "Graph 49: Epoch: 005, Loss: -0.4338\n",
      "Graph 49: Epoch: 006, Loss: -0.3834\n",
      "Graph 49: Epoch: 007, Loss: -0.3194\n",
      "Graph 49: Epoch: 008, Loss: -0.3198\n",
      "Graph 49: Epoch: 009, Loss: -0.5529\n",
      "Graph 49: Epoch: 010, Loss: -0.6235\n",
      "Graph 49: Epoch: 011, Loss: -0.3420\n",
      "Graph 49: Epoch: 012, Loss: -0.4643\n",
      "Graph 49: Epoch: 013, Loss: -0.4630\n",
      "Graph 49: Epoch: 014, Loss: -0.4707\n",
      "Graph 49: Epoch: 015, Loss: -0.2947\n",
      "Graph 49: Epoch: 016, Loss: -0.4128\n",
      "Graph 49: Epoch: 017, Loss: -0.6515\n",
      "Graph 49: Epoch: 018, Loss: -0.3558\n",
      "Graph 49: Epoch: 019, Loss: -0.2959\n",
      "Graph 49: Epoch: 020, Loss: -0.5371\n",
      "Graph 49: Epoch: 021, Loss: -0.5270\n",
      "Graph 49: Epoch: 022, Loss: -0.4793\n",
      "Graph 49: Epoch: 023, Loss: -0.6037\n",
      "Graph 49: Epoch: 024, Loss: -0.5437\n",
      "Graph 49: Epoch: 025, Loss: -0.6075\n",
      "Graph 49: Epoch: 026, Loss: -0.3025\n",
      "Graph 49: Epoch: 027, Loss: -0.1831\n",
      "Graph 49: Epoch: 028, Loss: -0.5478\n",
      "Graph 49: Epoch: 029, Loss: -0.4269\n",
      "Graph 49: Epoch: 030, Loss: -0.5481\n",
      "Graph 49: Epoch: 031, Loss: -0.4888\n",
      "Graph 49: Epoch: 032, Loss: -0.5424\n",
      "Graph 49: Epoch: 033, Loss: -0.3657\n",
      "Graph 49: Epoch: 034, Loss: -0.3627\n",
      "Graph 49: Epoch: 035, Loss: -0.2454\n",
      "Graph 49: Epoch: 036, Loss: -0.4290\n",
      "Graph 49: Epoch: 037, Loss: -0.4915\n",
      "Graph 49: Epoch: 038, Loss: -0.1842\n",
      "Graph 49: Epoch: 039, Loss: -0.4905\n",
      "Graph 49: Epoch: 040, Loss: -0.6132\n",
      "Graph 49: Epoch: 041, Loss: -0.4914\n",
      "Graph 49: Epoch: 042, Loss: -0.3688\n",
      "Graph 49: Epoch: 043, Loss: -0.4924\n",
      "Graph 49: Epoch: 044, Loss: -0.4313\n",
      "Graph 49: Epoch: 045, Loss: -0.6161\n",
      "Graph 49: Epoch: 046, Loss: -0.6781\n",
      "Graph 49: Epoch: 047, Loss: -0.2471\n",
      "Graph 49: Epoch: 048, Loss: -0.6791\n",
      "Graph 49: Epoch: 049, Loss: -0.4936\n",
      "Graph 49: Epoch: 050, Loss: -0.3706\n",
      "Graph 49: Epoch: 051, Loss: -0.4943\n",
      "Graph 49: Epoch: 052, Loss: -0.4942\n",
      "Graph 49: Epoch: 053, Loss: -0.3085\n",
      "Graph 49: Epoch: 054, Loss: -0.4942\n",
      "Graph 49: Epoch: 055, Loss: -0.3094\n",
      "Graph 49: Epoch: 056, Loss: -0.3703\n",
      "Graph 49: Epoch: 057, Loss: -0.3709\n",
      "Graph 49: Epoch: 058, Loss: -0.3703\n",
      "Graph 49: Epoch: 059, Loss: -0.3715\n",
      "Graph 49: Epoch: 060, Loss: -0.3095\n",
      "Graph 49: Epoch: 061, Loss: -0.4952\n",
      "Graph 49: Epoch: 062, Loss: -0.5573\n",
      "Graph 49: Epoch: 063, Loss: -0.6808\n",
      "Graph 49: Epoch: 064, Loss: -0.5571\n",
      "Graph 49: Epoch: 065, Loss: -0.5571\n",
      "Graph 49: Epoch: 066, Loss: -0.3716\n",
      "Graph 49: Epoch: 067, Loss: -0.4336\n",
      "Graph 49: Epoch: 068, Loss: -0.4955\n",
      "Graph 49: Epoch: 069, Loss: -0.5576\n",
      "Graph 49: Epoch: 070, Loss: -0.6815\n",
      "Graph 49: Epoch: 071, Loss: -0.3717\n",
      "Graph 49: Epoch: 072, Loss: -0.5577\n",
      "Graph 49: Epoch: 073, Loss: -0.6199\n",
      "Graph 49: Epoch: 074, Loss: -0.4959\n",
      "Graph 49: Epoch: 075, Loss: -0.4957\n",
      "Graph 49: Epoch: 076, Loss: -0.5581\n",
      "Graph 49: Epoch: 077, Loss: -0.5578\n",
      "Graph 49: Epoch: 078, Loss: -0.6202\n",
      "Graph 49: Epoch: 079, Loss: -0.4343\n",
      "Graph 49: Epoch: 080, Loss: -0.6201\n",
      "Graph 49: Epoch: 081, Loss: -0.4963\n",
      "Graph 49: Epoch: 082, Loss: -0.4959\n",
      "Graph 49: Epoch: 083, Loss: -0.3725\n",
      "Graph 49: Epoch: 084, Loss: -0.3725\n",
      "Graph 49: Epoch: 085, Loss: -0.3726\n",
      "Graph 49: Epoch: 086, Loss: -0.3103\n",
      "Graph 49: Epoch: 087, Loss: -0.4346\n",
      "Graph 49: Epoch: 088, Loss: -0.5588\n",
      "Graph 49: Epoch: 089, Loss: -0.4965\n",
      "Graph 49: Epoch: 090, Loss: -0.3727\n",
      "Graph 49: Epoch: 091, Loss: -0.3727\n",
      "Graph 49: Epoch: 092, Loss: -0.4347\n",
      "Graph 49: Epoch: 093, Loss: -0.4966\n",
      "Graph 49: Epoch: 094, Loss: -0.5590\n",
      "Graph 49: Epoch: 095, Loss: -0.6212\n",
      "Graph 49: Epoch: 096, Loss: -0.4347\n",
      "Graph 49: Epoch: 097, Loss: -0.6829\n",
      "Graph 49: Epoch: 098, Loss: -0.6211\n",
      "Graph 49: Epoch: 099, Loss: -0.4350\n",
      "Graph 49: Epoch: 100, Loss: -0.6213\n",
      "Graph 50: Epoch: 001, Loss: -0.0527\n",
      "Graph 50: Epoch: 002, Loss: -0.1329\n",
      "Graph 50: Epoch: 003, Loss: -0.2491\n",
      "Graph 50: Epoch: 004, Loss: -0.1872\n",
      "Graph 50: Epoch: 005, Loss: -0.3424\n",
      "Graph 50: Epoch: 006, Loss: -0.4120\n",
      "Graph 50: Epoch: 007, Loss: -0.3005\n",
      "Graph 50: Epoch: 008, Loss: -0.4417\n",
      "Graph 50: Epoch: 009, Loss: -0.5641\n",
      "Graph 50: Epoch: 010, Loss: -0.4875\n",
      "Graph 50: Epoch: 011, Loss: -0.3215\n",
      "Graph 50: Epoch: 012, Loss: -0.4837\n",
      "Graph 50: Epoch: 013, Loss: -0.4958\n",
      "Graph 50: Epoch: 014, Loss: -0.3233\n",
      "Graph 50: Epoch: 015, Loss: -0.4139\n",
      "Graph 50: Epoch: 016, Loss: -0.6546\n",
      "Graph 50: Epoch: 017, Loss: -0.4216\n",
      "Graph 50: Epoch: 018, Loss: -0.1801\n",
      "Graph 50: Epoch: 019, Loss: -0.4284\n",
      "Graph 50: Epoch: 020, Loss: -0.7514\n",
      "Graph 50: Epoch: 021, Loss: -0.3403\n",
      "Graph 50: Epoch: 022, Loss: -0.6021\n",
      "Graph 50: Epoch: 023, Loss: -0.4322\n",
      "Graph 50: Epoch: 024, Loss: -0.5207\n",
      "Graph 50: Epoch: 025, Loss: -0.4230\n",
      "Graph 50: Epoch: 026, Loss: -0.5212\n",
      "Graph 50: Epoch: 027, Loss: -0.4377\n",
      "Graph 50: Epoch: 028, Loss: -0.6112\n",
      "Graph 50: Epoch: 029, Loss: -0.4376\n",
      "Graph 50: Epoch: 030, Loss: -0.5240\n",
      "Graph 50: Epoch: 031, Loss: -0.2644\n",
      "Graph 50: Epoch: 032, Loss: -0.4386\n",
      "Graph 50: Epoch: 033, Loss: -0.3525\n",
      "Graph 50: Epoch: 034, Loss: -0.3546\n",
      "Graph 50: Epoch: 035, Loss: -0.3374\n",
      "Graph 50: Epoch: 036, Loss: -0.4417\n",
      "Graph 50: Epoch: 037, Loss: -0.4407\n",
      "Graph 50: Epoch: 038, Loss: -0.2660\n",
      "Graph 50: Epoch: 039, Loss: -0.3546\n",
      "Graph 50: Epoch: 040, Loss: -0.3547\n",
      "Graph 50: Epoch: 041, Loss: -0.4431\n",
      "Graph 50: Epoch: 042, Loss: -0.6193\n",
      "Graph 50: Epoch: 043, Loss: -0.6204\n",
      "Graph 50: Epoch: 044, Loss: -0.3526\n",
      "Graph 50: Epoch: 045, Loss: -0.8853\n",
      "Graph 50: Epoch: 046, Loss: -0.0908\n",
      "Graph 50: Epoch: 047, Loss: -0.3494\n",
      "Graph 50: Epoch: 048, Loss: -0.1782\n",
      "Graph 50: Epoch: 049, Loss: -0.2682\n",
      "Graph 50: Epoch: 050, Loss: -0.4460\n",
      "Graph 50: Epoch: 051, Loss: -0.5346\n",
      "Graph 50: Epoch: 052, Loss: -0.6251\n",
      "Graph 50: Epoch: 053, Loss: -0.8906\n",
      "Graph 50: Epoch: 054, Loss: -0.5357\n",
      "Graph 50: Epoch: 055, Loss: -0.5360\n",
      "Graph 50: Epoch: 056, Loss: -0.5362\n",
      "Graph 50: Epoch: 057, Loss: -0.2669\n",
      "Graph 50: Epoch: 058, Loss: -0.4469\n",
      "Graph 50: Epoch: 059, Loss: -0.6262\n",
      "Graph 50: Epoch: 060, Loss: -0.5374\n",
      "Graph 50: Epoch: 061, Loss: -0.4463\n",
      "Graph 50: Epoch: 062, Loss: -0.4478\n",
      "Graph 50: Epoch: 063, Loss: -0.5375\n",
      "Graph 50: Epoch: 064, Loss: -0.7148\n",
      "Graph 50: Epoch: 065, Loss: -0.4487\n",
      "Graph 50: Epoch: 066, Loss: -0.6279\n",
      "Graph 50: Epoch: 067, Loss: -0.4483\n",
      "Graph 50: Epoch: 068, Loss: -0.7162\n",
      "Graph 50: Epoch: 069, Loss: -0.6261\n",
      "Graph 50: Epoch: 070, Loss: -0.4491\n",
      "Graph 50: Epoch: 071, Loss: -0.7178\n",
      "Graph 50: Epoch: 072, Loss: -0.6285\n",
      "Graph 50: Epoch: 073, Loss: -0.6285\n",
      "Graph 50: Epoch: 074, Loss: -0.4496\n",
      "Graph 50: Epoch: 075, Loss: -0.5392\n",
      "Graph 50: Epoch: 076, Loss: -0.3600\n",
      "Graph 50: Epoch: 077, Loss: -0.5394\n",
      "Graph 50: Epoch: 078, Loss: -0.7190\n",
      "Graph 50: Epoch: 079, Loss: -0.7177\n",
      "Graph 50: Epoch: 080, Loss: -0.3718\n",
      "Graph 50: Epoch: 081, Loss: -0.4500\n",
      "Graph 50: Epoch: 082, Loss: -0.5397\n",
      "Graph 50: Epoch: 083, Loss: -0.4502\n",
      "Graph 50: Epoch: 084, Loss: -0.6301\n",
      "Graph 50: Epoch: 085, Loss: -0.5404\n",
      "Graph 50: Epoch: 086, Loss: -0.4503\n",
      "Graph 50: Epoch: 087, Loss: -0.3604\n",
      "Graph 50: Epoch: 088, Loss: -0.3602\n",
      "Graph 50: Epoch: 089, Loss: -0.7194\n",
      "Graph 50: Epoch: 090, Loss: -0.5404\n",
      "Graph 50: Epoch: 091, Loss: -0.5403\n",
      "Graph 50: Epoch: 092, Loss: -0.7206\n",
      "Graph 50: Epoch: 093, Loss: -0.2708\n",
      "Graph 50: Epoch: 094, Loss: -0.5407\n",
      "Graph 50: Epoch: 095, Loss: -0.6307\n",
      "Graph 50: Epoch: 096, Loss: -0.5411\n",
      "Graph 50: Epoch: 097, Loss: -0.4501\n",
      "Graph 50: Epoch: 098, Loss: -0.5401\n",
      "Graph 50: Epoch: 099, Loss: -0.6311\n",
      "Graph 50: Epoch: 100, Loss: -0.5411\n",
      "Graph 51: Epoch: 001, Loss: -0.0267\n",
      "Graph 51: Epoch: 002, Loss: -0.0953\n",
      "Graph 51: Epoch: 003, Loss: -0.1377\n",
      "Graph 51: Epoch: 004, Loss: -0.3723\n",
      "Graph 51: Epoch: 005, Loss: -0.4010\n",
      "Graph 51: Epoch: 006, Loss: -0.3872\n",
      "Graph 51: Epoch: 007, Loss: -0.0885\n",
      "Graph 51: Epoch: 008, Loss: -0.2350\n",
      "Graph 51: Epoch: 009, Loss: -0.5062\n",
      "Graph 51: Epoch: 010, Loss: -0.5361\n",
      "Graph 51: Epoch: 011, Loss: -0.2432\n",
      "Graph 51: Epoch: 012, Loss: -0.4866\n",
      "Graph 51: Epoch: 013, Loss: -0.1481\n",
      "Graph 51: Epoch: 014, Loss: -0.4214\n",
      "Graph 51: Epoch: 015, Loss: -0.4224\n",
      "Graph 51: Epoch: 016, Loss: -0.3600\n",
      "Graph 51: Epoch: 017, Loss: -0.4920\n",
      "Graph 51: Epoch: 018, Loss: -0.5709\n",
      "Graph 51: Epoch: 019, Loss: -0.2098\n",
      "Graph 51: Epoch: 020, Loss: -0.3631\n",
      "Graph 51: Epoch: 021, Loss: -0.4375\n",
      "Graph 51: Epoch: 022, Loss: -0.5122\n",
      "Graph 51: Epoch: 023, Loss: -0.7307\n",
      "Graph 51: Epoch: 024, Loss: -0.5111\n",
      "Graph 51: Epoch: 025, Loss: -0.7309\n",
      "Graph 51: Epoch: 026, Loss: -0.5171\n",
      "Graph 51: Epoch: 027, Loss: -0.2236\n",
      "Graph 51: Epoch: 028, Loss: -0.5113\n",
      "Graph 51: Epoch: 029, Loss: -0.4478\n",
      "Graph 51: Epoch: 030, Loss: -0.7426\n",
      "Graph 51: Epoch: 031, Loss: -0.3753\n",
      "Graph 51: Epoch: 032, Loss: -0.3743\n",
      "Graph 51: Epoch: 033, Loss: -0.6730\n",
      "Graph 51: Epoch: 034, Loss: -0.5901\n",
      "Graph 51: Epoch: 035, Loss: -0.3756\n",
      "Graph 51: Epoch: 036, Loss: -0.3892\n",
      "Graph 51: Epoch: 037, Loss: -0.3750\n",
      "Graph 51: Epoch: 038, Loss: -0.3758\n",
      "Graph 51: Epoch: 039, Loss: -0.5258\n",
      "Graph 51: Epoch: 040, Loss: -0.3009\n",
      "Graph 51: Epoch: 041, Loss: -0.5182\n",
      "Graph 51: Epoch: 042, Loss: -0.6024\n",
      "Graph 51: Epoch: 043, Loss: -0.3762\n",
      "Graph 51: Epoch: 044, Loss: -0.3771\n",
      "Graph 51: Epoch: 045, Loss: -0.3781\n",
      "Graph 51: Epoch: 046, Loss: -0.3030\n",
      "Graph 51: Epoch: 047, Loss: -0.3773\n",
      "Graph 51: Epoch: 048, Loss: -0.4536\n",
      "Graph 51: Epoch: 049, Loss: -0.6803\n",
      "Graph 51: Epoch: 050, Loss: -0.3034\n",
      "Graph 51: Epoch: 051, Loss: -0.3783\n",
      "Graph 51: Epoch: 052, Loss: -0.3791\n",
      "Graph 51: Epoch: 053, Loss: -0.3792\n",
      "Graph 51: Epoch: 054, Loss: -0.5293\n",
      "Graph 51: Epoch: 055, Loss: -0.3793\n",
      "Graph 51: Epoch: 056, Loss: -0.5306\n",
      "Graph 51: Epoch: 057, Loss: -0.3795\n",
      "Graph 51: Epoch: 058, Loss: -0.6062\n",
      "Graph 51: Epoch: 059, Loss: -0.4555\n",
      "Graph 51: Epoch: 060, Loss: -0.6834\n",
      "Graph 51: Epoch: 061, Loss: -0.3226\n",
      "Graph 51: Epoch: 062, Loss: -0.6070\n",
      "Graph 51: Epoch: 063, Loss: -0.4561\n",
      "Graph 51: Epoch: 064, Loss: -0.1527\n",
      "Graph 51: Epoch: 065, Loss: -0.5443\n",
      "Graph 51: Epoch: 066, Loss: -0.3044\n",
      "Graph 51: Epoch: 067, Loss: -0.4563\n",
      "Graph 51: Epoch: 068, Loss: -0.3799\n",
      "Graph 51: Epoch: 069, Loss: -0.4567\n",
      "Graph 51: Epoch: 070, Loss: -0.8357\n",
      "Graph 51: Epoch: 071, Loss: -0.5327\n",
      "Graph 51: Epoch: 072, Loss: -0.4568\n",
      "Graph 51: Epoch: 073, Loss: -0.6835\n",
      "Graph 51: Epoch: 074, Loss: -0.4568\n",
      "Graph 51: Epoch: 075, Loss: -0.4566\n",
      "Graph 51: Epoch: 076, Loss: -0.3051\n",
      "Graph 51: Epoch: 077, Loss: -0.7607\n",
      "Graph 51: Epoch: 078, Loss: -0.5306\n",
      "Graph 51: Epoch: 079, Loss: -0.4569\n",
      "Graph 51: Epoch: 080, Loss: -0.4574\n",
      "Graph 51: Epoch: 081, Loss: -0.2294\n",
      "Graph 51: Epoch: 082, Loss: -0.4575\n",
      "Graph 51: Epoch: 083, Loss: -0.5338\n",
      "Graph 51: Epoch: 084, Loss: -0.3807\n",
      "Graph 51: Epoch: 085, Loss: -0.3815\n",
      "Graph 51: Epoch: 086, Loss: -0.6096\n",
      "Graph 51: Epoch: 087, Loss: -0.6099\n",
      "Graph 51: Epoch: 088, Loss: -0.6101\n",
      "Graph 51: Epoch: 089, Loss: -0.5327\n",
      "Graph 51: Epoch: 090, Loss: -0.5341\n",
      "Graph 51: Epoch: 091, Loss: -0.6104\n",
      "Graph 51: Epoch: 092, Loss: -0.3816\n",
      "Graph 51: Epoch: 093, Loss: -0.3818\n",
      "Graph 51: Epoch: 094, Loss: -0.5337\n",
      "Graph 51: Epoch: 095, Loss: -0.6108\n",
      "Graph 51: Epoch: 096, Loss: -0.6108\n",
      "Graph 51: Epoch: 097, Loss: -0.3821\n",
      "Graph 51: Epoch: 098, Loss: -0.2295\n",
      "Graph 51: Epoch: 099, Loss: -0.6109\n",
      "Graph 51: Epoch: 100, Loss: -0.4583\n",
      "Graph 52: Epoch: 001, Loss: -0.0082\n",
      "Graph 52: Epoch: 002, Loss: -0.0398\n",
      "Graph 52: Epoch: 003, Loss: -0.1848\n",
      "Graph 52: Epoch: 004, Loss: -0.3838\n",
      "Graph 52: Epoch: 005, Loss: -0.4328\n",
      "Graph 52: Epoch: 006, Loss: -0.2999\n",
      "Graph 52: Epoch: 007, Loss: -0.5823\n",
      "Graph 52: Epoch: 008, Loss: -0.5636\n",
      "Graph 52: Epoch: 009, Loss: -0.4555\n",
      "Graph 52: Epoch: 010, Loss: -0.5417\n",
      "Graph 52: Epoch: 011, Loss: -0.6290\n",
      "Graph 52: Epoch: 012, Loss: -0.3431\n",
      "Graph 52: Epoch: 013, Loss: -0.6289\n",
      "Graph 52: Epoch: 014, Loss: -0.5953\n",
      "Graph 52: Epoch: 015, Loss: -0.6013\n",
      "Graph 52: Epoch: 016, Loss: -0.4973\n",
      "Graph 52: Epoch: 017, Loss: -0.3922\n",
      "Graph 52: Epoch: 018, Loss: -0.3506\n",
      "Graph 52: Epoch: 019, Loss: -0.5590\n",
      "Graph 52: Epoch: 020, Loss: -0.4084\n",
      "Graph 52: Epoch: 021, Loss: -0.5549\n",
      "Graph 52: Epoch: 022, Loss: -0.4608\n",
      "Graph 52: Epoch: 023, Loss: -0.4611\n",
      "Graph 52: Epoch: 024, Loss: -0.3051\n",
      "Graph 52: Epoch: 025, Loss: -0.4607\n",
      "Graph 52: Epoch: 026, Loss: -0.4102\n",
      "Graph 52: Epoch: 027, Loss: -0.3604\n",
      "Graph 52: Epoch: 028, Loss: -0.5143\n",
      "Graph 52: Epoch: 029, Loss: -0.3608\n",
      "Graph 52: Epoch: 030, Loss: -0.4132\n",
      "Graph 52: Epoch: 031, Loss: -0.3103\n",
      "Graph 52: Epoch: 032, Loss: -0.5680\n",
      "Graph 52: Epoch: 033, Loss: -0.5169\n",
      "Graph 52: Epoch: 034, Loss: -0.4587\n",
      "Graph 52: Epoch: 035, Loss: -0.4652\n",
      "Graph 52: Epoch: 036, Loss: -0.4663\n",
      "Graph 52: Epoch: 037, Loss: -0.5700\n",
      "Graph 52: Epoch: 038, Loss: -0.5712\n",
      "Graph 52: Epoch: 039, Loss: -0.3639\n",
      "Graph 52: Epoch: 040, Loss: -0.4677\n",
      "Graph 52: Epoch: 041, Loss: -0.4160\n",
      "Graph 52: Epoch: 042, Loss: -0.5196\n",
      "Graph 52: Epoch: 043, Loss: -0.5200\n",
      "Graph 52: Epoch: 044, Loss: -0.6760\n",
      "Graph 52: Epoch: 045, Loss: -0.5725\n",
      "Graph 52: Epoch: 046, Loss: -0.4163\n",
      "Graph 52: Epoch: 047, Loss: -0.6762\n",
      "Graph 52: Epoch: 048, Loss: -0.4685\n",
      "Graph 52: Epoch: 049, Loss: -0.3646\n",
      "Graph 52: Epoch: 050, Loss: -0.4666\n",
      "Graph 52: Epoch: 051, Loss: -0.4690\n",
      "Graph 52: Epoch: 052, Loss: -0.5211\n",
      "Graph 52: Epoch: 053, Loss: -0.5732\n",
      "Graph 52: Epoch: 054, Loss: -0.4171\n",
      "Graph 52: Epoch: 055, Loss: -0.3652\n",
      "Graph 52: Epoch: 056, Loss: -0.4691\n",
      "Graph 52: Epoch: 057, Loss: -0.2088\n",
      "Graph 52: Epoch: 058, Loss: -0.6261\n",
      "Graph 52: Epoch: 059, Loss: -0.4698\n",
      "Graph 52: Epoch: 060, Loss: -0.2611\n",
      "Graph 52: Epoch: 061, Loss: -0.5221\n",
      "Graph 52: Epoch: 062, Loss: -0.4178\n",
      "Graph 52: Epoch: 063, Loss: -0.6785\n",
      "Graph 52: Epoch: 064, Loss: -0.4701\n",
      "Graph 52: Epoch: 065, Loss: -0.4181\n",
      "Graph 52: Epoch: 066, Loss: -0.4700\n",
      "Graph 52: Epoch: 067, Loss: -0.6268\n",
      "Graph 52: Epoch: 068, Loss: -0.5758\n",
      "Graph 52: Epoch: 069, Loss: -0.5749\n",
      "Graph 52: Epoch: 070, Loss: -0.7317\n",
      "Graph 52: Epoch: 071, Loss: -0.4705\n",
      "Graph 52: Epoch: 072, Loss: -0.5229\n",
      "Graph 52: Epoch: 073, Loss: -0.6795\n",
      "Graph 52: Epoch: 074, Loss: -0.5752\n",
      "Graph 52: Epoch: 075, Loss: -0.4705\n",
      "Graph 52: Epoch: 076, Loss: -0.6275\n",
      "Graph 52: Epoch: 077, Loss: -0.4706\n",
      "Graph 52: Epoch: 078, Loss: -0.5750\n",
      "Graph 52: Epoch: 079, Loss: -0.6275\n",
      "Graph 52: Epoch: 080, Loss: -0.3141\n",
      "Graph 52: Epoch: 081, Loss: -0.6268\n",
      "Graph 52: Epoch: 082, Loss: -0.4710\n",
      "Graph 52: Epoch: 083, Loss: -0.6802\n",
      "Graph 52: Epoch: 084, Loss: -0.7325\n",
      "Graph 52: Epoch: 085, Loss: -0.4710\n",
      "Graph 52: Epoch: 086, Loss: -0.6280\n",
      "Graph 52: Epoch: 087, Loss: -0.4186\n",
      "Graph 52: Epoch: 088, Loss: -0.5233\n",
      "Graph 52: Epoch: 089, Loss: -0.5235\n",
      "Graph 52: Epoch: 090, Loss: -0.4189\n",
      "Graph 52: Epoch: 091, Loss: -0.3664\n",
      "Graph 52: Epoch: 092, Loss: -0.6278\n",
      "Graph 52: Epoch: 093, Loss: -0.5758\n",
      "Graph 52: Epoch: 094, Loss: -0.5237\n",
      "Graph 52: Epoch: 095, Loss: -0.6284\n",
      "Graph 52: Epoch: 096, Loss: -0.5760\n",
      "Graph 52: Epoch: 097, Loss: -0.5761\n",
      "Graph 52: Epoch: 098, Loss: -0.6285\n",
      "Graph 52: Epoch: 099, Loss: -0.4714\n",
      "Graph 52: Epoch: 100, Loss: -0.5238\n",
      "Graph 53: Epoch: 001, Loss: -0.0261\n",
      "Graph 53: Epoch: 002, Loss: -0.1150\n",
      "Graph 53: Epoch: 003, Loss: -0.2212\n",
      "Graph 53: Epoch: 004, Loss: -0.3458\n",
      "Graph 53: Epoch: 005, Loss: -0.2983\n",
      "Graph 53: Epoch: 006, Loss: -0.3273\n",
      "Graph 53: Epoch: 007, Loss: -0.3326\n",
      "Graph 53: Epoch: 008, Loss: -0.5102\n",
      "Graph 53: Epoch: 009, Loss: -0.2612\n",
      "Graph 53: Epoch: 010, Loss: -0.3921\n",
      "Graph 53: Epoch: 011, Loss: -0.5175\n",
      "Graph 53: Epoch: 012, Loss: -0.5787\n",
      "Graph 53: Epoch: 013, Loss: -0.4485\n",
      "Graph 53: Epoch: 014, Loss: -0.5423\n",
      "Graph 53: Epoch: 015, Loss: -0.4392\n",
      "Graph 53: Epoch: 016, Loss: -0.3027\n",
      "Graph 53: Epoch: 017, Loss: -0.4450\n",
      "Graph 53: Epoch: 018, Loss: -0.4399\n",
      "Graph 53: Epoch: 019, Loss: -0.4400\n",
      "Graph 53: Epoch: 020, Loss: -0.4057\n",
      "Graph 53: Epoch: 021, Loss: -0.5087\n",
      "Graph 53: Epoch: 022, Loss: -0.2598\n",
      "Graph 53: Epoch: 023, Loss: -0.3887\n",
      "Graph 53: Epoch: 024, Loss: -0.3872\n",
      "Graph 53: Epoch: 025, Loss: -0.4550\n",
      "Graph 53: Epoch: 026, Loss: -0.5828\n",
      "Graph 53: Epoch: 027, Loss: -0.4935\n",
      "Graph 53: Epoch: 028, Loss: -0.4556\n",
      "Graph 53: Epoch: 029, Loss: -0.5852\n",
      "Graph 53: Epoch: 030, Loss: -0.3265\n",
      "Graph 53: Epoch: 031, Loss: -0.3269\n",
      "Graph 53: Epoch: 032, Loss: -0.5870\n",
      "Graph 53: Epoch: 033, Loss: -0.3276\n",
      "Graph 53: Epoch: 034, Loss: -0.5226\n",
      "Graph 53: Epoch: 035, Loss: -0.3929\n",
      "Graph 53: Epoch: 036, Loss: -0.3925\n",
      "Graph 53: Epoch: 037, Loss: -0.5227\n",
      "Graph 53: Epoch: 038, Loss: -0.4591\n",
      "Graph 53: Epoch: 039, Loss: -0.7208\n",
      "Graph 53: Epoch: 040, Loss: -0.5248\n",
      "Graph 53: Epoch: 041, Loss: -0.7876\n",
      "Graph 53: Epoch: 042, Loss: -0.4594\n",
      "Graph 53: Epoch: 043, Loss: -0.3945\n",
      "Graph 53: Epoch: 044, Loss: -0.4600\n",
      "Graph 53: Epoch: 045, Loss: -0.5262\n",
      "Graph 53: Epoch: 046, Loss: -0.5247\n",
      "Graph 53: Epoch: 047, Loss: -0.2637\n",
      "Graph 53: Epoch: 048, Loss: -0.5268\n",
      "Graph 53: Epoch: 049, Loss: -0.3295\n",
      "Graph 53: Epoch: 050, Loss: -0.2639\n",
      "Graph 53: Epoch: 051, Loss: -0.5934\n",
      "Graph 53: Epoch: 052, Loss: -0.1981\n",
      "Graph 53: Epoch: 053, Loss: -0.5271\n",
      "Graph 53: Epoch: 054, Loss: -0.3954\n",
      "Graph 53: Epoch: 055, Loss: -0.3300\n",
      "Graph 53: Epoch: 056, Loss: -0.4620\n",
      "Graph 53: Epoch: 057, Loss: -0.6599\n",
      "Graph 53: Epoch: 058, Loss: -0.5931\n",
      "Graph 53: Epoch: 059, Loss: -0.3302\n",
      "Graph 53: Epoch: 060, Loss: -0.4596\n",
      "Graph 53: Epoch: 061, Loss: -0.4624\n",
      "Graph 53: Epoch: 062, Loss: -0.3305\n",
      "Graph 53: Epoch: 063, Loss: -0.5285\n",
      "Graph 53: Epoch: 064, Loss: -0.5282\n",
      "Graph 53: Epoch: 065, Loss: -0.5924\n",
      "Graph 53: Epoch: 066, Loss: -0.6607\n",
      "Graph 53: Epoch: 067, Loss: -0.3306\n",
      "Graph 53: Epoch: 068, Loss: -0.5290\n",
      "Graph 53: Epoch: 069, Loss: -0.3964\n",
      "Graph 53: Epoch: 070, Loss: -0.3963\n",
      "Graph 53: Epoch: 071, Loss: -0.3970\n",
      "Graph 53: Epoch: 072, Loss: -0.7266\n",
      "Graph 53: Epoch: 073, Loss: -0.3308\n",
      "Graph 53: Epoch: 074, Loss: -0.5953\n",
      "Graph 53: Epoch: 075, Loss: -0.4628\n",
      "Graph 53: Epoch: 076, Loss: -0.3970\n",
      "Graph 53: Epoch: 077, Loss: -0.1987\n",
      "Graph 53: Epoch: 078, Loss: -0.4628\n",
      "Graph 53: Epoch: 079, Loss: -0.3310\n",
      "Graph 53: Epoch: 080, Loss: -0.5957\n",
      "Graph 53: Epoch: 081, Loss: -0.5955\n",
      "Graph 53: Epoch: 082, Loss: -0.6616\n",
      "Graph 53: Epoch: 083, Loss: -0.4634\n",
      "Graph 53: Epoch: 084, Loss: -0.4696\n",
      "Graph 53: Epoch: 085, Loss: -0.5296\n",
      "Graph 53: Epoch: 086, Loss: -0.4637\n",
      "Graph 53: Epoch: 087, Loss: -0.7275\n",
      "Graph 53: Epoch: 088, Loss: -0.4638\n",
      "Graph 53: Epoch: 089, Loss: -0.5299\n",
      "Graph 53: Epoch: 090, Loss: -0.6624\n",
      "Graph 53: Epoch: 091, Loss: -0.1327\n",
      "Graph 53: Epoch: 092, Loss: -0.1990\n",
      "Graph 53: Epoch: 093, Loss: -0.6624\n",
      "Graph 53: Epoch: 094, Loss: -0.4638\n",
      "Graph 53: Epoch: 095, Loss: -0.4639\n",
      "Graph 53: Epoch: 096, Loss: -0.5302\n",
      "Graph 53: Epoch: 097, Loss: -0.3313\n",
      "Graph 53: Epoch: 098, Loss: -0.6628\n",
      "Graph 53: Epoch: 099, Loss: -0.1988\n",
      "Graph 53: Epoch: 100, Loss: -0.5303\n",
      "Graph 54: Epoch: 001, Loss: -0.0256\n",
      "Graph 54: Epoch: 002, Loss: -0.0855\n",
      "Graph 54: Epoch: 003, Loss: -0.1858\n",
      "Graph 54: Epoch: 004, Loss: -0.2289\n",
      "Graph 54: Epoch: 005, Loss: -0.4208\n",
      "Graph 54: Epoch: 006, Loss: -0.4135\n",
      "Graph 54: Epoch: 007, Loss: -0.4290\n",
      "Graph 54: Epoch: 008, Loss: -0.2468\n",
      "Graph 54: Epoch: 009, Loss: -0.4194\n",
      "Graph 54: Epoch: 010, Loss: -0.2570\n",
      "Graph 54: Epoch: 011, Loss: -0.2647\n",
      "Graph 54: Epoch: 012, Loss: -0.3672\n",
      "Graph 54: Epoch: 013, Loss: -0.5134\n",
      "Graph 54: Epoch: 014, Loss: -0.4312\n",
      "Graph 54: Epoch: 015, Loss: -0.4075\n",
      "Graph 54: Epoch: 016, Loss: -0.5403\n",
      "Graph 54: Epoch: 017, Loss: -0.3604\n",
      "Graph 54: Epoch: 018, Loss: -0.4082\n",
      "Graph 54: Epoch: 019, Loss: -0.4883\n",
      "Graph 54: Epoch: 020, Loss: -0.4196\n",
      "Graph 54: Epoch: 021, Loss: -0.5091\n",
      "Graph 54: Epoch: 022, Loss: -0.1913\n",
      "Graph 54: Epoch: 023, Loss: -0.7315\n",
      "Graph 54: Epoch: 024, Loss: -0.5820\n",
      "Graph 54: Epoch: 025, Loss: -0.4286\n",
      "Graph 54: Epoch: 026, Loss: -0.6855\n",
      "Graph 54: Epoch: 027, Loss: -0.5148\n",
      "Graph 54: Epoch: 028, Loss: -0.4353\n",
      "Graph 54: Epoch: 029, Loss: -0.3434\n",
      "Graph 54: Epoch: 030, Loss: -0.4306\n",
      "Graph 54: Epoch: 031, Loss: -0.3480\n",
      "Graph 54: Epoch: 032, Loss: -0.5179\n",
      "Graph 54: Epoch: 033, Loss: -0.3482\n",
      "Graph 54: Epoch: 034, Loss: -0.6141\n",
      "Graph 54: Epoch: 035, Loss: -0.3524\n",
      "Graph 54: Epoch: 036, Loss: -0.5247\n",
      "Graph 54: Epoch: 037, Loss: -0.5276\n",
      "Graph 54: Epoch: 038, Loss: -0.6136\n",
      "Graph 54: Epoch: 039, Loss: -0.4412\n",
      "Graph 54: Epoch: 040, Loss: -0.4411\n",
      "Graph 54: Epoch: 041, Loss: -0.4450\n",
      "Graph 54: Epoch: 042, Loss: -0.1791\n",
      "Graph 54: Epoch: 043, Loss: -0.2675\n",
      "Graph 54: Epoch: 044, Loss: -0.7930\n",
      "Graph 54: Epoch: 045, Loss: -0.5301\n",
      "Graph 54: Epoch: 046, Loss: -0.6996\n",
      "Graph 54: Epoch: 047, Loss: -0.5326\n",
      "Graph 54: Epoch: 048, Loss: -0.4421\n",
      "Graph 54: Epoch: 049, Loss: -0.4438\n",
      "Graph 54: Epoch: 050, Loss: -0.2652\n",
      "Graph 54: Epoch: 051, Loss: -0.5327\n",
      "Graph 54: Epoch: 052, Loss: -0.3568\n",
      "Graph 54: Epoch: 053, Loss: -0.6239\n",
      "Graph 54: Epoch: 054, Loss: -0.4459\n",
      "Graph 54: Epoch: 055, Loss: -0.2665\n",
      "Graph 54: Epoch: 056, Loss: -0.2846\n",
      "Graph 54: Epoch: 057, Loss: -0.1791\n",
      "Graph 54: Epoch: 058, Loss: -0.5346\n",
      "Graph 54: Epoch: 059, Loss: -0.5361\n",
      "Graph 54: Epoch: 060, Loss: -0.6244\n",
      "Graph 54: Epoch: 061, Loss: -0.6244\n",
      "Graph 54: Epoch: 062, Loss: -0.3550\n",
      "Graph 54: Epoch: 063, Loss: -0.5361\n",
      "Graph 54: Epoch: 064, Loss: -0.3580\n",
      "Graph 54: Epoch: 065, Loss: -0.4469\n",
      "Graph 54: Epoch: 066, Loss: -0.7105\n",
      "Graph 54: Epoch: 067, Loss: -0.4475\n",
      "Graph 54: Epoch: 068, Loss: -0.5360\n",
      "Graph 54: Epoch: 069, Loss: -0.4480\n",
      "Graph 54: Epoch: 070, Loss: -0.1779\n",
      "Graph 54: Epoch: 071, Loss: -0.3579\n",
      "Graph 54: Epoch: 072, Loss: -0.5376\n",
      "Graph 54: Epoch: 073, Loss: -0.6272\n",
      "Graph 54: Epoch: 074, Loss: -0.3589\n",
      "Graph 54: Epoch: 075, Loss: -0.4463\n",
      "Graph 54: Epoch: 076, Loss: -0.3589\n",
      "Graph 54: Epoch: 077, Loss: -0.4489\n",
      "Graph 54: Epoch: 078, Loss: -0.5383\n",
      "Graph 54: Epoch: 079, Loss: -0.0911\n",
      "Graph 54: Epoch: 080, Loss: -0.5384\n",
      "Graph 54: Epoch: 081, Loss: -0.5383\n",
      "Graph 54: Epoch: 082, Loss: -0.6277\n",
      "Graph 54: Epoch: 083, Loss: -0.6229\n",
      "Graph 54: Epoch: 084, Loss: -0.6270\n",
      "Graph 54: Epoch: 085, Loss: -0.5373\n",
      "Graph 54: Epoch: 086, Loss: -0.6317\n",
      "Graph 54: Epoch: 087, Loss: -0.4490\n",
      "Graph 54: Epoch: 088, Loss: -0.1804\n",
      "Graph 54: Epoch: 089, Loss: -0.6294\n",
      "Graph 54: Epoch: 090, Loss: -0.4494\n",
      "Graph 54: Epoch: 091, Loss: -0.5392\n",
      "Graph 54: Epoch: 092, Loss: -0.4499\n",
      "Graph 54: Epoch: 093, Loss: -0.7178\n",
      "Graph 54: Epoch: 094, Loss: -0.4625\n",
      "Graph 54: Epoch: 095, Loss: -0.6289\n",
      "Graph 54: Epoch: 096, Loss: -0.6275\n",
      "Graph 54: Epoch: 097, Loss: -0.3609\n",
      "Graph 54: Epoch: 098, Loss: -0.3606\n",
      "Graph 54: Epoch: 099, Loss: -0.0912\n",
      "Graph 54: Epoch: 100, Loss: -0.3602\n",
      "Graph 55: Epoch: 001, Loss: -0.0248\n",
      "Graph 55: Epoch: 002, Loss: -0.0712\n",
      "Graph 55: Epoch: 003, Loss: -0.2202\n",
      "Graph 55: Epoch: 004, Loss: -0.3552\n",
      "Graph 55: Epoch: 005, Loss: -0.3217\n",
      "Graph 55: Epoch: 006, Loss: -0.4364\n",
      "Graph 55: Epoch: 007, Loss: -0.3919\n",
      "Graph 55: Epoch: 008, Loss: -0.3456\n",
      "Graph 55: Epoch: 009, Loss: -0.3597\n",
      "Graph 55: Epoch: 010, Loss: -0.6938\n",
      "Graph 55: Epoch: 011, Loss: -0.3065\n",
      "Graph 55: Epoch: 012, Loss: -0.3660\n",
      "Graph 55: Epoch: 013, Loss: -0.5288\n",
      "Graph 55: Epoch: 014, Loss: -0.4227\n",
      "Graph 55: Epoch: 015, Loss: -0.5848\n",
      "Graph 55: Epoch: 016, Loss: -0.4142\n",
      "Graph 55: Epoch: 017, Loss: -0.4786\n",
      "Graph 55: Epoch: 018, Loss: -0.3228\n",
      "Graph 55: Epoch: 019, Loss: -0.2674\n",
      "Graph 55: Epoch: 020, Loss: -0.4303\n",
      "Graph 55: Epoch: 021, Loss: -0.3236\n",
      "Graph 55: Epoch: 022, Loss: -0.3254\n",
      "Graph 55: Epoch: 023, Loss: -0.3793\n",
      "Graph 55: Epoch: 024, Loss: -0.5952\n",
      "Graph 55: Epoch: 025, Loss: -0.5922\n",
      "Graph 55: Epoch: 026, Loss: -0.5435\n",
      "Graph 55: Epoch: 027, Loss: -0.4355\n",
      "Graph 55: Epoch: 028, Loss: -0.4899\n",
      "Graph 55: Epoch: 029, Loss: -0.3274\n",
      "Graph 55: Epoch: 030, Loss: -0.4365\n",
      "Graph 55: Epoch: 031, Loss: -0.5466\n",
      "Graph 55: Epoch: 032, Loss: -0.3283\n",
      "Graph 55: Epoch: 033, Loss: -0.3818\n",
      "Graph 55: Epoch: 034, Loss: -0.4922\n",
      "Graph 55: Epoch: 035, Loss: -0.6018\n",
      "Graph 55: Epoch: 036, Loss: -0.6022\n",
      "Graph 55: Epoch: 037, Loss: -0.7673\n",
      "Graph 55: Epoch: 038, Loss: -0.6028\n",
      "Graph 55: Epoch: 039, Loss: -0.4381\n",
      "Graph 55: Epoch: 040, Loss: -0.3283\n",
      "Graph 55: Epoch: 041, Loss: -0.7134\n",
      "Graph 55: Epoch: 042, Loss: -0.6043\n",
      "Graph 55: Epoch: 043, Loss: -0.6043\n",
      "Graph 55: Epoch: 044, Loss: -0.3832\n",
      "Graph 55: Epoch: 045, Loss: -0.5494\n",
      "Graph 55: Epoch: 046, Loss: -0.6051\n",
      "Graph 55: Epoch: 047, Loss: -0.6052\n",
      "Graph 55: Epoch: 048, Loss: -0.4405\n",
      "Graph 55: Epoch: 049, Loss: -0.5505\n",
      "Graph 55: Epoch: 050, Loss: -0.7150\n",
      "Graph 55: Epoch: 051, Loss: -0.7706\n",
      "Graph 55: Epoch: 052, Loss: -0.6060\n",
      "Graph 55: Epoch: 053, Loss: -0.5510\n",
      "Graph 55: Epoch: 054, Loss: -0.4961\n",
      "Graph 55: Epoch: 055, Loss: -0.6061\n",
      "Graph 55: Epoch: 056, Loss: -0.4406\n",
      "Graph 55: Epoch: 057, Loss: -0.3309\n",
      "Graph 55: Epoch: 058, Loss: -0.6618\n",
      "Graph 55: Epoch: 059, Loss: -0.3860\n",
      "Graph 55: Epoch: 060, Loss: -0.3310\n",
      "Graph 55: Epoch: 061, Loss: -0.4963\n",
      "Graph 55: Epoch: 062, Loss: -0.5517\n",
      "Graph 55: Epoch: 063, Loss: -0.5510\n",
      "Graph 55: Epoch: 064, Loss: -0.6071\n",
      "Graph 55: Epoch: 065, Loss: -0.4965\n",
      "Graph 55: Epoch: 066, Loss: -0.4964\n",
      "Graph 55: Epoch: 067, Loss: -0.4416\n",
      "Graph 55: Epoch: 068, Loss: -0.5522\n",
      "Graph 55: Epoch: 069, Loss: -0.3865\n",
      "Graph 55: Epoch: 070, Loss: -0.2211\n",
      "Graph 55: Epoch: 071, Loss: -0.6066\n",
      "Graph 55: Epoch: 072, Loss: -0.4971\n",
      "Graph 55: Epoch: 073, Loss: -0.6077\n",
      "Graph 55: Epoch: 074, Loss: -0.5523\n",
      "Graph 55: Epoch: 075, Loss: -0.6076\n",
      "Graph 55: Epoch: 076, Loss: -0.6628\n",
      "Graph 55: Epoch: 077, Loss: -0.7734\n",
      "Graph 55: Epoch: 078, Loss: -0.5526\n",
      "Graph 55: Epoch: 079, Loss: -0.4421\n",
      "Graph 55: Epoch: 080, Loss: -0.3868\n",
      "Graph 55: Epoch: 081, Loss: -0.6632\n",
      "Graph 55: Epoch: 082, Loss: -0.6080\n",
      "Graph 55: Epoch: 083, Loss: -0.3314\n",
      "Graph 55: Epoch: 084, Loss: -0.3869\n",
      "Graph 55: Epoch: 085, Loss: -0.4424\n",
      "Graph 55: Epoch: 086, Loss: -0.3319\n",
      "Graph 55: Epoch: 087, Loss: -0.6082\n",
      "Graph 55: Epoch: 088, Loss: -0.2765\n",
      "Graph 55: Epoch: 089, Loss: -0.3872\n",
      "Graph 55: Epoch: 090, Loss: -0.6083\n",
      "Graph 55: Epoch: 091, Loss: -0.4423\n",
      "Graph 55: Epoch: 092, Loss: -0.4978\n",
      "Graph 55: Epoch: 093, Loss: -0.6084\n",
      "Graph 55: Epoch: 094, Loss: -0.4978\n",
      "Graph 55: Epoch: 095, Loss: -0.4426\n",
      "Graph 55: Epoch: 096, Loss: -0.6085\n",
      "Graph 55: Epoch: 097, Loss: -0.4426\n",
      "Graph 55: Epoch: 098, Loss: -0.6639\n",
      "Graph 55: Epoch: 099, Loss: -0.4426\n",
      "Graph 55: Epoch: 100, Loss: -0.4427\n",
      "Graph 56: Epoch: 001, Loss: -0.1351\n",
      "Graph 56: Epoch: 002, Loss: -0.1350\n",
      "Graph 56: Epoch: 003, Loss: -0.2463\n",
      "Graph 56: Epoch: 004, Loss: -0.2932\n",
      "Graph 56: Epoch: 005, Loss: -0.2887\n",
      "Graph 56: Epoch: 006, Loss: -0.3169\n",
      "Graph 56: Epoch: 007, Loss: -0.3439\n",
      "Graph 56: Epoch: 008, Loss: -0.3987\n",
      "Graph 56: Epoch: 009, Loss: -0.3767\n",
      "Graph 56: Epoch: 010, Loss: -0.3421\n",
      "Graph 56: Epoch: 011, Loss: -0.3071\n",
      "Graph 56: Epoch: 012, Loss: -0.3350\n",
      "Graph 56: Epoch: 013, Loss: -0.4487\n",
      "Graph 56: Epoch: 014, Loss: -0.3858\n",
      "Graph 56: Epoch: 015, Loss: -0.3568\n",
      "Graph 56: Epoch: 016, Loss: -0.3064\n",
      "Graph 56: Epoch: 017, Loss: -0.3899\n",
      "Graph 56: Epoch: 018, Loss: -0.3973\n",
      "Graph 56: Epoch: 019, Loss: -0.4011\n",
      "Graph 56: Epoch: 020, Loss: -0.4191\n",
      "Graph 56: Epoch: 021, Loss: -0.4007\n",
      "Graph 56: Epoch: 022, Loss: -0.3491\n",
      "Graph 56: Epoch: 023, Loss: -0.3348\n",
      "Graph 56: Epoch: 024, Loss: -0.4278\n",
      "Graph 56: Epoch: 025, Loss: -0.3285\n",
      "Graph 56: Epoch: 026, Loss: -0.4093\n",
      "Graph 56: Epoch: 027, Loss: -0.4122\n",
      "Graph 56: Epoch: 028, Loss: -0.4278\n",
      "Graph 56: Epoch: 029, Loss: -0.4255\n",
      "Graph 56: Epoch: 030, Loss: -0.4276\n",
      "Graph 56: Epoch: 031, Loss: -0.3270\n",
      "Graph 56: Epoch: 032, Loss: -0.3983\n",
      "Graph 56: Epoch: 033, Loss: -0.5051\n",
      "Graph 56: Epoch: 034, Loss: -0.4716\n",
      "Graph 56: Epoch: 035, Loss: -0.4594\n",
      "Graph 56: Epoch: 036, Loss: -0.3821\n",
      "Graph 56: Epoch: 037, Loss: -0.4281\n",
      "Graph 56: Epoch: 038, Loss: -0.4717\n",
      "Graph 56: Epoch: 039, Loss: -0.4129\n",
      "Graph 56: Epoch: 040, Loss: -0.4774\n",
      "Graph 56: Epoch: 041, Loss: -0.4478\n",
      "Graph 56: Epoch: 042, Loss: -0.4683\n",
      "Graph 56: Epoch: 043, Loss: -0.3999\n",
      "Graph 56: Epoch: 044, Loss: -0.4052\n",
      "Graph 56: Epoch: 045, Loss: -0.3462\n",
      "Graph 56: Epoch: 046, Loss: -0.4604\n",
      "Graph 56: Epoch: 047, Loss: -0.3061\n",
      "Graph 56: Epoch: 048, Loss: -0.4651\n",
      "Graph 56: Epoch: 049, Loss: -0.4233\n",
      "Graph 56: Epoch: 050, Loss: -0.4614\n",
      "Graph 56: Epoch: 051, Loss: -0.4874\n",
      "Graph 56: Epoch: 052, Loss: -0.4487\n",
      "Graph 56: Epoch: 053, Loss: -0.5056\n",
      "Graph 56: Epoch: 054, Loss: -0.4079\n",
      "Graph 56: Epoch: 055, Loss: -0.4673\n",
      "Graph 56: Epoch: 056, Loss: -0.3897\n",
      "Graph 56: Epoch: 057, Loss: -0.5037\n",
      "Graph 56: Epoch: 058, Loss: -0.3988\n",
      "Graph 56: Epoch: 059, Loss: -0.4949\n",
      "Graph 56: Epoch: 060, Loss: -0.4499\n",
      "Graph 56: Epoch: 061, Loss: -0.5128\n",
      "Graph 56: Epoch: 062, Loss: -0.4621\n",
      "Graph 56: Epoch: 063, Loss: -0.4111\n",
      "Graph 56: Epoch: 064, Loss: -0.4450\n",
      "Graph 56: Epoch: 065, Loss: -0.4607\n",
      "Graph 56: Epoch: 066, Loss: -0.4574\n",
      "Graph 56: Epoch: 067, Loss: -0.5235\n",
      "Graph 56: Epoch: 068, Loss: -0.3875\n",
      "Graph 56: Epoch: 069, Loss: -0.4458\n",
      "Graph 56: Epoch: 070, Loss: -0.5517\n",
      "Graph 56: Epoch: 071, Loss: -0.5934\n",
      "Graph 56: Epoch: 072, Loss: -0.4736\n",
      "Graph 56: Epoch: 073, Loss: -0.3319\n",
      "Graph 56: Epoch: 074, Loss: -0.4611\n",
      "Graph 56: Epoch: 075, Loss: -0.5597\n",
      "Graph 56: Epoch: 076, Loss: -0.4745\n",
      "Graph 56: Epoch: 077, Loss: -0.1032\n",
      "Graph 56: Epoch: 078, Loss: -0.5887\n",
      "Graph 56: Epoch: 079, Loss: -0.6665\n",
      "Graph 56: Epoch: 080, Loss: -0.7894\n",
      "Graph 56: Epoch: 081, Loss: -0.3797\n",
      "Graph 56: Epoch: 082, Loss: -0.4780\n",
      "Graph 56: Epoch: 083, Loss: -0.8083\n",
      "Graph 56: Epoch: 084, Loss: -0.6016\n",
      "Graph 56: Epoch: 085, Loss: -0.6017\n",
      "Graph 56: Epoch: 086, Loss: -0.3836\n",
      "Graph 56: Epoch: 087, Loss: -0.3161\n",
      "Graph 56: Epoch: 088, Loss: -0.6352\n",
      "Graph 56: Epoch: 089, Loss: -0.7187\n",
      "Graph 56: Epoch: 090, Loss: -0.4885\n",
      "Graph 56: Epoch: 091, Loss: -0.3682\n",
      "Graph 56: Epoch: 092, Loss: -0.8463\n",
      "Graph 56: Epoch: 093, Loss: -0.1336\n",
      "Graph 56: Epoch: 094, Loss: -0.1732\n",
      "Graph 56: Epoch: 095, Loss: -0.4940\n",
      "Graph 56: Epoch: 096, Loss: -0.6124\n",
      "Graph 56: Epoch: 097, Loss: -0.1247\n",
      "Graph 56: Epoch: 098, Loss: -0.3723\n",
      "Graph 56: Epoch: 099, Loss: -0.4935\n",
      "Graph 56: Epoch: 100, Loss: -0.3904\n",
      "Graph 57: Epoch: 001, Loss: -0.0375\n",
      "Graph 57: Epoch: 002, Loss: -0.1499\n",
      "Graph 57: Epoch: 003, Loss: -0.2331\n",
      "Graph 57: Epoch: 004, Loss: -0.2631\n",
      "Graph 57: Epoch: 005, Loss: -0.3191\n",
      "Graph 57: Epoch: 006, Loss: -0.3521\n",
      "Graph 57: Epoch: 007, Loss: -0.3376\n",
      "Graph 57: Epoch: 008, Loss: -0.4647\n",
      "Graph 57: Epoch: 009, Loss: -0.4774\n",
      "Graph 57: Epoch: 010, Loss: -0.5341\n",
      "Graph 57: Epoch: 011, Loss: -0.5451\n",
      "Graph 57: Epoch: 012, Loss: -0.4488\n",
      "Graph 57: Epoch: 013, Loss: -0.3793\n",
      "Graph 57: Epoch: 014, Loss: -0.5173\n",
      "Graph 57: Epoch: 015, Loss: -0.5724\n",
      "Graph 57: Epoch: 016, Loss: -0.3163\n",
      "Graph 57: Epoch: 017, Loss: -0.1608\n",
      "Graph 57: Epoch: 018, Loss: -0.5043\n",
      "Graph 57: Epoch: 019, Loss: -0.5251\n",
      "Graph 57: Epoch: 020, Loss: -0.5326\n",
      "Graph 57: Epoch: 021, Loss: -0.5379\n",
      "Graph 57: Epoch: 022, Loss: -0.5412\n",
      "Graph 57: Epoch: 023, Loss: -0.5959\n",
      "Graph 57: Epoch: 024, Loss: -0.7060\n",
      "Graph 57: Epoch: 025, Loss: -0.4340\n",
      "Graph 57: Epoch: 026, Loss: -0.6486\n",
      "Graph 57: Epoch: 027, Loss: -0.4900\n",
      "Graph 57: Epoch: 028, Loss: -0.4893\n",
      "Graph 57: Epoch: 029, Loss: -0.6003\n",
      "Graph 57: Epoch: 030, Loss: -0.6012\n",
      "Graph 57: Epoch: 031, Loss: -0.4923\n",
      "Graph 57: Epoch: 032, Loss: -0.3834\n",
      "Graph 57: Epoch: 033, Loss: -0.4376\n",
      "Graph 57: Epoch: 034, Loss: -0.3818\n",
      "Graph 57: Epoch: 035, Loss: -0.6550\n",
      "Graph 57: Epoch: 036, Loss: -0.5485\n",
      "Graph 57: Epoch: 037, Loss: -0.4397\n",
      "Graph 57: Epoch: 038, Loss: -0.4395\n",
      "Graph 57: Epoch: 039, Loss: -0.4397\n",
      "Graph 57: Epoch: 040, Loss: -0.4932\n",
      "Graph 57: Epoch: 041, Loss: -0.3279\n",
      "Graph 57: Epoch: 042, Loss: -0.4398\n",
      "Graph 57: Epoch: 043, Loss: -0.5501\n",
      "Graph 57: Epoch: 044, Loss: -0.2198\n",
      "Graph 57: Epoch: 045, Loss: -0.6601\n",
      "Graph 57: Epoch: 046, Loss: -0.4394\n",
      "Graph 57: Epoch: 047, Loss: -0.4406\n",
      "Graph 57: Epoch: 048, Loss: -0.4406\n",
      "Graph 57: Epoch: 049, Loss: -0.4404\n",
      "Graph 57: Epoch: 050, Loss: -0.4955\n",
      "Graph 57: Epoch: 051, Loss: -0.4399\n",
      "Graph 57: Epoch: 052, Loss: -0.4952\n",
      "Graph 57: Epoch: 053, Loss: -0.4939\n",
      "Graph 57: Epoch: 054, Loss: -0.5507\n",
      "Graph 57: Epoch: 055, Loss: -0.4408\n",
      "Graph 57: Epoch: 056, Loss: -0.5494\n",
      "Graph 57: Epoch: 057, Loss: -0.6065\n",
      "Graph 57: Epoch: 058, Loss: -0.4960\n",
      "Graph 57: Epoch: 059, Loss: -0.5511\n",
      "Graph 57: Epoch: 060, Loss: -0.3305\n",
      "Graph 57: Epoch: 061, Loss: -0.3859\n",
      "Graph 57: Epoch: 062, Loss: -0.2209\n",
      "Graph 57: Epoch: 063, Loss: -0.3310\n",
      "Graph 57: Epoch: 064, Loss: -0.5516\n",
      "Graph 57: Epoch: 065, Loss: -0.4414\n",
      "Graph 57: Epoch: 066, Loss: -0.4965\n",
      "Graph 57: Epoch: 067, Loss: -0.3313\n",
      "Graph 57: Epoch: 068, Loss: -0.3865\n",
      "Graph 57: Epoch: 069, Loss: -0.5516\n",
      "Graph 57: Epoch: 070, Loss: -0.4971\n",
      "Graph 57: Epoch: 071, Loss: -0.4415\n",
      "Graph 57: Epoch: 072, Loss: -0.7728\n",
      "Graph 57: Epoch: 073, Loss: -0.5520\n",
      "Graph 57: Epoch: 074, Loss: -0.3864\n",
      "Graph 57: Epoch: 075, Loss: -0.4971\n",
      "Graph 57: Epoch: 076, Loss: -0.5521\n",
      "Graph 57: Epoch: 077, Loss: -0.5524\n",
      "Graph 57: Epoch: 078, Loss: -0.4969\n",
      "Graph 57: Epoch: 079, Loss: -0.3867\n",
      "Graph 57: Epoch: 080, Loss: -0.4973\n",
      "Graph 57: Epoch: 081, Loss: -0.3314\n",
      "Graph 57: Epoch: 082, Loss: -0.2763\n",
      "Graph 57: Epoch: 083, Loss: -0.3316\n",
      "Graph 57: Epoch: 084, Loss: -0.3870\n",
      "Graph 57: Epoch: 085, Loss: -0.6634\n",
      "Graph 57: Epoch: 086, Loss: -0.6080\n",
      "Graph 57: Epoch: 087, Loss: -0.3317\n",
      "Graph 57: Epoch: 088, Loss: -0.3318\n",
      "Graph 57: Epoch: 089, Loss: -0.7180\n",
      "Graph 57: Epoch: 090, Loss: -0.3318\n",
      "Graph 57: Epoch: 091, Loss: -0.8842\n",
      "Graph 57: Epoch: 092, Loss: -0.6078\n",
      "Graph 57: Epoch: 093, Loss: -0.3318\n",
      "Graph 57: Epoch: 094, Loss: -0.3318\n",
      "Graph 57: Epoch: 095, Loss: -0.3872\n",
      "Graph 57: Epoch: 096, Loss: -0.6083\n",
      "Graph 57: Epoch: 097, Loss: -0.4423\n",
      "Graph 57: Epoch: 098, Loss: -0.6636\n",
      "Graph 57: Epoch: 099, Loss: -0.4978\n",
      "Graph 57: Epoch: 100, Loss: -0.4423\n",
      "Graph 58: Epoch: 001, Loss: -0.5000\n",
      "Graph 58: Epoch: 002, Loss: -0.4950\n",
      "Graph 58: Epoch: 003, Loss: -0.1172\n",
      "Graph 58: Epoch: 004, Loss: -0.1191\n",
      "Graph 58: Epoch: 005, Loss: -0.4980\n",
      "Graph 58: Epoch: 006, Loss: -0.8766\n",
      "Graph 58: Epoch: 007, Loss: -0.4965\n",
      "Graph 58: Epoch: 008, Loss: -0.5045\n",
      "Graph 58: Epoch: 009, Loss: -0.1218\n",
      "Graph 58: Epoch: 010, Loss: -0.5075\n",
      "Graph 58: Epoch: 011, Loss: -0.4906\n",
      "Graph 58: Epoch: 012, Loss: -0.4898\n",
      "Graph 58: Epoch: 013, Loss: -0.5100\n",
      "Graph 58: Epoch: 014, Loss: -0.4894\n",
      "Graph 58: Epoch: 015, Loss: -0.5103\n",
      "Graph 58: Epoch: 016, Loss: -0.1236\n",
      "Graph 58: Epoch: 017, Loss: -0.4890\n",
      "Graph 58: Epoch: 018, Loss: -0.5105\n",
      "Graph 58: Epoch: 019, Loss: -0.4894\n",
      "Graph 58: Epoch: 020, Loss: -0.5102\n",
      "Graph 58: Epoch: 021, Loss: -0.8729\n",
      "Graph 58: Epoch: 022, Loss: -0.1269\n",
      "Graph 58: Epoch: 023, Loss: -0.5110\n",
      "Graph 58: Epoch: 024, Loss: -0.4883\n",
      "Graph 58: Epoch: 025, Loss: -0.4882\n",
      "Graph 58: Epoch: 026, Loss: -0.4888\n",
      "Graph 58: Epoch: 027, Loss: -0.5101\n",
      "Graph 58: Epoch: 028, Loss: -0.5097\n",
      "Graph 58: Epoch: 029, Loss: -0.4900\n",
      "Graph 58: Epoch: 030, Loss: -0.4904\n",
      "Graph 58: Epoch: 031, Loss: -0.5087\n",
      "Graph 58: Epoch: 032, Loss: -0.5084\n",
      "Graph 58: Epoch: 033, Loss: -0.4912\n",
      "Graph 58: Epoch: 034, Loss: -0.4914\n",
      "Graph 58: Epoch: 035, Loss: -0.1317\n",
      "Graph 58: Epoch: 036, Loss: -0.4932\n",
      "Graph 58: Epoch: 037, Loss: -0.4946\n",
      "Graph 58: Epoch: 038, Loss: -0.4965\n",
      "Graph 58: Epoch: 039, Loss: -0.5013\n",
      "Graph 58: Epoch: 040, Loss: -0.5001\n",
      "Graph 58: Epoch: 041, Loss: -0.5019\n",
      "Graph 58: Epoch: 042, Loss: -0.8592\n",
      "Graph 58: Epoch: 043, Loss: -0.4942\n",
      "Graph 58: Epoch: 044, Loss: -0.5068\n",
      "Graph 58: Epoch: 045, Loss: -0.5083\n",
      "Graph 58: Epoch: 046, Loss: -0.4899\n",
      "Graph 58: Epoch: 047, Loss: -0.4888\n",
      "Graph 58: Epoch: 048, Loss: -0.5117\n",
      "Graph 58: Epoch: 049, Loss: -0.4874\n",
      "Graph 58: Epoch: 050, Loss: -0.4871\n",
      "Graph 58: Epoch: 051, Loss: -0.4874\n",
      "Graph 58: Epoch: 052, Loss: -0.4882\n",
      "Graph 58: Epoch: 053, Loss: -0.4894\n",
      "Graph 58: Epoch: 054, Loss: -0.5089\n",
      "Graph 58: Epoch: 055, Loss: -0.8593\n",
      "Graph 58: Epoch: 056, Loss: -0.4931\n",
      "Graph 58: Epoch: 057, Loss: -0.5053\n",
      "Graph 58: Epoch: 058, Loss: -0.4955\n",
      "Graph 58: Epoch: 059, Loss: -0.5032\n",
      "Graph 58: Epoch: 060, Loss: -0.5025\n",
      "Graph 58: Epoch: 061, Loss: -0.5025\n",
      "Graph 58: Epoch: 062, Loss: -0.4970\n",
      "Graph 58: Epoch: 063, Loss: -0.4971\n",
      "Graph 58: Epoch: 064, Loss: -0.5023\n",
      "Graph 58: Epoch: 065, Loss: -0.4977\n",
      "Graph 58: Epoch: 066, Loss: -0.4983\n",
      "Graph 58: Epoch: 067, Loss: -0.1322\n",
      "Graph 58: Epoch: 068, Loss: -0.5000\n",
      "Graph 58: Epoch: 069, Loss: -0.5011\n",
      "Graph 58: Epoch: 070, Loss: -0.4973\n",
      "Graph 58: Epoch: 071, Loss: -0.1316\n",
      "Graph 58: Epoch: 072, Loss: -0.4959\n",
      "Graph 58: Epoch: 073, Loss: -0.8670\n",
      "Graph 58: Epoch: 074, Loss: -0.8670\n",
      "Graph 58: Epoch: 075, Loss: -0.5047\n",
      "Graph 58: Epoch: 076, Loss: -0.4944\n",
      "Graph 58: Epoch: 077, Loss: -0.4941\n",
      "Graph 58: Epoch: 078, Loss: -0.4944\n",
      "Graph 58: Epoch: 079, Loss: -0.8692\n",
      "Graph 58: Epoch: 080, Loss: -0.5044\n",
      "Graph 58: Epoch: 081, Loss: -0.4955\n",
      "Graph 58: Epoch: 082, Loss: -0.8711\n",
      "Graph 58: Epoch: 083, Loss: -0.4960\n",
      "Graph 58: Epoch: 084, Loss: -0.4967\n",
      "Graph 58: Epoch: 085, Loss: -0.4979\n",
      "Graph 58: Epoch: 086, Loss: -0.4994\n",
      "Graph 58: Epoch: 087, Loss: -0.4986\n",
      "Graph 58: Epoch: 088, Loss: -0.5026\n",
      "Graph 58: Epoch: 089, Loss: -0.8730\n",
      "Graph 58: Epoch: 090, Loss: -0.4945\n",
      "Graph 58: Epoch: 091, Loss: -0.4939\n",
      "Graph 58: Epoch: 092, Loss: -0.5060\n",
      "Graph 58: Epoch: 093, Loss: -0.4935\n",
      "Graph 58: Epoch: 094, Loss: -0.4935\n",
      "Graph 58: Epoch: 095, Loss: -0.4941\n",
      "Graph 58: Epoch: 096, Loss: -0.5048\n",
      "Graph 58: Epoch: 097, Loss: -0.1213\n",
      "Graph 58: Epoch: 098, Loss: -0.5042\n",
      "Graph 58: Epoch: 099, Loss: -0.4954\n",
      "Graph 58: Epoch: 100, Loss: -0.5044\n",
      "Graph 59: Epoch: 001, Loss: -0.0417\n",
      "Graph 59: Epoch: 002, Loss: -0.1863\n",
      "Graph 59: Epoch: 003, Loss: -0.1580\n",
      "Graph 59: Epoch: 004, Loss: -0.3753\n",
      "Graph 59: Epoch: 005, Loss: -0.3470\n",
      "Graph 59: Epoch: 006, Loss: -0.2677\n",
      "Graph 59: Epoch: 007, Loss: -0.4929\n",
      "Graph 59: Epoch: 008, Loss: -0.2151\n",
      "Graph 59: Epoch: 009, Loss: -0.4041\n",
      "Graph 59: Epoch: 010, Loss: -0.4940\n",
      "Graph 59: Epoch: 011, Loss: -0.4584\n",
      "Graph 59: Epoch: 012, Loss: -0.5613\n",
      "Graph 59: Epoch: 013, Loss: -0.2469\n",
      "Graph 59: Epoch: 014, Loss: -0.4907\n",
      "Graph 59: Epoch: 015, Loss: -0.3281\n",
      "Graph 59: Epoch: 016, Loss: -0.3247\n",
      "Graph 59: Epoch: 017, Loss: -0.4895\n",
      "Graph 59: Epoch: 018, Loss: -0.4739\n",
      "Graph 59: Epoch: 019, Loss: -0.2783\n",
      "Graph 59: Epoch: 020, Loss: -0.4873\n",
      "Graph 59: Epoch: 021, Loss: -0.5973\n",
      "Graph 59: Epoch: 022, Loss: -0.5322\n",
      "Graph 59: Epoch: 023, Loss: -0.6015\n",
      "Graph 59: Epoch: 024, Loss: -0.4255\n",
      "Graph 59: Epoch: 025, Loss: -0.5031\n",
      "Graph 59: Epoch: 026, Loss: -0.5139\n",
      "Graph 59: Epoch: 027, Loss: -0.3387\n",
      "Graph 59: Epoch: 028, Loss: -0.5841\n",
      "Graph 59: Epoch: 029, Loss: -0.6944\n",
      "Graph 59: Epoch: 030, Loss: -0.4350\n",
      "Graph 59: Epoch: 031, Loss: -0.2632\n",
      "Graph 59: Epoch: 032, Loss: -0.4381\n",
      "Graph 59: Epoch: 033, Loss: -0.4374\n",
      "Graph 59: Epoch: 034, Loss: -0.4327\n",
      "Graph 59: Epoch: 035, Loss: -0.5262\n",
      "Graph 59: Epoch: 036, Loss: -0.4410\n",
      "Graph 59: Epoch: 037, Loss: -0.2655\n",
      "Graph 59: Epoch: 038, Loss: -0.2658\n",
      "Graph 59: Epoch: 039, Loss: -0.5292\n",
      "Graph 59: Epoch: 040, Loss: -0.6167\n",
      "Graph 59: Epoch: 041, Loss: -0.4420\n",
      "Graph 59: Epoch: 042, Loss: -0.8807\n",
      "Graph 59: Epoch: 043, Loss: -0.5235\n",
      "Graph 59: Epoch: 044, Loss: -0.3542\n",
      "Graph 59: Epoch: 045, Loss: -0.7080\n",
      "Graph 59: Epoch: 046, Loss: -0.3529\n",
      "Graph 59: Epoch: 047, Loss: -0.4334\n",
      "Graph 59: Epoch: 048, Loss: -0.4437\n",
      "Graph 59: Epoch: 049, Loss: -0.6209\n",
      "Graph 59: Epoch: 050, Loss: -0.4440\n",
      "Graph 59: Epoch: 051, Loss: -0.5331\n",
      "Graph 59: Epoch: 052, Loss: -0.2573\n",
      "Graph 59: Epoch: 053, Loss: -0.5351\n",
      "Graph 59: Epoch: 054, Loss: -0.3571\n",
      "Graph 59: Epoch: 055, Loss: -0.5351\n",
      "Graph 59: Epoch: 056, Loss: -0.4691\n",
      "Graph 59: Epoch: 057, Loss: -0.5358\n",
      "Graph 59: Epoch: 058, Loss: -0.6247\n",
      "Graph 59: Epoch: 059, Loss: -0.2691\n",
      "Graph 59: Epoch: 060, Loss: -0.3563\n",
      "Graph 59: Epoch: 061, Loss: -0.6229\n",
      "Graph 59: Epoch: 062, Loss: -0.3587\n",
      "Graph 59: Epoch: 063, Loss: -0.4474\n",
      "Graph 59: Epoch: 064, Loss: -0.6263\n",
      "Graph 59: Epoch: 065, Loss: -0.2687\n",
      "Graph 59: Epoch: 066, Loss: -0.4478\n",
      "Graph 59: Epoch: 067, Loss: -0.6243\n",
      "Graph 59: Epoch: 068, Loss: -0.2672\n",
      "Graph 59: Epoch: 069, Loss: -0.6266\n",
      "Graph 59: Epoch: 070, Loss: -0.6268\n",
      "Graph 59: Epoch: 071, Loss: -0.5373\n",
      "Graph 59: Epoch: 072, Loss: -0.5374\n",
      "Graph 59: Epoch: 073, Loss: -0.5382\n",
      "Graph 59: Epoch: 074, Loss: -0.7175\n",
      "Graph 59: Epoch: 075, Loss: -0.7164\n",
      "Graph 59: Epoch: 076, Loss: -0.8071\n",
      "Graph 59: Epoch: 077, Loss: -0.7177\n",
      "Graph 59: Epoch: 078, Loss: -0.2700\n",
      "Graph 59: Epoch: 079, Loss: -0.5388\n",
      "Graph 59: Epoch: 080, Loss: -0.7168\n",
      "Graph 59: Epoch: 081, Loss: -0.3596\n",
      "Graph 59: Epoch: 082, Loss: -0.5381\n",
      "Graph 59: Epoch: 083, Loss: -0.6278\n",
      "Graph 59: Epoch: 084, Loss: -0.5396\n",
      "Graph 59: Epoch: 085, Loss: -0.6287\n",
      "Graph 59: Epoch: 086, Loss: -0.7186\n",
      "Graph 59: Epoch: 087, Loss: -0.8092\n",
      "Graph 59: Epoch: 088, Loss: -0.5397\n",
      "Graph 59: Epoch: 089, Loss: -0.6287\n",
      "Graph 59: Epoch: 090, Loss: -0.3603\n",
      "Graph 59: Epoch: 091, Loss: -0.7191\n",
      "Graph 59: Epoch: 092, Loss: -0.1743\n",
      "Graph 59: Epoch: 093, Loss: -0.5397\n",
      "Graph 59: Epoch: 094, Loss: -0.6299\n",
      "Graph 59: Epoch: 095, Loss: -0.7198\n",
      "Graph 59: Epoch: 096, Loss: -0.5405\n",
      "Graph 59: Epoch: 097, Loss: -0.4499\n",
      "Graph 59: Epoch: 098, Loss: -0.4505\n",
      "Graph 59: Epoch: 099, Loss: -0.5405\n",
      "Graph 59: Epoch: 100, Loss: -0.4622\n",
      "Graph 60: Epoch: 001, Loss: -0.1465\n",
      "Graph 60: Epoch: 002, Loss: -0.1524\n",
      "Graph 60: Epoch: 003, Loss: -0.3644\n",
      "Graph 60: Epoch: 004, Loss: -0.1492\n",
      "Graph 60: Epoch: 005, Loss: -0.2336\n",
      "Graph 60: Epoch: 006, Loss: -0.3962\n",
      "Graph 60: Epoch: 007, Loss: -0.2557\n",
      "Graph 60: Epoch: 008, Loss: -0.3913\n",
      "Graph 60: Epoch: 009, Loss: -0.0771\n",
      "Graph 60: Epoch: 010, Loss: -0.4171\n",
      "Graph 60: Epoch: 011, Loss: -0.2150\n",
      "Graph 60: Epoch: 012, Loss: -0.4274\n",
      "Graph 60: Epoch: 013, Loss: -0.4922\n",
      "Graph 60: Epoch: 014, Loss: -0.4969\n",
      "Graph 60: Epoch: 015, Loss: -0.3534\n",
      "Graph 60: Epoch: 016, Loss: -0.4876\n",
      "Graph 60: Epoch: 017, Loss: -0.2068\n",
      "Graph 60: Epoch: 018, Loss: -0.5213\n",
      "Graph 60: Epoch: 019, Loss: -0.6284\n",
      "Graph 60: Epoch: 020, Loss: -0.0886\n",
      "Graph 60: Epoch: 021, Loss: -0.4182\n",
      "Graph 60: Epoch: 022, Loss: -0.2360\n",
      "Graph 60: Epoch: 023, Loss: -0.2332\n",
      "Graph 60: Epoch: 024, Loss: -0.4380\n",
      "Graph 60: Epoch: 025, Loss: -0.4090\n",
      "Graph 60: Epoch: 026, Loss: -0.5792\n",
      "Graph 60: Epoch: 027, Loss: -0.4900\n",
      "Graph 60: Epoch: 028, Loss: -0.4584\n",
      "Graph 60: Epoch: 029, Loss: -0.3690\n",
      "Graph 60: Epoch: 030, Loss: -0.3882\n",
      "Graph 60: Epoch: 031, Loss: -0.4197\n",
      "Graph 60: Epoch: 032, Loss: -0.3076\n",
      "Graph 60: Epoch: 033, Loss: -0.4469\n",
      "Graph 60: Epoch: 034, Loss: -0.7237\n",
      "Graph 60: Epoch: 035, Loss: -0.4539\n",
      "Graph 60: Epoch: 036, Loss: -0.5177\n",
      "Graph 60: Epoch: 037, Loss: -0.4830\n",
      "Graph 60: Epoch: 038, Loss: -0.5123\n",
      "Graph 60: Epoch: 039, Loss: -0.3462\n",
      "Graph 60: Epoch: 040, Loss: -0.5300\n",
      "Graph 60: Epoch: 041, Loss: -0.5131\n",
      "Graph 60: Epoch: 042, Loss: -0.3104\n",
      "Graph 60: Epoch: 043, Loss: -0.6966\n",
      "Graph 60: Epoch: 044, Loss: -0.6226\n",
      "Graph 60: Epoch: 045, Loss: -0.5683\n",
      "Graph 60: Epoch: 046, Loss: -0.5470\n",
      "Graph 60: Epoch: 047, Loss: -0.2752\n",
      "Graph 60: Epoch: 048, Loss: -0.0301\n",
      "Graph 60: Epoch: 049, Loss: -0.3386\n",
      "Graph 60: Epoch: 050, Loss: -0.5322\n",
      "Graph 60: Epoch: 051, Loss: -0.0950\n",
      "Graph 60: Epoch: 052, Loss: -0.3210\n",
      "Graph 60: Epoch: 053, Loss: -0.3210\n",
      "Graph 60: Epoch: 054, Loss: -0.1653\n",
      "Graph 60: Epoch: 055, Loss: -0.4320\n",
      "Graph 60: Epoch: 056, Loss: -0.6254\n",
      "Graph 60: Epoch: 057, Loss: -0.5660\n",
      "Graph 60: Epoch: 058, Loss: -0.7026\n",
      "Graph 60: Epoch: 059, Loss: -0.5152\n",
      "Graph 60: Epoch: 060, Loss: -0.6154\n",
      "Graph 60: Epoch: 061, Loss: -0.3559\n",
      "Graph 60: Epoch: 062, Loss: -0.4151\n",
      "Graph 60: Epoch: 063, Loss: -0.3223\n",
      "Graph 60: Epoch: 064, Loss: -0.3315\n",
      "Graph 60: Epoch: 065, Loss: -0.0605\n",
      "Graph 60: Epoch: 066, Loss: -0.4165\n",
      "Graph 60: Epoch: 067, Loss: -0.5933\n",
      "Graph 60: Epoch: 068, Loss: -0.6240\n",
      "Graph 60: Epoch: 069, Loss: -0.3899\n",
      "Graph 60: Epoch: 070, Loss: -0.1589\n",
      "Graph 60: Epoch: 071, Loss: -0.7776\n",
      "Graph 60: Epoch: 072, Loss: -0.8826\n",
      "Graph 60: Epoch: 073, Loss: -0.7452\n",
      "Graph 60: Epoch: 074, Loss: -0.4987\n",
      "Graph 60: Epoch: 075, Loss: -0.0567\n",
      "Graph 60: Epoch: 076, Loss: -0.9355\n",
      "Graph 60: Epoch: 077, Loss: -0.4169\n",
      "Graph 60: Epoch: 078, Loss: -0.6340\n",
      "Graph 60: Epoch: 079, Loss: -0.8928\n",
      "Graph 60: Epoch: 080, Loss: -0.1791\n",
      "Graph 60: Epoch: 081, Loss: -0.9337\n",
      "Graph 60: Epoch: 082, Loss: -0.3857\n",
      "Graph 60: Epoch: 083, Loss: -0.0928\n",
      "Graph 60: Epoch: 084, Loss: -0.1759\n",
      "Graph 60: Epoch: 085, Loss: -0.6265\n",
      "Graph 60: Epoch: 086, Loss: -0.3247\n",
      "Graph 60: Epoch: 087, Loss: -0.7144\n",
      "Graph 60: Epoch: 088, Loss: -0.5242\n",
      "Graph 60: Epoch: 089, Loss: -0.1283\n",
      "Graph 60: Epoch: 090, Loss: -0.6983\n",
      "Graph 60: Epoch: 091, Loss: -0.3736\n",
      "Graph 60: Epoch: 092, Loss: -0.7061\n",
      "Graph 60: Epoch: 093, Loss: -0.1394\n",
      "Graph 60: Epoch: 094, Loss: -0.6343\n",
      "Graph 60: Epoch: 095, Loss: -0.4909\n",
      "Graph 60: Epoch: 096, Loss: -0.0222\n",
      "Graph 60: Epoch: 097, Loss: -0.3365\n",
      "Graph 60: Epoch: 098, Loss: -0.3276\n",
      "Graph 60: Epoch: 099, Loss: -0.6409\n",
      "Graph 60: Epoch: 100, Loss: -0.6336\n",
      "Graph 61: Epoch: 001, Loss: -0.5000\n",
      "Graph 61: Epoch: 002, Loss: -0.1213\n",
      "Graph 61: Epoch: 003, Loss: -0.4905\n",
      "Graph 61: Epoch: 004, Loss: -0.4900\n",
      "Graph 61: Epoch: 005, Loss: -0.4914\n",
      "Graph 61: Epoch: 006, Loss: -0.4937\n",
      "Graph 61: Epoch: 007, Loss: -0.5033\n",
      "Graph 61: Epoch: 008, Loss: -0.4980\n",
      "Graph 61: Epoch: 009, Loss: -0.5000\n",
      "Graph 61: Epoch: 010, Loss: -0.5026\n",
      "Graph 61: Epoch: 011, Loss: -0.5055\n",
      "Graph 61: Epoch: 012, Loss: -0.5087\n",
      "Graph 61: Epoch: 013, Loss: -0.5123\n",
      "Graph 61: Epoch: 014, Loss: -0.5160\n",
      "Graph 61: Epoch: 015, Loss: -0.5199\n",
      "Graph 61: Epoch: 016, Loss: -0.4761\n",
      "Graph 61: Epoch: 017, Loss: -0.5268\n",
      "Graph 61: Epoch: 018, Loss: -0.4700\n",
      "Graph 61: Epoch: 019, Loss: -0.5322\n",
      "Graph 61: Epoch: 020, Loss: -0.4653\n",
      "Graph 61: Epoch: 021, Loss: -0.4636\n",
      "Graph 61: Epoch: 022, Loss: -0.4627\n",
      "Graph 61: Epoch: 023, Loss: -0.4625\n",
      "Graph 61: Epoch: 024, Loss: -0.1478\n",
      "Graph 61: Epoch: 025, Loss: -0.5372\n",
      "Graph 61: Epoch: 026, Loss: -0.4623\n",
      "Graph 61: Epoch: 027, Loss: -0.4624\n",
      "Graph 61: Epoch: 028, Loss: -0.1517\n",
      "Graph 61: Epoch: 029, Loss: -0.1530\n",
      "Graph 61: Epoch: 030, Loss: -0.1551\n",
      "Graph 61: Epoch: 031, Loss: -0.4629\n",
      "Graph 61: Epoch: 032, Loss: -0.4632\n",
      "Graph 61: Epoch: 033, Loss: -0.1256\n",
      "Graph 61: Epoch: 034, Loss: -0.5351\n",
      "Graph 61: Epoch: 035, Loss: -0.4652\n",
      "Graph 61: Epoch: 036, Loss: -0.4660\n",
      "Graph 61: Epoch: 037, Loss: -0.4673\n",
      "Graph 61: Epoch: 038, Loss: -0.8643\n",
      "Graph 61: Epoch: 039, Loss: -0.5298\n",
      "Graph 61: Epoch: 040, Loss: -0.5292\n",
      "Graph 61: Epoch: 041, Loss: -0.5292\n",
      "Graph 61: Epoch: 042, Loss: -0.1385\n",
      "Graph 61: Epoch: 043, Loss: -0.4700\n",
      "Graph 61: Epoch: 044, Loss: -0.5297\n",
      "Graph 61: Epoch: 045, Loss: -0.4701\n",
      "Graph 61: Epoch: 046, Loss: -0.5296\n",
      "Graph 61: Epoch: 047, Loss: -0.8577\n",
      "Graph 61: Epoch: 048, Loss: -0.8579\n",
      "Graph 61: Epoch: 049, Loss: -0.5311\n",
      "Graph 61: Epoch: 050, Loss: -0.4678\n",
      "Graph 61: Epoch: 051, Loss: -0.4672\n",
      "Graph 61: Epoch: 052, Loss: -0.4673\n",
      "Graph 61: Epoch: 053, Loss: -0.5321\n",
      "Graph 61: Epoch: 054, Loss: -0.4679\n",
      "Graph 61: Epoch: 055, Loss: -0.5315\n",
      "Graph 61: Epoch: 056, Loss: -0.5316\n",
      "Graph 61: Epoch: 057, Loss: -0.4678\n",
      "Graph 61: Epoch: 058, Loss: -0.5322\n",
      "Graph 61: Epoch: 059, Loss: -0.4673\n",
      "Graph 61: Epoch: 060, Loss: -0.4674\n",
      "Graph 61: Epoch: 061, Loss: -0.8629\n",
      "Graph 61: Epoch: 062, Loss: -0.4683\n",
      "Graph 61: Epoch: 063, Loss: -0.5310\n",
      "Graph 61: Epoch: 064, Loss: -0.4692\n",
      "Graph 61: Epoch: 065, Loss: -0.5301\n",
      "Graph 61: Epoch: 066, Loss: -0.1352\n",
      "Graph 61: Epoch: 067, Loss: -0.5297\n",
      "Graph 61: Epoch: 068, Loss: -0.5299\n",
      "Graph 61: Epoch: 069, Loss: -0.8647\n",
      "Graph 61: Epoch: 070, Loss: -0.4684\n",
      "Graph 61: Epoch: 071, Loss: -0.4681\n",
      "Graph 61: Epoch: 072, Loss: -0.5316\n",
      "Graph 61: Epoch: 073, Loss: -0.5319\n",
      "Graph 61: Epoch: 074, Loss: -0.4673\n",
      "Graph 61: Epoch: 075, Loss: -0.5329\n",
      "Graph 61: Epoch: 076, Loss: -0.5336\n",
      "Graph 61: Epoch: 077, Loss: -0.4652\n",
      "Graph 61: Epoch: 078, Loss: -0.4647\n",
      "Graph 61: Epoch: 079, Loss: -0.5352\n",
      "Graph 61: Epoch: 080, Loss: -0.5357\n",
      "Graph 61: Epoch: 081, Loss: -0.5367\n",
      "Graph 61: Epoch: 082, Loss: -0.4620\n",
      "Graph 61: Epoch: 083, Loss: -0.5388\n",
      "Graph 61: Epoch: 084, Loss: -0.4601\n",
      "Graph 61: Epoch: 085, Loss: -0.4595\n",
      "Graph 61: Epoch: 086, Loss: -0.1272\n",
      "Graph 61: Epoch: 087, Loss: -0.1278\n",
      "Graph 61: Epoch: 088, Loss: -0.8711\n",
      "Graph 61: Epoch: 089, Loss: -0.4606\n",
      "Graph 61: Epoch: 090, Loss: -0.4613\n",
      "Graph 61: Epoch: 091, Loss: -0.1306\n",
      "Graph 61: Epoch: 092, Loss: -0.5362\n",
      "Graph 61: Epoch: 093, Loss: -0.4645\n",
      "Graph 61: Epoch: 094, Loss: -0.5344\n",
      "Graph 61: Epoch: 095, Loss: -0.8652\n",
      "Graph 61: Epoch: 096, Loss: -0.5338\n",
      "Graph 61: Epoch: 097, Loss: -0.4658\n",
      "Graph 61: Epoch: 098, Loss: -0.5340\n",
      "Graph 61: Epoch: 099, Loss: -0.5343\n",
      "Graph 61: Epoch: 100, Loss: -0.8660\n",
      "Graph 62: Epoch: 001, Loss: -0.0331\n",
      "Graph 62: Epoch: 002, Loss: -0.1373\n",
      "Graph 62: Epoch: 003, Loss: -0.2281\n",
      "Graph 62: Epoch: 004, Loss: -0.2876\n",
      "Graph 62: Epoch: 005, Loss: -0.3703\n",
      "Graph 62: Epoch: 006, Loss: -0.4417\n",
      "Graph 62: Epoch: 007, Loss: -0.4361\n",
      "Graph 62: Epoch: 008, Loss: -0.4505\n",
      "Graph 62: Epoch: 009, Loss: -0.4370\n",
      "Graph 62: Epoch: 010, Loss: -0.4453\n",
      "Graph 62: Epoch: 011, Loss: -0.3730\n",
      "Graph 62: Epoch: 012, Loss: -0.3079\n",
      "Graph 62: Epoch: 013, Loss: -0.4896\n",
      "Graph 62: Epoch: 014, Loss: -0.5041\n",
      "Graph 62: Epoch: 015, Loss: -0.4121\n",
      "Graph 62: Epoch: 016, Loss: -0.4188\n",
      "Graph 62: Epoch: 017, Loss: -0.2548\n",
      "Graph 62: Epoch: 018, Loss: -0.5708\n",
      "Graph 62: Epoch: 019, Loss: -0.4656\n",
      "Graph 62: Epoch: 020, Loss: -0.6729\n",
      "Graph 62: Epoch: 021, Loss: -0.4257\n",
      "Graph 62: Epoch: 022, Loss: -0.4286\n",
      "Graph 62: Epoch: 023, Loss: -0.5148\n",
      "Graph 62: Epoch: 024, Loss: -0.3464\n",
      "Graph 62: Epoch: 025, Loss: -0.4334\n",
      "Graph 62: Epoch: 026, Loss: -0.4351\n",
      "Graph 62: Epoch: 027, Loss: -0.5164\n",
      "Graph 62: Epoch: 028, Loss: -0.7796\n",
      "Graph 62: Epoch: 029, Loss: -0.4360\n",
      "Graph 62: Epoch: 030, Loss: -0.2635\n",
      "Graph 62: Epoch: 031, Loss: -0.1776\n",
      "Graph 62: Epoch: 032, Loss: -0.4372\n",
      "Graph 62: Epoch: 033, Loss: -0.4395\n",
      "Graph 62: Epoch: 034, Loss: -0.4328\n",
      "Graph 62: Epoch: 035, Loss: -0.3538\n",
      "Graph 62: Epoch: 036, Loss: -0.3503\n",
      "Graph 62: Epoch: 037, Loss: -0.4385\n",
      "Graph 62: Epoch: 038, Loss: -0.3548\n",
      "Graph 62: Epoch: 039, Loss: -0.6174\n",
      "Graph 62: Epoch: 040, Loss: -0.2668\n",
      "Graph 62: Epoch: 041, Loss: -0.6136\n",
      "Graph 62: Epoch: 042, Loss: -0.4405\n",
      "Graph 62: Epoch: 043, Loss: -0.5302\n",
      "Graph 62: Epoch: 044, Loss: -0.3554\n",
      "Graph 62: Epoch: 045, Loss: -0.6902\n",
      "Graph 62: Epoch: 046, Loss: -0.5286\n",
      "Graph 62: Epoch: 047, Loss: -0.6194\n",
      "Graph 62: Epoch: 048, Loss: -0.0899\n",
      "Graph 62: Epoch: 049, Loss: -0.6221\n",
      "Graph 62: Epoch: 050, Loss: -0.4455\n",
      "Graph 62: Epoch: 051, Loss: -0.4402\n",
      "Graph 62: Epoch: 052, Loss: -0.4450\n",
      "Graph 62: Epoch: 053, Loss: -0.4450\n",
      "Graph 62: Epoch: 054, Loss: -0.2678\n",
      "Graph 62: Epoch: 055, Loss: -0.2686\n",
      "Graph 62: Epoch: 056, Loss: -0.4449\n",
      "Graph 62: Epoch: 057, Loss: -0.4439\n",
      "Graph 62: Epoch: 058, Loss: -0.5354\n",
      "Graph 62: Epoch: 059, Loss: -0.5345\n",
      "Graph 62: Epoch: 060, Loss: -0.2686\n",
      "Graph 62: Epoch: 061, Loss: -0.6250\n",
      "Graph 62: Epoch: 062, Loss: -0.5366\n",
      "Graph 62: Epoch: 063, Loss: -0.6232\n",
      "Graph 62: Epoch: 064, Loss: -0.7144\n",
      "Graph 62: Epoch: 065, Loss: -0.6239\n",
      "Graph 62: Epoch: 066, Loss: -0.7144\n",
      "Graph 62: Epoch: 067, Loss: -0.5376\n",
      "Graph 62: Epoch: 068, Loss: -0.2700\n",
      "Graph 62: Epoch: 069, Loss: -0.1799\n",
      "Graph 62: Epoch: 070, Loss: -0.6270\n",
      "Graph 62: Epoch: 071, Loss: -0.4486\n",
      "Graph 62: Epoch: 072, Loss: -0.3571\n",
      "Graph 62: Epoch: 073, Loss: -0.2699\n",
      "Graph 62: Epoch: 074, Loss: -0.3596\n",
      "Graph 62: Epoch: 075, Loss: -0.6278\n",
      "Graph 62: Epoch: 076, Loss: -0.3598\n",
      "Graph 62: Epoch: 077, Loss: -0.8070\n",
      "Graph 62: Epoch: 078, Loss: -0.3608\n",
      "Graph 62: Epoch: 079, Loss: -0.3596\n",
      "Graph 62: Epoch: 080, Loss: -0.5388\n",
      "Graph 62: Epoch: 081, Loss: -0.6291\n",
      "Graph 62: Epoch: 082, Loss: -0.3601\n",
      "Graph 62: Epoch: 083, Loss: -0.3598\n",
      "Graph 62: Epoch: 084, Loss: -0.5395\n",
      "Graph 62: Epoch: 085, Loss: -0.7195\n",
      "Graph 62: Epoch: 086, Loss: -0.2706\n",
      "Graph 62: Epoch: 087, Loss: -0.1807\n",
      "Graph 62: Epoch: 088, Loss: -0.2707\n",
      "Graph 62: Epoch: 089, Loss: -0.5393\n",
      "Graph 62: Epoch: 090, Loss: -0.5400\n",
      "Graph 62: Epoch: 091, Loss: -0.5392\n",
      "Graph 62: Epoch: 092, Loss: -0.6295\n",
      "Graph 62: Epoch: 093, Loss: -0.6295\n",
      "Graph 62: Epoch: 094, Loss: -0.8098\n",
      "Graph 62: Epoch: 095, Loss: -0.8096\n",
      "Graph 62: Epoch: 096, Loss: -0.6302\n",
      "Graph 62: Epoch: 097, Loss: -0.5406\n",
      "Graph 62: Epoch: 098, Loss: -0.3601\n",
      "Graph 62: Epoch: 099, Loss: -0.5406\n",
      "Graph 62: Epoch: 100, Loss: -0.6306\n",
      "Graph 63: Epoch: 001, Loss: -0.0843\n",
      "Graph 63: Epoch: 002, Loss: -0.1560\n",
      "Graph 63: Epoch: 003, Loss: -0.2160\n",
      "Graph 63: Epoch: 004, Loss: -0.4926\n",
      "Graph 63: Epoch: 005, Loss: -0.2242\n",
      "Graph 63: Epoch: 006, Loss: -0.4152\n",
      "Graph 63: Epoch: 007, Loss: -0.3718\n",
      "Graph 63: Epoch: 008, Loss: -0.4422\n",
      "Graph 63: Epoch: 009, Loss: -0.4302\n",
      "Graph 63: Epoch: 010, Loss: -0.5392\n",
      "Graph 63: Epoch: 011, Loss: -0.3299\n",
      "Graph 63: Epoch: 012, Loss: -0.3895\n",
      "Graph 63: Epoch: 013, Loss: -0.1524\n",
      "Graph 63: Epoch: 014, Loss: -0.6636\n",
      "Graph 63: Epoch: 015, Loss: -0.3118\n",
      "Graph 63: Epoch: 016, Loss: -0.2026\n",
      "Graph 63: Epoch: 017, Loss: -0.5065\n",
      "Graph 63: Epoch: 018, Loss: -0.4963\n",
      "Graph 63: Epoch: 019, Loss: -0.1076\n",
      "Graph 63: Epoch: 020, Loss: -0.6074\n",
      "Graph 63: Epoch: 021, Loss: -0.5151\n",
      "Graph 63: Epoch: 022, Loss: -0.6219\n",
      "Graph 63: Epoch: 023, Loss: -0.2142\n",
      "Graph 63: Epoch: 024, Loss: -0.4141\n",
      "Graph 63: Epoch: 025, Loss: -0.8262\n",
      "Graph 63: Epoch: 026, Loss: -0.6175\n",
      "Graph 63: Epoch: 027, Loss: -0.4146\n",
      "Graph 63: Epoch: 028, Loss: -0.4177\n",
      "Graph 63: Epoch: 029, Loss: -0.3012\n",
      "Graph 63: Epoch: 030, Loss: -0.7320\n",
      "Graph 63: Epoch: 031, Loss: -0.5212\n",
      "Graph 63: Epoch: 032, Loss: -0.6361\n",
      "Graph 63: Epoch: 033, Loss: -0.5072\n",
      "Graph 63: Epoch: 034, Loss: -0.3194\n",
      "Graph 63: Epoch: 035, Loss: -0.6018\n",
      "Graph 63: Epoch: 036, Loss: -0.4382\n",
      "Graph 63: Epoch: 037, Loss: -0.8452\n",
      "Graph 63: Epoch: 038, Loss: -0.4291\n",
      "Graph 63: Epoch: 039, Loss: -0.6270\n",
      "Graph 63: Epoch: 040, Loss: -0.5337\n",
      "Graph 63: Epoch: 041, Loss: -0.3199\n",
      "Graph 63: Epoch: 042, Loss: -0.7480\n",
      "Graph 63: Epoch: 043, Loss: -0.5387\n",
      "Graph 63: Epoch: 044, Loss: -0.5371\n",
      "Graph 63: Epoch: 045, Loss: -0.3235\n",
      "Graph 63: Epoch: 046, Loss: -0.5375\n",
      "Graph 63: Epoch: 047, Loss: -0.4287\n",
      "Graph 63: Epoch: 048, Loss: -0.5412\n",
      "Graph 63: Epoch: 049, Loss: -0.8554\n",
      "Graph 63: Epoch: 050, Loss: -0.4308\n",
      "Graph 63: Epoch: 051, Loss: -0.5423\n",
      "Graph 63: Epoch: 052, Loss: -0.4343\n",
      "Graph 63: Epoch: 053, Loss: -0.5321\n",
      "Graph 63: Epoch: 054, Loss: -0.5402\n",
      "Graph 63: Epoch: 055, Loss: -0.4299\n",
      "Graph 63: Epoch: 056, Loss: -0.4261\n",
      "Graph 63: Epoch: 057, Loss: -0.7608\n",
      "Graph 63: Epoch: 058, Loss: -0.3295\n",
      "Graph 63: Epoch: 059, Loss: -0.5447\n",
      "Graph 63: Epoch: 060, Loss: -0.4355\n",
      "Graph 63: Epoch: 061, Loss: -0.5436\n",
      "Graph 63: Epoch: 062, Loss: -0.3631\n",
      "Graph 63: Epoch: 063, Loss: -0.3245\n",
      "Graph 63: Epoch: 064, Loss: -0.8710\n",
      "Graph 63: Epoch: 065, Loss: -0.5432\n",
      "Graph 63: Epoch: 066, Loss: -0.6545\n",
      "Graph 63: Epoch: 067, Loss: -0.7642\n",
      "Graph 63: Epoch: 068, Loss: -0.5404\n",
      "Graph 63: Epoch: 069, Loss: -0.4371\n",
      "Graph 63: Epoch: 070, Loss: -0.6559\n",
      "Graph 63: Epoch: 071, Loss: -0.6524\n",
      "Graph 63: Epoch: 072, Loss: -0.5408\n",
      "Graph 63: Epoch: 073, Loss: -0.6564\n",
      "Graph 63: Epoch: 074, Loss: -0.4352\n",
      "Graph 63: Epoch: 075, Loss: -0.2203\n",
      "Graph 63: Epoch: 076, Loss: -0.4385\n",
      "Graph 63: Epoch: 077, Loss: -0.6565\n",
      "Graph 63: Epoch: 078, Loss: -0.5482\n",
      "Graph 63: Epoch: 079, Loss: -0.1109\n",
      "Graph 63: Epoch: 080, Loss: -0.6576\n",
      "Graph 63: Epoch: 081, Loss: -0.5469\n",
      "Graph 63: Epoch: 082, Loss: -0.4363\n",
      "Graph 63: Epoch: 083, Loss: -0.5482\n",
      "Graph 63: Epoch: 084, Loss: -0.5479\n",
      "Graph 63: Epoch: 085, Loss: -0.4394\n",
      "Graph 63: Epoch: 086, Loss: -0.3297\n",
      "Graph 63: Epoch: 087, Loss: -0.5481\n",
      "Graph 63: Epoch: 088, Loss: -0.2202\n",
      "Graph 63: Epoch: 089, Loss: -0.4398\n",
      "Graph 63: Epoch: 090, Loss: -0.5472\n",
      "Graph 63: Epoch: 091, Loss: -0.2203\n",
      "Graph 63: Epoch: 092, Loss: -0.5492\n",
      "Graph 63: Epoch: 093, Loss: -0.3302\n",
      "Graph 63: Epoch: 094, Loss: -0.4400\n",
      "Graph 63: Epoch: 095, Loss: -0.4399\n",
      "Graph 63: Epoch: 096, Loss: -0.6598\n",
      "Graph 63: Epoch: 097, Loss: -0.0010\n",
      "Graph 63: Epoch: 098, Loss: -0.4395\n",
      "Graph 63: Epoch: 099, Loss: -0.6590\n",
      "Graph 63: Epoch: 100, Loss: -0.5497\n",
      "Graph 64: Epoch: 001, Loss: -0.0263\n",
      "Graph 64: Epoch: 002, Loss: -0.1248\n",
      "Graph 64: Epoch: 003, Loss: -0.3438\n",
      "Graph 64: Epoch: 004, Loss: -0.2490\n",
      "Graph 64: Epoch: 005, Loss: -0.5426\n",
      "Graph 64: Epoch: 006, Loss: -0.3135\n",
      "Graph 64: Epoch: 007, Loss: -0.5945\n",
      "Graph 64: Epoch: 008, Loss: -0.2856\n",
      "Graph 64: Epoch: 009, Loss: -0.3908\n",
      "Graph 64: Epoch: 010, Loss: -0.4922\n",
      "Graph 64: Epoch: 011, Loss: -0.5858\n",
      "Graph 64: Epoch: 012, Loss: -0.4866\n",
      "Graph 64: Epoch: 013, Loss: -0.4859\n",
      "Graph 64: Epoch: 014, Loss: -0.3989\n",
      "Graph 64: Epoch: 015, Loss: -0.6527\n",
      "Graph 64: Epoch: 016, Loss: -0.3024\n",
      "Graph 64: Epoch: 017, Loss: -0.4571\n",
      "Graph 64: Epoch: 018, Loss: -0.6645\n",
      "Graph 64: Epoch: 019, Loss: -0.4603\n",
      "Graph 64: Epoch: 020, Loss: -0.4604\n",
      "Graph 64: Epoch: 021, Loss: -0.4616\n",
      "Graph 64: Epoch: 022, Loss: -0.4632\n",
      "Graph 64: Epoch: 023, Loss: -0.3081\n",
      "Graph 64: Epoch: 024, Loss: -0.3091\n",
      "Graph 64: Epoch: 025, Loss: -0.4123\n",
      "Graph 64: Epoch: 026, Loss: -0.4080\n",
      "Graph 64: Epoch: 027, Loss: -0.3612\n",
      "Graph 64: Epoch: 028, Loss: -0.4649\n",
      "Graph 64: Epoch: 029, Loss: -0.6209\n",
      "Graph 64: Epoch: 030, Loss: -0.4657\n",
      "Graph 64: Epoch: 031, Loss: -0.5698\n",
      "Graph 64: Epoch: 032, Loss: -0.3628\n",
      "Graph 64: Epoch: 033, Loss: -0.3110\n",
      "Graph 64: Epoch: 034, Loss: -0.4653\n",
      "Graph 64: Epoch: 035, Loss: -0.3634\n",
      "Graph 64: Epoch: 036, Loss: -0.6227\n",
      "Graph 64: Epoch: 037, Loss: -0.4674\n",
      "Graph 64: Epoch: 038, Loss: -0.3634\n",
      "Graph 64: Epoch: 039, Loss: -0.5198\n",
      "Graph 64: Epoch: 040, Loss: -0.5200\n",
      "Graph 64: Epoch: 041, Loss: -0.4163\n",
      "Graph 64: Epoch: 042, Loss: -0.2598\n",
      "Graph 64: Epoch: 043, Loss: -0.4166\n",
      "Graph 64: Epoch: 044, Loss: -0.4682\n",
      "Graph 64: Epoch: 045, Loss: -0.5723\n",
      "Graph 64: Epoch: 046, Loss: -0.5198\n",
      "Graph 64: Epoch: 047, Loss: -0.5206\n",
      "Graph 64: Epoch: 048, Loss: -0.4688\n",
      "Graph 64: Epoch: 049, Loss: -0.3129\n",
      "Graph 64: Epoch: 050, Loss: -0.4172\n",
      "Graph 64: Epoch: 051, Loss: -0.7301\n",
      "Graph 64: Epoch: 052, Loss: -0.5735\n",
      "Graph 64: Epoch: 053, Loss: -0.4695\n",
      "Graph 64: Epoch: 054, Loss: -0.5218\n",
      "Graph 64: Epoch: 055, Loss: -0.7302\n",
      "Graph 64: Epoch: 056, Loss: -0.5218\n",
      "Graph 64: Epoch: 057, Loss: -0.5743\n",
      "Graph 64: Epoch: 058, Loss: -0.3657\n",
      "Graph 64: Epoch: 059, Loss: -0.5219\n",
      "Graph 64: Epoch: 060, Loss: -0.3136\n",
      "Graph 64: Epoch: 061, Loss: -0.6265\n",
      "Graph 64: Epoch: 062, Loss: -0.6793\n",
      "Graph 64: Epoch: 063, Loss: -0.4704\n",
      "Graph 64: Epoch: 064, Loss: -0.2092\n",
      "Graph 64: Epoch: 065, Loss: -0.4181\n",
      "Graph 64: Epoch: 066, Loss: -0.4704\n",
      "Graph 64: Epoch: 067, Loss: -0.6792\n",
      "Graph 64: Epoch: 068, Loss: -0.3659\n",
      "Graph 64: Epoch: 069, Loss: -0.4705\n",
      "Graph 64: Epoch: 070, Loss: -0.5226\n",
      "Graph 64: Epoch: 071, Loss: -0.3662\n",
      "Graph 64: Epoch: 072, Loss: -0.4707\n",
      "Graph 64: Epoch: 073, Loss: -0.3140\n",
      "Graph 64: Epoch: 074, Loss: -0.5754\n",
      "Graph 64: Epoch: 075, Loss: -0.6275\n",
      "Graph 64: Epoch: 076, Loss: -0.4186\n",
      "Graph 64: Epoch: 077, Loss: -0.4709\n",
      "Graph 64: Epoch: 078, Loss: -0.3141\n",
      "Graph 64: Epoch: 079, Loss: -0.3664\n",
      "Graph 64: Epoch: 080, Loss: -0.5755\n",
      "Graph 64: Epoch: 081, Loss: -0.4708\n",
      "Graph 64: Epoch: 082, Loss: -0.4711\n",
      "Graph 64: Epoch: 083, Loss: -0.3666\n",
      "Graph 64: Epoch: 084, Loss: -0.3665\n",
      "Graph 64: Epoch: 085, Loss: -0.6805\n",
      "Graph 64: Epoch: 086, Loss: -0.6282\n",
      "Graph 64: Epoch: 087, Loss: -0.5234\n",
      "Graph 64: Epoch: 088, Loss: -0.6282\n",
      "Graph 64: Epoch: 089, Loss: -0.4712\n",
      "Graph 64: Epoch: 090, Loss: -0.5760\n",
      "Graph 64: Epoch: 091, Loss: -0.3666\n",
      "Graph 64: Epoch: 092, Loss: -0.4190\n",
      "Graph 64: Epoch: 093, Loss: -0.4713\n",
      "Graph 64: Epoch: 094, Loss: -0.6285\n",
      "Graph 64: Epoch: 095, Loss: -0.5761\n",
      "Graph 64: Epoch: 096, Loss: -0.4715\n",
      "Graph 64: Epoch: 097, Loss: -0.4191\n",
      "Graph 64: Epoch: 098, Loss: -0.5762\n",
      "Graph 64: Epoch: 099, Loss: -0.5240\n",
      "Graph 64: Epoch: 100, Loss: -0.5761\n",
      "Graph 65: Epoch: 001, Loss: -0.1922\n",
      "Graph 65: Epoch: 002, Loss: -0.2200\n",
      "Graph 65: Epoch: 003, Loss: -0.2833\n",
      "Graph 65: Epoch: 004, Loss: -0.2322\n",
      "Graph 65: Epoch: 005, Loss: -0.4964\n",
      "Graph 65: Epoch: 006, Loss: -0.6005\n",
      "Graph 65: Epoch: 007, Loss: -0.5454\n",
      "Graph 65: Epoch: 008, Loss: -0.4955\n",
      "Graph 65: Epoch: 009, Loss: -0.2773\n",
      "Graph 65: Epoch: 010, Loss: -0.4339\n",
      "Graph 65: Epoch: 011, Loss: -0.6066\n",
      "Graph 65: Epoch: 012, Loss: -0.5002\n",
      "Graph 65: Epoch: 013, Loss: -0.2668\n",
      "Graph 65: Epoch: 014, Loss: -0.4973\n",
      "Graph 65: Epoch: 015, Loss: -0.5422\n",
      "Graph 65: Epoch: 016, Loss: -0.2972\n",
      "Graph 65: Epoch: 017, Loss: -0.4193\n",
      "Graph 65: Epoch: 018, Loss: -0.4293\n",
      "Graph 65: Epoch: 019, Loss: -0.6680\n",
      "Graph 65: Epoch: 020, Loss: -0.2804\n",
      "Graph 65: Epoch: 021, Loss: -0.6252\n",
      "Graph 65: Epoch: 022, Loss: -0.2175\n",
      "Graph 65: Epoch: 023, Loss: -0.6185\n",
      "Graph 65: Epoch: 024, Loss: -0.8670\n",
      "Graph 65: Epoch: 025, Loss: -0.4413\n",
      "Graph 65: Epoch: 026, Loss: -0.4605\n",
      "Graph 65: Epoch: 027, Loss: -0.6072\n",
      "Graph 65: Epoch: 028, Loss: -0.2337\n",
      "Graph 65: Epoch: 029, Loss: -0.2961\n",
      "Graph 65: Epoch: 030, Loss: -0.4573\n",
      "Graph 65: Epoch: 031, Loss: -0.1049\n",
      "Graph 65: Epoch: 032, Loss: -0.6845\n",
      "Graph 65: Epoch: 033, Loss: -0.0183\n",
      "Graph 65: Epoch: 034, Loss: -0.6511\n",
      "Graph 65: Epoch: 035, Loss: -0.6495\n",
      "Graph 65: Epoch: 036, Loss: -0.2388\n",
      "Graph 65: Epoch: 037, Loss: -0.4788\n",
      "Graph 65: Epoch: 038, Loss: -0.3030\n",
      "Graph 65: Epoch: 039, Loss: -0.2360\n",
      "Graph 65: Epoch: 040, Loss: -0.5211\n",
      "Graph 65: Epoch: 041, Loss: -0.2454\n",
      "Graph 65: Epoch: 042, Loss: -0.6956\n",
      "Graph 65: Epoch: 043, Loss: -0.2420\n",
      "Graph 65: Epoch: 044, Loss: -0.2875\n",
      "Graph 65: Epoch: 045, Loss: -0.2447\n",
      "Graph 65: Epoch: 046, Loss: -0.5623\n",
      "Graph 65: Epoch: 047, Loss: -0.4750\n",
      "Graph 65: Epoch: 048, Loss: -0.2406\n",
      "Graph 65: Epoch: 049, Loss: -0.5181\n",
      "Graph 65: Epoch: 050, Loss: -0.4652\n",
      "Graph 65: Epoch: 051, Loss: -0.3784\n",
      "Graph 65: Epoch: 052, Loss: -0.4810\n",
      "Graph 65: Epoch: 053, Loss: -0.5361\n",
      "Graph 65: Epoch: 054, Loss: -0.2466\n",
      "Graph 65: Epoch: 055, Loss: -0.4853\n",
      "Graph 65: Epoch: 056, Loss: -0.8386\n",
      "Graph 65: Epoch: 057, Loss: -0.2544\n",
      "Graph 65: Epoch: 058, Loss: -0.5390\n",
      "Graph 65: Epoch: 059, Loss: -0.5092\n",
      "Graph 65: Epoch: 060, Loss: -0.7048\n",
      "Graph 65: Epoch: 061, Loss: -0.4791\n",
      "Graph 65: Epoch: 062, Loss: -0.4837\n",
      "Graph 65: Epoch: 063, Loss: -0.4809\n",
      "Graph 65: Epoch: 064, Loss: -0.8656\n",
      "Graph 65: Epoch: 065, Loss: -0.6078\n",
      "Graph 65: Epoch: 066, Loss: -0.4819\n",
      "Graph 65: Epoch: 067, Loss: -0.4229\n",
      "Graph 65: Epoch: 068, Loss: -0.6735\n",
      "Graph 65: Epoch: 069, Loss: -0.3085\n",
      "Graph 65: Epoch: 070, Loss: -0.4607\n",
      "Graph 65: Epoch: 071, Loss: -0.3184\n",
      "Graph 65: Epoch: 072, Loss: -0.2950\n",
      "Graph 65: Epoch: 073, Loss: -0.7125\n",
      "Graph 65: Epoch: 074, Loss: -0.0689\n",
      "Graph 65: Epoch: 075, Loss: -0.8192\n",
      "Graph 65: Epoch: 076, Loss: -0.2488\n",
      "Graph 65: Epoch: 077, Loss: -0.4034\n",
      "Graph 65: Epoch: 078, Loss: -0.4550\n",
      "Graph 65: Epoch: 079, Loss: -0.4914\n",
      "Graph 65: Epoch: 080, Loss: -0.0172\n",
      "Graph 65: Epoch: 081, Loss: -0.4868\n",
      "Graph 65: Epoch: 082, Loss: -0.2549\n",
      "Graph 65: Epoch: 083, Loss: -0.4838\n",
      "Graph 65: Epoch: 084, Loss: -0.2519\n",
      "Graph 65: Epoch: 085, Loss: -0.4872\n",
      "Graph 65: Epoch: 086, Loss: -0.5367\n",
      "Graph 65: Epoch: 087, Loss: -0.2498\n",
      "Graph 65: Epoch: 088, Loss: -0.6448\n",
      "Graph 65: Epoch: 089, Loss: -0.4067\n",
      "Graph 65: Epoch: 090, Loss: -0.3584\n",
      "Graph 65: Epoch: 091, Loss: -0.6180\n",
      "Graph 65: Epoch: 092, Loss: -0.7035\n",
      "Graph 65: Epoch: 093, Loss: -0.3526\n",
      "Graph 65: Epoch: 094, Loss: -0.2517\n",
      "Graph 65: Epoch: 095, Loss: -0.6193\n",
      "Graph 65: Epoch: 096, Loss: -0.4900\n",
      "Graph 65: Epoch: 097, Loss: -0.2490\n",
      "Graph 65: Epoch: 098, Loss: -0.6349\n",
      "Graph 65: Epoch: 099, Loss: -0.4925\n",
      "Graph 65: Epoch: 100, Loss: -0.6226\n",
      "Graph 66: Epoch: 001, Loss: -0.0328\n",
      "Graph 66: Epoch: 002, Loss: -0.0476\n",
      "Graph 66: Epoch: 003, Loss: -0.0742\n",
      "Graph 66: Epoch: 004, Loss: -0.1453\n",
      "Graph 66: Epoch: 005, Loss: -0.1938\n",
      "Graph 66: Epoch: 006, Loss: -0.2185\n",
      "Graph 66: Epoch: 007, Loss: -0.3050\n",
      "Graph 66: Epoch: 008, Loss: -0.2665\n",
      "Graph 66: Epoch: 009, Loss: -0.3356\n",
      "Graph 66: Epoch: 010, Loss: -0.3668\n",
      "Graph 66: Epoch: 011, Loss: -0.3376\n",
      "Graph 66: Epoch: 012, Loss: -0.3842\n",
      "Graph 66: Epoch: 013, Loss: -0.3595\n",
      "Graph 66: Epoch: 014, Loss: -0.3484\n",
      "Graph 66: Epoch: 015, Loss: -0.3437\n",
      "Graph 66: Epoch: 016, Loss: -0.3801\n",
      "Graph 66: Epoch: 017, Loss: -0.4185\n",
      "Graph 66: Epoch: 018, Loss: -0.4233\n",
      "Graph 66: Epoch: 019, Loss: -0.4142\n",
      "Graph 66: Epoch: 020, Loss: -0.3131\n",
      "Graph 66: Epoch: 021, Loss: -0.4291\n",
      "Graph 66: Epoch: 022, Loss: -0.4536\n",
      "Graph 66: Epoch: 023, Loss: -0.4130\n",
      "Graph 66: Epoch: 024, Loss: -0.4925\n",
      "Graph 66: Epoch: 025, Loss: -0.2168\n",
      "Graph 66: Epoch: 026, Loss: -0.3035\n",
      "Graph 66: Epoch: 027, Loss: -0.2385\n",
      "Graph 66: Epoch: 028, Loss: -0.3937\n",
      "Graph 66: Epoch: 029, Loss: -0.2860\n",
      "Graph 66: Epoch: 030, Loss: -0.4264\n",
      "Graph 66: Epoch: 031, Loss: -0.3317\n",
      "Graph 66: Epoch: 032, Loss: -0.3939\n",
      "Graph 66: Epoch: 033, Loss: -0.3987\n",
      "Graph 66: Epoch: 034, Loss: -0.4455\n",
      "Graph 66: Epoch: 035, Loss: -0.2506\n",
      "Graph 66: Epoch: 036, Loss: -0.3104\n",
      "Graph 66: Epoch: 037, Loss: -0.3378\n",
      "Graph 66: Epoch: 038, Loss: -0.5463\n",
      "Graph 66: Epoch: 039, Loss: -0.3489\n",
      "Graph 66: Epoch: 040, Loss: -0.5888\n",
      "Graph 66: Epoch: 041, Loss: -0.3196\n",
      "Graph 66: Epoch: 042, Loss: -0.4045\n",
      "Graph 66: Epoch: 043, Loss: -0.2961\n",
      "Graph 66: Epoch: 044, Loss: -0.3482\n",
      "Graph 66: Epoch: 045, Loss: -0.4788\n",
      "Graph 66: Epoch: 046, Loss: -0.5143\n",
      "Graph 66: Epoch: 047, Loss: -0.4355\n",
      "Graph 66: Epoch: 048, Loss: -0.4447\n",
      "Graph 66: Epoch: 049, Loss: -0.5487\n",
      "Graph 66: Epoch: 050, Loss: -0.4804\n",
      "Graph 66: Epoch: 051, Loss: -0.4408\n",
      "Graph 66: Epoch: 052, Loss: -0.4004\n",
      "Graph 66: Epoch: 053, Loss: -0.4667\n",
      "Graph 66: Epoch: 054, Loss: -0.5099\n",
      "Graph 66: Epoch: 055, Loss: -0.4032\n",
      "Graph 66: Epoch: 056, Loss: -0.4854\n",
      "Graph 66: Epoch: 057, Loss: -0.5673\n",
      "Graph 66: Epoch: 058, Loss: -0.4907\n",
      "Graph 66: Epoch: 059, Loss: -0.1630\n",
      "Graph 66: Epoch: 060, Loss: -0.4518\n",
      "Graph 66: Epoch: 061, Loss: -0.3073\n",
      "Graph 66: Epoch: 062, Loss: -0.3117\n",
      "Graph 66: Epoch: 063, Loss: -0.3884\n",
      "Graph 66: Epoch: 064, Loss: -0.5279\n",
      "Graph 66: Epoch: 065, Loss: -0.5056\n",
      "Graph 66: Epoch: 066, Loss: -0.6029\n",
      "Graph 66: Epoch: 067, Loss: -0.4148\n",
      "Graph 66: Epoch: 068, Loss: -0.4290\n",
      "Graph 66: Epoch: 069, Loss: -0.4063\n",
      "Graph 66: Epoch: 070, Loss: -0.3742\n",
      "Graph 66: Epoch: 071, Loss: -0.3956\n",
      "Graph 66: Epoch: 072, Loss: -0.4481\n",
      "Graph 66: Epoch: 073, Loss: -0.4629\n",
      "Graph 66: Epoch: 074, Loss: -0.5386\n",
      "Graph 66: Epoch: 075, Loss: -0.4356\n",
      "Graph 66: Epoch: 076, Loss: -0.4347\n",
      "Graph 66: Epoch: 077, Loss: -0.6988\n",
      "Graph 66: Epoch: 078, Loss: -0.7418\n",
      "Graph 66: Epoch: 079, Loss: -0.4635\n",
      "Graph 66: Epoch: 080, Loss: -0.4300\n",
      "Graph 66: Epoch: 081, Loss: -0.5488\n",
      "Graph 66: Epoch: 082, Loss: -0.4146\n",
      "Graph 66: Epoch: 083, Loss: -0.3487\n",
      "Graph 66: Epoch: 084, Loss: -0.4295\n",
      "Graph 66: Epoch: 085, Loss: -0.6447\n",
      "Graph 66: Epoch: 086, Loss: -0.5484\n",
      "Graph 66: Epoch: 087, Loss: -0.4269\n",
      "Graph 66: Epoch: 088, Loss: -0.5499\n",
      "Graph 66: Epoch: 089, Loss: -0.4211\n",
      "Graph 66: Epoch: 090, Loss: -0.7571\n",
      "Graph 66: Epoch: 091, Loss: -0.5407\n",
      "Graph 66: Epoch: 092, Loss: -0.5567\n",
      "Graph 66: Epoch: 093, Loss: -0.5493\n",
      "Graph 66: Epoch: 094, Loss: -0.5826\n",
      "Graph 66: Epoch: 095, Loss: -0.5592\n",
      "Graph 66: Epoch: 096, Loss: -0.4254\n",
      "Graph 66: Epoch: 097, Loss: -0.4268\n",
      "Graph 66: Epoch: 098, Loss: -0.4258\n",
      "Graph 66: Epoch: 099, Loss: -0.4170\n",
      "Graph 66: Epoch: 100, Loss: -0.2887\n",
      "Graph 67: Epoch: 001, Loss: -0.0130\n",
      "Graph 67: Epoch: 002, Loss: -0.0404\n",
      "Graph 67: Epoch: 003, Loss: -0.0547\n",
      "Graph 67: Epoch: 004, Loss: -0.1605\n",
      "Graph 67: Epoch: 005, Loss: -0.2376\n",
      "Graph 67: Epoch: 006, Loss: -0.2087\n",
      "Graph 67: Epoch: 007, Loss: -0.2730\n",
      "Graph 67: Epoch: 008, Loss: -0.3911\n",
      "Graph 67: Epoch: 009, Loss: -0.2846\n",
      "Graph 67: Epoch: 010, Loss: -0.3449\n",
      "Graph 67: Epoch: 011, Loss: -0.5910\n",
      "Graph 67: Epoch: 012, Loss: -0.1195\n",
      "Graph 67: Epoch: 013, Loss: -0.3545\n",
      "Graph 67: Epoch: 014, Loss: -0.5873\n",
      "Graph 67: Epoch: 015, Loss: -0.4232\n",
      "Graph 67: Epoch: 016, Loss: -0.2188\n",
      "Graph 67: Epoch: 017, Loss: -0.4220\n",
      "Graph 67: Epoch: 018, Loss: -0.3037\n",
      "Graph 67: Epoch: 019, Loss: -0.3185\n",
      "Graph 67: Epoch: 020, Loss: -0.2219\n",
      "Graph 67: Epoch: 021, Loss: -0.5410\n",
      "Graph 67: Epoch: 022, Loss: -0.6696\n",
      "Graph 67: Epoch: 023, Loss: -0.5282\n",
      "Graph 67: Epoch: 024, Loss: -0.1771\n",
      "Graph 67: Epoch: 025, Loss: -0.5686\n",
      "Graph 67: Epoch: 026, Loss: -0.2417\n",
      "Graph 67: Epoch: 027, Loss: -0.3464\n",
      "Graph 67: Epoch: 028, Loss: -0.3522\n",
      "Graph 67: Epoch: 029, Loss: -0.5460\n",
      "Graph 67: Epoch: 030, Loss: -0.6920\n",
      "Graph 67: Epoch: 031, Loss: -0.3574\n",
      "Graph 67: Epoch: 032, Loss: -0.3400\n",
      "Graph 67: Epoch: 033, Loss: -0.5053\n",
      "Graph 67: Epoch: 034, Loss: -0.4551\n",
      "Graph 67: Epoch: 035, Loss: -0.7095\n",
      "Graph 67: Epoch: 036, Loss: -0.5956\n",
      "Graph 67: Epoch: 037, Loss: -0.1238\n",
      "Graph 67: Epoch: 038, Loss: -0.4786\n",
      "Graph 67: Epoch: 039, Loss: -0.4790\n",
      "Graph 67: Epoch: 040, Loss: -0.5972\n",
      "Graph 67: Epoch: 041, Loss: -0.4813\n",
      "Graph 67: Epoch: 042, Loss: -0.4803\n",
      "Graph 67: Epoch: 043, Loss: -0.3609\n",
      "Graph 67: Epoch: 044, Loss: -0.4829\n",
      "Graph 67: Epoch: 045, Loss: -0.5985\n",
      "Graph 67: Epoch: 046, Loss: -0.4787\n",
      "Graph 67: Epoch: 047, Loss: -0.4822\n",
      "Graph 67: Epoch: 048, Loss: -0.3640\n",
      "Graph 67: Epoch: 049, Loss: -0.4836\n",
      "Graph 67: Epoch: 050, Loss: -0.3635\n",
      "Graph 67: Epoch: 051, Loss: -0.1251\n",
      "Graph 67: Epoch: 052, Loss: -0.5036\n",
      "Graph 67: Epoch: 053, Loss: -0.7235\n",
      "Graph 67: Epoch: 054, Loss: -0.3642\n",
      "Graph 67: Epoch: 055, Loss: -0.6052\n",
      "Graph 67: Epoch: 056, Loss: -0.4839\n",
      "Graph 67: Epoch: 057, Loss: -0.4850\n",
      "Graph 67: Epoch: 058, Loss: -0.3664\n",
      "Graph 67: Epoch: 059, Loss: -0.7296\n",
      "Graph 67: Epoch: 060, Loss: -0.4875\n",
      "Graph 67: Epoch: 061, Loss: -0.3653\n",
      "Graph 67: Epoch: 062, Loss: -0.9687\n",
      "Graph 67: Epoch: 063, Loss: -0.6079\n",
      "Graph 67: Epoch: 064, Loss: -0.4884\n",
      "Graph 67: Epoch: 065, Loss: -0.6093\n",
      "Graph 67: Epoch: 066, Loss: -0.3668\n",
      "Graph 67: Epoch: 067, Loss: -0.3676\n",
      "Graph 67: Epoch: 068, Loss: -0.5346\n",
      "Graph 67: Epoch: 069, Loss: -0.4871\n",
      "Graph 67: Epoch: 070, Loss: -0.4897\n",
      "Graph 67: Epoch: 071, Loss: -0.4903\n",
      "Graph 67: Epoch: 072, Loss: -0.2889\n",
      "Graph 67: Epoch: 073, Loss: -0.4907\n",
      "Graph 67: Epoch: 074, Loss: -0.3559\n",
      "Graph 67: Epoch: 075, Loss: -0.3683\n",
      "Graph 67: Epoch: 076, Loss: -0.5182\n",
      "Graph 67: Epoch: 077, Loss: -0.3684\n",
      "Graph 67: Epoch: 078, Loss: -0.2969\n",
      "Graph 67: Epoch: 079, Loss: -0.6114\n",
      "Graph 67: Epoch: 080, Loss: -0.6126\n",
      "Graph 67: Epoch: 081, Loss: -0.1260\n",
      "Graph 67: Epoch: 082, Loss: -0.3685\n",
      "Graph 67: Epoch: 083, Loss: -0.6129\n",
      "Graph 67: Epoch: 084, Loss: -0.3701\n",
      "Graph 67: Epoch: 085, Loss: -0.3830\n",
      "Graph 67: Epoch: 086, Loss: -0.1265\n",
      "Graph 67: Epoch: 087, Loss: -0.5087\n",
      "Graph 67: Epoch: 088, Loss: -0.6681\n",
      "Graph 67: Epoch: 089, Loss: -0.6105\n",
      "Graph 67: Epoch: 090, Loss: -0.3684\n",
      "Graph 67: Epoch: 091, Loss: -0.7355\n",
      "Graph 67: Epoch: 092, Loss: -0.6150\n",
      "Graph 67: Epoch: 093, Loss: -0.7227\n",
      "Graph 67: Epoch: 094, Loss: -0.6841\n",
      "Graph 67: Epoch: 095, Loss: -0.3698\n",
      "Graph 67: Epoch: 096, Loss: -0.3709\n",
      "Graph 67: Epoch: 097, Loss: -0.6846\n",
      "Graph 67: Epoch: 098, Loss: -0.3710\n",
      "Graph 67: Epoch: 099, Loss: -0.3709\n",
      "Graph 67: Epoch: 100, Loss: -0.2483\n",
      "Graph 68: Epoch: 001, Loss: -0.5000\n",
      "Graph 68: Epoch: 002, Loss: -0.4950\n",
      "Graph 68: Epoch: 003, Loss: -0.5047\n",
      "Graph 68: Epoch: 004, Loss: -0.5064\n",
      "Graph 68: Epoch: 005, Loss: -0.5091\n",
      "Graph 68: Epoch: 006, Loss: -0.5123\n",
      "Graph 68: Epoch: 007, Loss: -0.5159\n",
      "Graph 68: Epoch: 008, Loss: -0.4803\n",
      "Graph 68: Epoch: 009, Loss: -0.5220\n",
      "Graph 68: Epoch: 010, Loss: -0.4752\n",
      "Graph 68: Epoch: 011, Loss: -0.4737\n",
      "Graph 68: Epoch: 012, Loss: -0.5270\n",
      "Graph 68: Epoch: 013, Loss: -0.5282\n",
      "Graph 68: Epoch: 014, Loss: -0.5299\n",
      "Graph 68: Epoch: 015, Loss: -0.4680\n",
      "Graph 68: Epoch: 016, Loss: -0.5333\n",
      "Graph 68: Epoch: 017, Loss: -0.4650\n",
      "Graph 68: Epoch: 018, Loss: -0.5359\n",
      "Graph 68: Epoch: 019, Loss: -0.5372\n",
      "Graph 68: Epoch: 020, Loss: -0.4610\n",
      "Graph 68: Epoch: 021, Loss: -0.4599\n",
      "Graph 68: Epoch: 022, Loss: -0.4596\n",
      "Graph 68: Epoch: 023, Loss: -0.4598\n",
      "Graph 68: Epoch: 024, Loss: -0.4606\n",
      "Graph 68: Epoch: 025, Loss: -0.5382\n",
      "Graph 68: Epoch: 026, Loss: -0.4623\n",
      "Graph 68: Epoch: 027, Loss: -0.4633\n",
      "Graph 68: Epoch: 028, Loss: -0.5352\n",
      "Graph 68: Epoch: 029, Loss: -0.5345\n",
      "Graph 68: Epoch: 030, Loss: -0.4657\n",
      "Graph 68: Epoch: 031, Loss: -0.4664\n",
      "Graph 68: Epoch: 032, Loss: -0.4675\n",
      "Graph 68: Epoch: 033, Loss: -0.4690\n",
      "Graph 68: Epoch: 034, Loss: -0.4709\n",
      "Graph 68: Epoch: 035, Loss: -0.4731\n",
      "Graph 68: Epoch: 036, Loss: -0.4755\n",
      "Graph 68: Epoch: 037, Loss: -0.4783\n",
      "Graph 68: Epoch: 038, Loss: -0.5188\n",
      "Graph 68: Epoch: 039, Loss: -0.1125\n",
      "Graph 68: Epoch: 040, Loss: -0.5144\n",
      "Graph 68: Epoch: 041, Loss: -0.1274\n",
      "Graph 68: Epoch: 042, Loss: -0.4881\n",
      "Graph 68: Epoch: 043, Loss: -0.5105\n",
      "Graph 68: Epoch: 044, Loss: -0.5096\n",
      "Graph 68: Epoch: 045, Loss: -0.8677\n",
      "Graph 68: Epoch: 046, Loss: -0.4910\n",
      "Graph 68: Epoch: 047, Loss: -0.8681\n",
      "Graph 68: Epoch: 048, Loss: -0.4930\n",
      "Graph 68: Epoch: 049, Loss: -0.4945\n",
      "Graph 68: Epoch: 050, Loss: -0.5037\n",
      "Graph 68: Epoch: 051, Loss: -0.8732\n",
      "Graph 68: Epoch: 052, Loss: -0.5013\n",
      "Graph 68: Epoch: 053, Loss: -0.4993\n",
      "Graph 68: Epoch: 054, Loss: -0.5004\n",
      "Graph 68: Epoch: 055, Loss: -0.4981\n",
      "Graph 68: Epoch: 056, Loss: -0.4973\n",
      "Graph 68: Epoch: 057, Loss: -0.5029\n",
      "Graph 68: Epoch: 058, Loss: -0.5036\n",
      "Graph 68: Epoch: 059, Loss: -0.5048\n",
      "Graph 68: Epoch: 060, Loss: -0.4936\n",
      "Graph 68: Epoch: 061, Loss: -0.4927\n",
      "Graph 68: Epoch: 062, Loss: -0.1147\n",
      "Graph 68: Epoch: 063, Loss: -0.4924\n",
      "Graph 68: Epoch: 064, Loss: -0.5071\n",
      "Graph 68: Epoch: 065, Loss: -0.5072\n",
      "Graph 68: Epoch: 066, Loss: -0.1155\n",
      "Graph 68: Epoch: 067, Loss: -0.5081\n",
      "Graph 68: Epoch: 068, Loss: -0.4911\n",
      "Graph 68: Epoch: 069, Loss: -0.4909\n",
      "Graph 68: Epoch: 070, Loss: -0.4912\n",
      "Graph 68: Epoch: 071, Loss: -0.5080\n",
      "Graph 68: Epoch: 072, Loss: -0.5078\n",
      "Graph 68: Epoch: 073, Loss: -0.1195\n",
      "Graph 68: Epoch: 074, Loss: -0.4918\n",
      "Graph 68: Epoch: 075, Loss: -0.8784\n",
      "Graph 68: Epoch: 076, Loss: -0.4925\n",
      "Graph 68: Epoch: 077, Loss: -0.5068\n",
      "Graph 68: Epoch: 078, Loss: -0.8775\n",
      "Graph 68: Epoch: 079, Loss: -0.5069\n",
      "Graph 68: Epoch: 080, Loss: -0.4925\n",
      "Graph 68: Epoch: 081, Loss: -0.4924\n",
      "Graph 68: Epoch: 082, Loss: -0.4929\n",
      "Graph 68: Epoch: 083, Loss: -0.4939\n",
      "Graph 68: Epoch: 084, Loss: -0.5048\n",
      "Graph 68: Epoch: 085, Loss: -0.4960\n",
      "Graph 68: Epoch: 086, Loss: -0.1214\n",
      "Graph 68: Epoch: 087, Loss: -0.5016\n",
      "Graph 68: Epoch: 088, Loss: -0.4991\n",
      "Graph 68: Epoch: 089, Loss: -0.4998\n",
      "Graph 68: Epoch: 090, Loss: -0.8758\n",
      "Graph 68: Epoch: 091, Loss: -0.5008\n",
      "Graph 68: Epoch: 092, Loss: -0.4985\n",
      "Graph 68: Epoch: 093, Loss: -0.5016\n",
      "Graph 68: Epoch: 094, Loss: -0.5022\n",
      "Graph 68: Epoch: 095, Loss: -0.4967\n",
      "Graph 68: Epoch: 096, Loss: -0.5037\n",
      "Graph 68: Epoch: 097, Loss: -0.5047\n",
      "Graph 68: Epoch: 098, Loss: -0.4940\n",
      "Graph 68: Epoch: 099, Loss: -0.5067\n",
      "Graph 68: Epoch: 100, Loss: -0.5079\n",
      "Graph 69: Epoch: 001, Loss: -0.0447\n",
      "Graph 69: Epoch: 002, Loss: -0.1085\n",
      "Graph 69: Epoch: 003, Loss: -0.2434\n",
      "Graph 69: Epoch: 004, Loss: -0.2890\n",
      "Graph 69: Epoch: 005, Loss: -0.5506\n",
      "Graph 69: Epoch: 006, Loss: -0.3025\n",
      "Graph 69: Epoch: 007, Loss: -0.5026\n",
      "Graph 69: Epoch: 008, Loss: -0.4263\n",
      "Graph 69: Epoch: 009, Loss: -0.2960\n",
      "Graph 69: Epoch: 010, Loss: -0.2333\n",
      "Graph 69: Epoch: 011, Loss: -0.5342\n",
      "Graph 69: Epoch: 012, Loss: -0.3326\n",
      "Graph 69: Epoch: 013, Loss: -0.3334\n",
      "Graph 69: Epoch: 014, Loss: -0.5566\n",
      "Graph 69: Epoch: 015, Loss: -0.4063\n",
      "Graph 69: Epoch: 016, Loss: -0.5537\n",
      "Graph 69: Epoch: 017, Loss: -0.6435\n",
      "Graph 69: Epoch: 018, Loss: -0.4253\n",
      "Graph 69: Epoch: 019, Loss: -0.3468\n",
      "Graph 69: Epoch: 020, Loss: -0.4809\n",
      "Graph 69: Epoch: 021, Loss: -0.6511\n",
      "Graph 69: Epoch: 022, Loss: -0.4552\n",
      "Graph 69: Epoch: 023, Loss: -0.4319\n",
      "Graph 69: Epoch: 024, Loss: -0.3273\n",
      "Graph 69: Epoch: 025, Loss: -0.3513\n",
      "Graph 69: Epoch: 026, Loss: -0.4696\n",
      "Graph 69: Epoch: 027, Loss: -0.3489\n",
      "Graph 69: Epoch: 028, Loss: -0.3593\n",
      "Graph 69: Epoch: 029, Loss: -0.6978\n",
      "Graph 69: Epoch: 030, Loss: -0.2377\n",
      "Graph 69: Epoch: 031, Loss: -0.4748\n",
      "Graph 69: Epoch: 032, Loss: -0.8246\n",
      "Graph 69: Epoch: 033, Loss: -0.5915\n",
      "Graph 69: Epoch: 034, Loss: -0.5747\n",
      "Graph 69: Epoch: 035, Loss: -0.2359\n",
      "Graph 69: Epoch: 036, Loss: -0.4096\n",
      "Graph 69: Epoch: 037, Loss: -0.7153\n",
      "Graph 69: Epoch: 038, Loss: -0.5966\n",
      "Graph 69: Epoch: 039, Loss: -0.1480\n",
      "Graph 69: Epoch: 040, Loss: -0.4808\n",
      "Graph 69: Epoch: 041, Loss: -0.3605\n",
      "Graph 69: Epoch: 042, Loss: -0.6022\n",
      "Graph 69: Epoch: 043, Loss: -0.6007\n",
      "Graph 69: Epoch: 044, Loss: -0.7091\n",
      "Graph 69: Epoch: 045, Loss: -0.3636\n",
      "Graph 69: Epoch: 046, Loss: -0.6043\n",
      "Graph 69: Epoch: 047, Loss: -0.4851\n",
      "Graph 69: Epoch: 048, Loss: -0.2447\n",
      "Graph 69: Epoch: 049, Loss: -0.8453\n",
      "Graph 69: Epoch: 050, Loss: -0.4775\n",
      "Graph 69: Epoch: 051, Loss: -0.4859\n",
      "Graph 69: Epoch: 052, Loss: -0.6061\n",
      "Graph 69: Epoch: 053, Loss: -0.5107\n",
      "Graph 69: Epoch: 054, Loss: -0.3673\n",
      "Graph 69: Epoch: 055, Loss: -0.6081\n",
      "Graph 69: Epoch: 056, Loss: -0.2471\n",
      "Graph 69: Epoch: 057, Loss: -0.9709\n",
      "Graph 69: Epoch: 058, Loss: -0.1246\n",
      "Graph 69: Epoch: 059, Loss: -0.6110\n",
      "Graph 69: Epoch: 060, Loss: -0.2452\n",
      "Graph 69: Epoch: 061, Loss: -0.4896\n",
      "Graph 69: Epoch: 062, Loss: -0.3682\n",
      "Graph 69: Epoch: 063, Loss: -0.4881\n",
      "Graph 69: Epoch: 064, Loss: -0.8554\n",
      "Graph 69: Epoch: 065, Loss: -0.4895\n",
      "Graph 69: Epoch: 066, Loss: -0.6541\n",
      "Graph 69: Epoch: 067, Loss: -0.2674\n",
      "Graph 69: Epoch: 068, Loss: -0.3590\n",
      "Graph 69: Epoch: 069, Loss: -0.4924\n",
      "Graph 69: Epoch: 070, Loss: -0.4916\n",
      "Graph 69: Epoch: 071, Loss: -0.5477\n",
      "Graph 69: Epoch: 072, Loss: -0.3695\n",
      "Graph 69: Epoch: 073, Loss: -0.7317\n",
      "Graph 69: Epoch: 074, Loss: -0.3691\n",
      "Graph 69: Epoch: 075, Loss: -0.2468\n",
      "Graph 69: Epoch: 076, Loss: -0.7369\n",
      "Graph 69: Epoch: 077, Loss: -0.3566\n",
      "Graph 69: Epoch: 078, Loss: -0.7330\n",
      "Graph 69: Epoch: 079, Loss: -0.6116\n",
      "Graph 69: Epoch: 080, Loss: -0.7261\n",
      "Graph 69: Epoch: 081, Loss: -0.7372\n",
      "Graph 69: Epoch: 082, Loss: -0.3879\n",
      "Graph 69: Epoch: 083, Loss: -0.5050\n",
      "Graph 69: Epoch: 084, Loss: -0.6157\n",
      "Graph 69: Epoch: 085, Loss: -0.4923\n",
      "Graph 69: Epoch: 086, Loss: -0.4765\n",
      "Graph 69: Epoch: 087, Loss: -0.7383\n",
      "Graph 69: Epoch: 088, Loss: -0.3702\n",
      "Graph 69: Epoch: 089, Loss: -0.6165\n",
      "Graph 69: Epoch: 090, Loss: -0.4930\n",
      "Graph 69: Epoch: 091, Loss: -0.2724\n",
      "Graph 69: Epoch: 092, Loss: -0.3677\n",
      "Graph 69: Epoch: 093, Loss: -0.5700\n",
      "Graph 69: Epoch: 094, Loss: -0.3703\n",
      "Graph 69: Epoch: 095, Loss: -0.8151\n",
      "Graph 69: Epoch: 096, Loss: -0.2500\n",
      "Graph 69: Epoch: 097, Loss: -0.2481\n",
      "Graph 69: Epoch: 098, Loss: -0.4944\n",
      "Graph 69: Epoch: 099, Loss: -0.2419\n",
      "Graph 69: Epoch: 100, Loss: -0.3700\n",
      "Graph 70: Epoch: 001, Loss: -0.0154\n",
      "Graph 70: Epoch: 002, Loss: -0.0894\n",
      "Graph 70: Epoch: 003, Loss: -0.2064\n",
      "Graph 70: Epoch: 004, Loss: -0.2451\n",
      "Graph 70: Epoch: 005, Loss: -0.3310\n",
      "Graph 70: Epoch: 006, Loss: -0.3689\n",
      "Graph 70: Epoch: 007, Loss: -0.3458\n",
      "Graph 70: Epoch: 008, Loss: -0.3732\n",
      "Graph 70: Epoch: 009, Loss: -0.3970\n",
      "Graph 70: Epoch: 010, Loss: -0.3444\n",
      "Graph 70: Epoch: 011, Loss: -0.4049\n",
      "Graph 70: Epoch: 012, Loss: -0.4368\n",
      "Graph 70: Epoch: 013, Loss: -0.4702\n",
      "Graph 70: Epoch: 014, Loss: -0.4804\n",
      "Graph 70: Epoch: 015, Loss: -0.5006\n",
      "Graph 70: Epoch: 016, Loss: -0.4966\n",
      "Graph 70: Epoch: 017, Loss: -0.4144\n",
      "Graph 70: Epoch: 018, Loss: -0.2647\n",
      "Graph 70: Epoch: 019, Loss: -0.5750\n",
      "Graph 70: Epoch: 020, Loss: -0.5556\n",
      "Graph 70: Epoch: 021, Loss: -0.4226\n",
      "Graph 70: Epoch: 022, Loss: -0.3245\n",
      "Graph 70: Epoch: 023, Loss: -0.7392\n",
      "Graph 70: Epoch: 024, Loss: -0.5372\n",
      "Graph 70: Epoch: 025, Loss: -0.4348\n",
      "Graph 70: Epoch: 026, Loss: -0.5363\n",
      "Graph 70: Epoch: 027, Loss: -0.3788\n",
      "Graph 70: Epoch: 028, Loss: -0.4865\n",
      "Graph 70: Epoch: 029, Loss: -0.4346\n",
      "Graph 70: Epoch: 030, Loss: -0.4878\n",
      "Graph 70: Epoch: 031, Loss: -0.4367\n",
      "Graph 70: Epoch: 032, Loss: -0.4378\n",
      "Graph 70: Epoch: 033, Loss: -0.5429\n",
      "Graph 70: Epoch: 034, Loss: -0.6029\n",
      "Graph 70: Epoch: 035, Loss: -0.3826\n",
      "Graph 70: Epoch: 036, Loss: -0.3842\n",
      "Graph 70: Epoch: 037, Loss: -0.4389\n",
      "Graph 70: Epoch: 038, Loss: -0.6028\n",
      "Graph 70: Epoch: 039, Loss: -0.3846\n",
      "Graph 70: Epoch: 040, Loss: -0.3295\n",
      "Graph 70: Epoch: 041, Loss: -0.4942\n",
      "Graph 70: Epoch: 042, Loss: -0.5495\n",
      "Graph 70: Epoch: 043, Loss: -0.4949\n",
      "Graph 70: Epoch: 044, Loss: -0.6596\n",
      "Graph 70: Epoch: 045, Loss: -0.6051\n",
      "Graph 70: Epoch: 046, Loss: -0.4402\n",
      "Graph 70: Epoch: 047, Loss: -0.4398\n",
      "Graph 70: Epoch: 048, Loss: -0.6603\n",
      "Graph 70: Epoch: 049, Loss: -0.2203\n",
      "Graph 70: Epoch: 050, Loss: -0.3305\n",
      "Graph 70: Epoch: 051, Loss: -0.4955\n",
      "Graph 70: Epoch: 052, Loss: -0.3305\n",
      "Graph 70: Epoch: 053, Loss: -0.4405\n",
      "Graph 70: Epoch: 054, Loss: -0.4404\n",
      "Graph 70: Epoch: 055, Loss: -0.3852\n",
      "Graph 70: Epoch: 056, Loss: -0.6058\n",
      "Graph 70: Epoch: 057, Loss: -0.6054\n",
      "Graph 70: Epoch: 058, Loss: -0.3858\n",
      "Graph 70: Epoch: 059, Loss: -0.6061\n",
      "Graph 70: Epoch: 060, Loss: -0.3858\n",
      "Graph 70: Epoch: 061, Loss: -0.5514\n",
      "Graph 70: Epoch: 062, Loss: -0.4412\n",
      "Graph 70: Epoch: 063, Loss: -0.5514\n",
      "Graph 70: Epoch: 064, Loss: -0.3861\n",
      "Graph 70: Epoch: 065, Loss: -0.3311\n",
      "Graph 70: Epoch: 066, Loss: -0.6067\n",
      "Graph 70: Epoch: 067, Loss: -0.4414\n",
      "Graph 70: Epoch: 068, Loss: -0.5517\n",
      "Graph 70: Epoch: 069, Loss: -0.4415\n",
      "Graph 70: Epoch: 070, Loss: -0.5517\n",
      "Graph 70: Epoch: 071, Loss: -0.4965\n",
      "Graph 70: Epoch: 072, Loss: -0.4967\n",
      "Graph 70: Epoch: 073, Loss: -0.4968\n",
      "Graph 70: Epoch: 074, Loss: -0.3865\n",
      "Graph 70: Epoch: 075, Loss: -0.4416\n",
      "Graph 70: Epoch: 076, Loss: -0.4966\n",
      "Graph 70: Epoch: 077, Loss: -0.4969\n",
      "Graph 70: Epoch: 078, Loss: -0.4418\n",
      "Graph 70: Epoch: 079, Loss: -0.4968\n",
      "Graph 70: Epoch: 080, Loss: -0.5522\n",
      "Graph 70: Epoch: 081, Loss: -0.6075\n",
      "Graph 70: Epoch: 082, Loss: -0.5523\n",
      "Graph 70: Epoch: 083, Loss: -0.6626\n",
      "Graph 70: Epoch: 084, Loss: -0.6076\n",
      "Graph 70: Epoch: 085, Loss: -0.5523\n",
      "Graph 70: Epoch: 086, Loss: -0.6629\n",
      "Graph 70: Epoch: 087, Loss: -0.4932\n",
      "Graph 70: Epoch: 088, Loss: -0.2212\n",
      "Graph 70: Epoch: 089, Loss: -0.4419\n",
      "Graph 70: Epoch: 090, Loss: -0.5525\n",
      "Graph 70: Epoch: 091, Loss: -0.6628\n",
      "Graph 70: Epoch: 092, Loss: -0.5523\n",
      "Graph 70: Epoch: 093, Loss: -0.6073\n",
      "Graph 70: Epoch: 094, Loss: -0.4971\n",
      "Graph 70: Epoch: 095, Loss: -0.4414\n",
      "Graph 70: Epoch: 096, Loss: -0.6624\n",
      "Graph 70: Epoch: 097, Loss: -0.5522\n",
      "Graph 70: Epoch: 098, Loss: -0.4971\n",
      "Graph 70: Epoch: 099, Loss: -0.4966\n",
      "Graph 70: Epoch: 100, Loss: -0.5520\n",
      "Graph 71: Epoch: 001, Loss: -0.0800\n",
      "Graph 71: Epoch: 002, Loss: -0.1808\n",
      "Graph 71: Epoch: 003, Loss: -0.1492\n",
      "Graph 71: Epoch: 004, Loss: -0.2169\n",
      "Graph 71: Epoch: 005, Loss: -0.1082\n",
      "Graph 71: Epoch: 006, Loss: -0.3078\n",
      "Graph 71: Epoch: 007, Loss: -0.3266\n",
      "Graph 71: Epoch: 008, Loss: -0.4532\n",
      "Graph 71: Epoch: 009, Loss: -0.3421\n",
      "Graph 71: Epoch: 010, Loss: -0.2457\n",
      "Graph 71: Epoch: 011, Loss: -0.4221\n",
      "Graph 71: Epoch: 012, Loss: -0.4463\n",
      "Graph 71: Epoch: 013, Loss: -0.2993\n",
      "Graph 71: Epoch: 014, Loss: -0.2182\n",
      "Graph 71: Epoch: 015, Loss: -0.3700\n",
      "Graph 71: Epoch: 016, Loss: -0.3386\n",
      "Graph 71: Epoch: 017, Loss: -0.5458\n",
      "Graph 71: Epoch: 018, Loss: -0.6344\n",
      "Graph 71: Epoch: 019, Loss: -0.3964\n",
      "Graph 71: Epoch: 020, Loss: -0.5064\n",
      "Graph 71: Epoch: 021, Loss: -0.3581\n",
      "Graph 71: Epoch: 022, Loss: -0.5290\n",
      "Graph 71: Epoch: 023, Loss: -0.2832\n",
      "Graph 71: Epoch: 024, Loss: -0.4783\n",
      "Graph 71: Epoch: 025, Loss: -0.4154\n",
      "Graph 71: Epoch: 026, Loss: -0.2074\n",
      "Graph 71: Epoch: 027, Loss: -0.3684\n",
      "Graph 71: Epoch: 028, Loss: -0.3569\n",
      "Graph 71: Epoch: 029, Loss: -0.6620\n",
      "Graph 71: Epoch: 030, Loss: -0.1626\n",
      "Graph 71: Epoch: 031, Loss: -0.7633\n",
      "Graph 71: Epoch: 032, Loss: -0.2750\n",
      "Graph 71: Epoch: 033, Loss: -0.2315\n",
      "Graph 71: Epoch: 034, Loss: -0.6973\n",
      "Graph 71: Epoch: 035, Loss: -0.6116\n",
      "Graph 71: Epoch: 036, Loss: -0.5396\n",
      "Graph 71: Epoch: 037, Loss: -0.1928\n",
      "Graph 71: Epoch: 038, Loss: -0.2380\n",
      "Graph 71: Epoch: 039, Loss: -0.2508\n",
      "Graph 71: Epoch: 040, Loss: -0.0616\n",
      "Graph 71: Epoch: 041, Loss: -0.2567\n",
      "Graph 71: Epoch: 042, Loss: -0.5654\n",
      "Graph 71: Epoch: 043, Loss: -0.2362\n",
      "Graph 71: Epoch: 044, Loss: -0.2069\n",
      "Graph 71: Epoch: 045, Loss: -0.3601\n",
      "Graph 71: Epoch: 046, Loss: -0.1877\n",
      "Graph 71: Epoch: 047, Loss: -0.7559\n",
      "Graph 71: Epoch: 048, Loss: -0.5685\n",
      "Graph 71: Epoch: 049, Loss: -0.3975\n",
      "Graph 71: Epoch: 050, Loss: -0.5716\n",
      "Graph 71: Epoch: 051, Loss: -0.5566\n",
      "Graph 71: Epoch: 052, Loss: -0.9265\n",
      "Graph 71: Epoch: 053, Loss: -0.8220\n",
      "Graph 71: Epoch: 054, Loss: -0.3914\n",
      "Graph 71: Epoch: 055, Loss: -0.0226\n",
      "Graph 71: Epoch: 056, Loss: -0.3928\n",
      "Graph 71: Epoch: 057, Loss: -0.7623\n",
      "Graph 71: Epoch: 058, Loss: -0.7319\n",
      "Graph 71: Epoch: 059, Loss: -0.7477\n",
      "Graph 71: Epoch: 060, Loss: -0.4392\n",
      "Graph 71: Epoch: 061, Loss: -0.3449\n",
      "Graph 71: Epoch: 062, Loss: -0.3942\n",
      "Graph 71: Epoch: 063, Loss: -0.4798\n",
      "Graph 71: Epoch: 064, Loss: -0.7459\n",
      "Graph 71: Epoch: 065, Loss: -0.9386\n",
      "Graph 71: Epoch: 066, Loss: -0.3076\n",
      "Graph 71: Epoch: 067, Loss: -0.1985\n",
      "Graph 71: Epoch: 068, Loss: -0.3125\n",
      "Graph 71: Epoch: 069, Loss: -0.3926\n",
      "Graph 71: Epoch: 070, Loss: -0.6641\n",
      "Graph 71: Epoch: 071, Loss: -0.1983\n",
      "Graph 71: Epoch: 072, Loss: -0.2542\n",
      "Graph 71: Epoch: 073, Loss: -0.2166\n",
      "Graph 71: Epoch: 074, Loss: -0.5807\n",
      "Graph 71: Epoch: 075, Loss: -0.3951\n",
      "Graph 71: Epoch: 076, Loss: -0.4364\n",
      "Graph 71: Epoch: 077, Loss: -0.5832\n",
      "Graph 71: Epoch: 078, Loss: -0.7679\n",
      "Graph 71: Epoch: 079, Loss: -0.7692\n",
      "Graph 71: Epoch: 080, Loss: -0.5604\n",
      "Graph 71: Epoch: 081, Loss: -0.5864\n",
      "Graph 71: Epoch: 082, Loss: -0.3962\n",
      "Graph 71: Epoch: 083, Loss: -0.2253\n",
      "Graph 71: Epoch: 084, Loss: -0.3951\n",
      "Graph 71: Epoch: 085, Loss: -0.7782\n",
      "Graph 71: Epoch: 086, Loss: -0.2095\n",
      "Graph 71: Epoch: 087, Loss: -0.3991\n",
      "Graph 71: Epoch: 088, Loss: -0.7806\n",
      "Graph 71: Epoch: 089, Loss: -0.5008\n",
      "Graph 71: Epoch: 090, Loss: -0.5986\n",
      "Graph 71: Epoch: 091, Loss: -0.5484\n",
      "Graph 71: Epoch: 092, Loss: -0.3944\n",
      "Graph 71: Epoch: 093, Loss: -0.2068\n",
      "Graph 71: Epoch: 094, Loss: -0.3799\n",
      "Graph 71: Epoch: 095, Loss: -0.6697\n",
      "Graph 71: Epoch: 096, Loss: -0.4519\n",
      "Graph 71: Epoch: 097, Loss: -0.1720\n",
      "Graph 71: Epoch: 098, Loss: -0.5880\n",
      "Graph 71: Epoch: 099, Loss: -0.3938\n",
      "Graph 71: Epoch: 100, Loss: -0.3016\n",
      "Graph 72: Epoch: 001, Loss: -0.0886\n",
      "Graph 72: Epoch: 002, Loss: -0.0738\n",
      "Graph 72: Epoch: 003, Loss: -0.2842\n",
      "Graph 72: Epoch: 004, Loss: -0.3173\n",
      "Graph 72: Epoch: 005, Loss: -0.3919\n",
      "Graph 72: Epoch: 006, Loss: -0.4351\n",
      "Graph 72: Epoch: 007, Loss: -0.4074\n",
      "Graph 72: Epoch: 008, Loss: -0.5000\n",
      "Graph 72: Epoch: 009, Loss: -0.5310\n",
      "Graph 72: Epoch: 010, Loss: -0.2900\n",
      "Graph 72: Epoch: 011, Loss: -0.4007\n",
      "Graph 72: Epoch: 012, Loss: -0.6866\n",
      "Graph 72: Epoch: 013, Loss: -0.6920\n",
      "Graph 72: Epoch: 014, Loss: -0.4691\n",
      "Graph 72: Epoch: 015, Loss: -0.4084\n",
      "Graph 72: Epoch: 016, Loss: -0.5012\n",
      "Graph 72: Epoch: 017, Loss: -0.5068\n",
      "Graph 72: Epoch: 018, Loss: -0.4784\n",
      "Graph 72: Epoch: 019, Loss: -0.4604\n",
      "Graph 72: Epoch: 020, Loss: -0.4808\n",
      "Graph 72: Epoch: 021, Loss: -0.5441\n",
      "Graph 72: Epoch: 022, Loss: -0.6049\n",
      "Graph 72: Epoch: 023, Loss: -0.4852\n",
      "Graph 72: Epoch: 024, Loss: -0.5434\n",
      "Graph 72: Epoch: 025, Loss: -0.4237\n",
      "Graph 72: Epoch: 026, Loss: -0.4228\n",
      "Graph 72: Epoch: 027, Loss: -0.4868\n",
      "Graph 72: Epoch: 028, Loss: -0.6081\n",
      "Graph 72: Epoch: 029, Loss: -0.4884\n",
      "Graph 72: Epoch: 030, Loss: -0.4275\n",
      "Graph 72: Epoch: 031, Loss: -0.4828\n",
      "Graph 72: Epoch: 032, Loss: -0.6121\n",
      "Graph 72: Epoch: 033, Loss: -0.3066\n",
      "Graph 72: Epoch: 034, Loss: -0.5519\n",
      "Graph 72: Epoch: 035, Loss: -0.6756\n",
      "Graph 72: Epoch: 036, Loss: -0.2461\n",
      "Graph 72: Epoch: 037, Loss: -0.5528\n",
      "Graph 72: Epoch: 038, Loss: -0.6151\n",
      "Graph 72: Epoch: 039, Loss: -0.6147\n",
      "Graph 72: Epoch: 040, Loss: -0.7384\n",
      "Graph 72: Epoch: 041, Loss: -0.3695\n",
      "Graph 72: Epoch: 042, Loss: -0.6145\n",
      "Graph 72: Epoch: 043, Loss: -0.3085\n",
      "Graph 72: Epoch: 044, Loss: -0.5540\n",
      "Graph 72: Epoch: 045, Loss: -0.6164\n",
      "Graph 72: Epoch: 046, Loss: -0.4937\n",
      "Graph 72: Epoch: 047, Loss: -0.4317\n",
      "Graph 72: Epoch: 048, Loss: -0.4320\n",
      "Graph 72: Epoch: 049, Loss: -0.3702\n",
      "Graph 72: Epoch: 050, Loss: -0.5560\n",
      "Graph 72: Epoch: 051, Loss: -0.6159\n",
      "Graph 72: Epoch: 052, Loss: -0.3092\n",
      "Graph 72: Epoch: 053, Loss: -0.6181\n",
      "Graph 72: Epoch: 054, Loss: -0.4328\n",
      "Graph 72: Epoch: 055, Loss: -0.4324\n",
      "Graph 72: Epoch: 056, Loss: -0.6179\n",
      "Graph 72: Epoch: 057, Loss: -0.4332\n",
      "Graph 72: Epoch: 058, Loss: -0.4951\n",
      "Graph 72: Epoch: 059, Loss: -0.6804\n",
      "Graph 72: Epoch: 060, Loss: -0.4327\n",
      "Graph 72: Epoch: 061, Loss: -0.6809\n",
      "Graph 72: Epoch: 062, Loss: -0.3714\n",
      "Graph 72: Epoch: 063, Loss: -0.6807\n",
      "Graph 72: Epoch: 064, Loss: -0.4954\n",
      "Graph 72: Epoch: 065, Loss: -0.5573\n",
      "Graph 72: Epoch: 066, Loss: -0.5574\n",
      "Graph 72: Epoch: 067, Loss: -0.5576\n",
      "Graph 72: Epoch: 068, Loss: -0.3718\n",
      "Graph 72: Epoch: 069, Loss: -0.6196\n",
      "Graph 72: Epoch: 070, Loss: -0.3719\n",
      "Graph 72: Epoch: 071, Loss: -0.6821\n",
      "Graph 72: Epoch: 072, Loss: -0.3102\n",
      "Graph 72: Epoch: 073, Loss: -0.4960\n",
      "Graph 72: Epoch: 074, Loss: -0.6199\n",
      "Graph 72: Epoch: 075, Loss: -0.2481\n",
      "Graph 72: Epoch: 076, Loss: -0.4398\n",
      "Graph 72: Epoch: 077, Loss: -0.6201\n",
      "Graph 72: Epoch: 078, Loss: -0.3724\n",
      "Graph 72: Epoch: 079, Loss: -0.4936\n",
      "Graph 72: Epoch: 080, Loss: -0.4961\n",
      "Graph 72: Epoch: 081, Loss: -0.3724\n",
      "Graph 72: Epoch: 082, Loss: -0.4965\n",
      "Graph 72: Epoch: 083, Loss: -0.6204\n",
      "Graph 72: Epoch: 084, Loss: -0.6820\n",
      "Graph 72: Epoch: 085, Loss: -0.7446\n",
      "Graph 72: Epoch: 086, Loss: -0.5587\n",
      "Graph 72: Epoch: 087, Loss: -0.3725\n",
      "Graph 72: Epoch: 088, Loss: -0.4346\n",
      "Graph 72: Epoch: 089, Loss: -0.5587\n",
      "Graph 72: Epoch: 090, Loss: -0.5586\n",
      "Graph 72: Epoch: 091, Loss: -0.6210\n",
      "Graph 72: Epoch: 092, Loss: -0.4970\n",
      "Graph 72: Epoch: 093, Loss: -0.4970\n",
      "Graph 72: Epoch: 094, Loss: -0.3727\n",
      "Graph 72: Epoch: 095, Loss: -0.6210\n",
      "Graph 72: Epoch: 096, Loss: -0.4349\n",
      "Graph 72: Epoch: 097, Loss: -0.4971\n",
      "Graph 72: Epoch: 098, Loss: -0.4932\n",
      "Graph 72: Epoch: 099, Loss: -0.4958\n",
      "Graph 72: Epoch: 100, Loss: -0.6214\n",
      "Graph 73: Epoch: 001, Loss: -0.0489\n",
      "Graph 73: Epoch: 002, Loss: -0.1544\n",
      "Graph 73: Epoch: 003, Loss: -0.3920\n",
      "Graph 73: Epoch: 004, Loss: -0.3095\n",
      "Graph 73: Epoch: 005, Loss: -0.3945\n",
      "Graph 73: Epoch: 006, Loss: -0.6676\n",
      "Graph 73: Epoch: 007, Loss: -0.4870\n",
      "Graph 73: Epoch: 008, Loss: -0.2231\n",
      "Graph 73: Epoch: 009, Loss: -0.4294\n",
      "Graph 73: Epoch: 010, Loss: -0.3486\n",
      "Graph 73: Epoch: 011, Loss: -0.6951\n",
      "Graph 73: Epoch: 012, Loss: -0.4226\n",
      "Graph 73: Epoch: 013, Loss: -0.4237\n",
      "Graph 73: Epoch: 014, Loss: -0.4263\n",
      "Graph 73: Epoch: 015, Loss: -0.0734\n",
      "Graph 73: Epoch: 016, Loss: -0.4081\n",
      "Graph 73: Epoch: 017, Loss: -0.3590\n",
      "Graph 73: Epoch: 018, Loss: -0.3590\n",
      "Graph 73: Epoch: 019, Loss: -0.5007\n",
      "Graph 73: Epoch: 020, Loss: -0.4320\n",
      "Graph 73: Epoch: 021, Loss: -0.2944\n",
      "Graph 73: Epoch: 022, Loss: -0.2171\n",
      "Graph 73: Epoch: 023, Loss: -0.2958\n",
      "Graph 73: Epoch: 024, Loss: -0.2952\n",
      "Graph 73: Epoch: 025, Loss: -0.5908\n",
      "Graph 73: Epoch: 026, Loss: -0.3702\n",
      "Graph 73: Epoch: 027, Loss: -0.5911\n",
      "Graph 73: Epoch: 028, Loss: -0.5943\n",
      "Graph 73: Epoch: 029, Loss: -0.4459\n",
      "Graph 73: Epoch: 030, Loss: -0.5200\n",
      "Graph 73: Epoch: 031, Loss: -0.6711\n",
      "Graph 73: Epoch: 032, Loss: -0.6704\n",
      "Graph 73: Epoch: 033, Loss: -0.5974\n",
      "Graph 73: Epoch: 034, Loss: -0.6735\n",
      "Graph 73: Epoch: 035, Loss: -0.3737\n",
      "Graph 73: Epoch: 036, Loss: -0.5247\n",
      "Graph 73: Epoch: 037, Loss: -0.6002\n",
      "Graph 73: Epoch: 038, Loss: -0.6684\n",
      "Graph 73: Epoch: 039, Loss: -0.6756\n",
      "Graph 73: Epoch: 040, Loss: -0.7519\n",
      "Graph 73: Epoch: 041, Loss: -0.4520\n",
      "Graph 73: Epoch: 042, Loss: -0.5272\n",
      "Graph 73: Epoch: 043, Loss: -0.5988\n",
      "Graph 73: Epoch: 044, Loss: -0.3777\n",
      "Graph 73: Epoch: 045, Loss: -0.3781\n",
      "Graph 73: Epoch: 046, Loss: -0.4536\n",
      "Graph 73: Epoch: 047, Loss: -0.5291\n",
      "Graph 73: Epoch: 048, Loss: -0.5297\n",
      "Graph 73: Epoch: 049, Loss: -0.5267\n",
      "Graph 73: Epoch: 050, Loss: -0.5299\n",
      "Graph 73: Epoch: 051, Loss: -0.3782\n",
      "Graph 73: Epoch: 052, Loss: -0.5304\n",
      "Graph 73: Epoch: 053, Loss: -0.4546\n",
      "Graph 73: Epoch: 054, Loss: -0.5305\n",
      "Graph 73: Epoch: 055, Loss: -0.5302\n",
      "Graph 73: Epoch: 056, Loss: -0.3039\n",
      "Graph 73: Epoch: 057, Loss: -0.6069\n",
      "Graph 73: Epoch: 058, Loss: -0.4558\n",
      "Graph 73: Epoch: 059, Loss: -0.2283\n",
      "Graph 73: Epoch: 060, Loss: -0.3799\n",
      "Graph 73: Epoch: 061, Loss: -0.6831\n",
      "Graph 73: Epoch: 062, Loss: -0.4558\n",
      "Graph 73: Epoch: 063, Loss: -0.6064\n",
      "Graph 73: Epoch: 064, Loss: -0.7599\n",
      "Graph 73: Epoch: 065, Loss: -0.5324\n",
      "Graph 73: Epoch: 066, Loss: -0.6055\n",
      "Graph 73: Epoch: 067, Loss: -0.5328\n",
      "Graph 73: Epoch: 068, Loss: -0.3808\n",
      "Graph 73: Epoch: 069, Loss: -0.5328\n",
      "Graph 73: Epoch: 070, Loss: -0.3808\n",
      "Graph 73: Epoch: 071, Loss: -0.5323\n",
      "Graph 73: Epoch: 072, Loss: -0.5329\n",
      "Graph 73: Epoch: 073, Loss: -0.4569\n",
      "Graph 73: Epoch: 074, Loss: -0.6835\n",
      "Graph 73: Epoch: 075, Loss: -0.3812\n",
      "Graph 73: Epoch: 076, Loss: -0.5328\n",
      "Graph 73: Epoch: 077, Loss: -0.2290\n",
      "Graph 73: Epoch: 078, Loss: -0.5331\n",
      "Graph 73: Epoch: 079, Loss: -0.3809\n",
      "Graph 73: Epoch: 080, Loss: -0.5336\n",
      "Graph 73: Epoch: 081, Loss: -0.5335\n",
      "Graph 73: Epoch: 082, Loss: -0.6857\n",
      "Graph 73: Epoch: 083, Loss: -0.3052\n",
      "Graph 73: Epoch: 084, Loss: -0.7624\n",
      "Graph 73: Epoch: 085, Loss: -0.4579\n",
      "Graph 73: Epoch: 086, Loss: -0.6866\n",
      "Graph 73: Epoch: 087, Loss: -0.4578\n",
      "Graph 73: Epoch: 088, Loss: -0.6867\n",
      "Graph 73: Epoch: 089, Loss: -0.4583\n",
      "Graph 73: Epoch: 090, Loss: -0.4580\n",
      "Graph 73: Epoch: 091, Loss: -0.2292\n",
      "Graph 73: Epoch: 092, Loss: -0.3053\n",
      "Graph 73: Epoch: 093, Loss: -0.3819\n",
      "Graph 73: Epoch: 094, Loss: -0.6104\n",
      "Graph 73: Epoch: 095, Loss: -0.4581\n",
      "Graph 73: Epoch: 096, Loss: -0.5342\n",
      "Graph 73: Epoch: 097, Loss: -0.4583\n",
      "Graph 73: Epoch: 098, Loss: -0.6109\n",
      "Graph 73: Epoch: 099, Loss: -0.3058\n",
      "Graph 73: Epoch: 100, Loss: -0.3058\n",
      "Graph 74: Epoch: 001, Loss: -0.0299\n",
      "Graph 74: Epoch: 002, Loss: -0.0202\n",
      "Graph 74: Epoch: 003, Loss: -0.0700\n",
      "Graph 74: Epoch: 004, Loss: -0.1616\n",
      "Graph 74: Epoch: 005, Loss: -0.2199\n",
      "Graph 74: Epoch: 006, Loss: -0.2352\n",
      "Graph 74: Epoch: 007, Loss: -0.3399\n",
      "Graph 74: Epoch: 008, Loss: -0.2378\n",
      "Graph 74: Epoch: 009, Loss: -0.2793\n",
      "Graph 74: Epoch: 010, Loss: -0.2711\n",
      "Graph 74: Epoch: 011, Loss: -0.3452\n",
      "Graph 74: Epoch: 012, Loss: -0.3508\n",
      "Graph 74: Epoch: 013, Loss: -0.3515\n",
      "Graph 74: Epoch: 014, Loss: -0.3662\n",
      "Graph 74: Epoch: 015, Loss: -0.3719\n",
      "Graph 74: Epoch: 016, Loss: -0.4352\n",
      "Graph 74: Epoch: 017, Loss: -0.3883\n",
      "Graph 74: Epoch: 018, Loss: -0.2846\n",
      "Graph 74: Epoch: 019, Loss: -0.3797\n",
      "Graph 74: Epoch: 020, Loss: -0.3479\n",
      "Graph 74: Epoch: 021, Loss: -0.4671\n",
      "Graph 74: Epoch: 022, Loss: -0.5072\n",
      "Graph 74: Epoch: 023, Loss: -0.4763\n",
      "Graph 74: Epoch: 024, Loss: -0.3801\n",
      "Graph 74: Epoch: 025, Loss: -0.2957\n",
      "Graph 74: Epoch: 026, Loss: -0.3797\n",
      "Graph 74: Epoch: 027, Loss: -0.4852\n",
      "Graph 74: Epoch: 028, Loss: -0.4036\n",
      "Graph 74: Epoch: 029, Loss: -0.1294\n",
      "Graph 74: Epoch: 030, Loss: -0.5904\n",
      "Graph 74: Epoch: 031, Loss: -0.3099\n",
      "Graph 74: Epoch: 032, Loss: -0.6113\n",
      "Graph 74: Epoch: 033, Loss: -0.4289\n",
      "Graph 74: Epoch: 034, Loss: -0.5012\n",
      "Graph 74: Epoch: 035, Loss: -0.3829\n",
      "Graph 74: Epoch: 036, Loss: -0.7028\n",
      "Graph 74: Epoch: 037, Loss: -0.5156\n",
      "Graph 74: Epoch: 038, Loss: -0.3248\n",
      "Graph 74: Epoch: 039, Loss: -0.4174\n",
      "Graph 74: Epoch: 040, Loss: -0.6317\n",
      "Graph 74: Epoch: 041, Loss: -0.5166\n",
      "Graph 74: Epoch: 042, Loss: -0.4012\n",
      "Graph 74: Epoch: 043, Loss: -0.4187\n",
      "Graph 74: Epoch: 044, Loss: -0.4121\n",
      "Graph 74: Epoch: 045, Loss: -0.1521\n",
      "Graph 74: Epoch: 046, Loss: -0.4099\n",
      "Graph 74: Epoch: 047, Loss: -0.4308\n",
      "Graph 74: Epoch: 048, Loss: -0.4157\n",
      "Graph 74: Epoch: 049, Loss: -0.2817\n",
      "Graph 74: Epoch: 050, Loss: -0.7939\n",
      "Graph 74: Epoch: 051, Loss: -0.4183\n",
      "Graph 74: Epoch: 052, Loss: -0.2860\n",
      "Graph 74: Epoch: 053, Loss: -0.4180\n",
      "Graph 74: Epoch: 054, Loss: -0.5534\n",
      "Graph 74: Epoch: 055, Loss: -0.5379\n",
      "Graph 74: Epoch: 056, Loss: -0.1467\n",
      "Graph 74: Epoch: 057, Loss: -0.4172\n",
      "Graph 74: Epoch: 058, Loss: -0.6841\n",
      "Graph 74: Epoch: 059, Loss: -0.5547\n",
      "Graph 74: Epoch: 060, Loss: -0.2819\n",
      "Graph 74: Epoch: 061, Loss: -0.4180\n",
      "Graph 74: Epoch: 062, Loss: -0.8204\n",
      "Graph 74: Epoch: 063, Loss: -0.8245\n",
      "Graph 74: Epoch: 064, Loss: -0.5553\n",
      "Graph 74: Epoch: 065, Loss: -0.2836\n",
      "Graph 74: Epoch: 066, Loss: -0.8308\n",
      "Graph 74: Epoch: 067, Loss: -0.5551\n",
      "Graph 74: Epoch: 068, Loss: -0.2832\n",
      "Graph 74: Epoch: 069, Loss: -0.4423\n",
      "Graph 74: Epoch: 070, Loss: -0.5598\n",
      "Graph 74: Epoch: 071, Loss: -0.5529\n",
      "Graph 74: Epoch: 072, Loss: -0.8326\n",
      "Graph 74: Epoch: 073, Loss: -0.4218\n",
      "Graph 74: Epoch: 074, Loss: -0.6974\n",
      "Graph 74: Epoch: 075, Loss: -0.5794\n",
      "Graph 74: Epoch: 076, Loss: -0.5609\n",
      "Graph 74: Epoch: 077, Loss: -0.5594\n",
      "Graph 74: Epoch: 078, Loss: -0.7001\n",
      "Graph 74: Epoch: 079, Loss: -0.1429\n",
      "Graph 74: Epoch: 080, Loss: -0.4216\n",
      "Graph 74: Epoch: 081, Loss: -0.6976\n",
      "Graph 74: Epoch: 082, Loss: -0.5611\n",
      "Graph 74: Epoch: 083, Loss: -0.5597\n",
      "Graph 74: Epoch: 084, Loss: -0.4225\n",
      "Graph 74: Epoch: 085, Loss: -0.5621\n",
      "Graph 74: Epoch: 086, Loss: -0.3060\n",
      "Graph 74: Epoch: 087, Loss: -0.4231\n",
      "Graph 74: Epoch: 088, Loss: -0.7010\n",
      "Graph 74: Epoch: 089, Loss: -0.5626\n",
      "Graph 74: Epoch: 090, Loss: -0.2839\n",
      "Graph 74: Epoch: 091, Loss: -0.1454\n",
      "Graph 74: Epoch: 092, Loss: -0.5605\n",
      "Graph 74: Epoch: 093, Loss: -0.4689\n",
      "Graph 74: Epoch: 094, Loss: -0.4231\n",
      "Graph 74: Epoch: 095, Loss: -0.3152\n",
      "Graph 74: Epoch: 096, Loss: -0.4213\n",
      "Graph 74: Epoch: 097, Loss: -0.9829\n",
      "Graph 74: Epoch: 098, Loss: -0.4228\n",
      "Graph 74: Epoch: 099, Loss: -0.3364\n",
      "Graph 74: Epoch: 100, Loss: -0.4235\n",
      "Graph 75: Epoch: 001, Loss: -0.1342\n",
      "Graph 75: Epoch: 002, Loss: -0.1454\n",
      "Graph 75: Epoch: 003, Loss: -0.2105\n",
      "Graph 75: Epoch: 004, Loss: -0.3586\n",
      "Graph 75: Epoch: 005, Loss: -0.2564\n",
      "Graph 75: Epoch: 006, Loss: -0.4755\n",
      "Graph 75: Epoch: 007, Loss: -0.2886\n",
      "Graph 75: Epoch: 008, Loss: -0.3649\n",
      "Graph 75: Epoch: 009, Loss: -0.4742\n",
      "Graph 75: Epoch: 010, Loss: -0.3743\n",
      "Graph 75: Epoch: 011, Loss: -0.1140\n",
      "Graph 75: Epoch: 012, Loss: -0.4140\n",
      "Graph 75: Epoch: 013, Loss: -0.4440\n",
      "Graph 75: Epoch: 014, Loss: -0.4266\n",
      "Graph 75: Epoch: 015, Loss: -0.4426\n",
      "Graph 75: Epoch: 016, Loss: -0.5313\n",
      "Graph 75: Epoch: 017, Loss: -0.4252\n",
      "Graph 75: Epoch: 018, Loss: -0.6081\n",
      "Graph 75: Epoch: 019, Loss: -0.4437\n",
      "Graph 75: Epoch: 020, Loss: -0.5509\n",
      "Graph 75: Epoch: 021, Loss: -0.2143\n",
      "Graph 75: Epoch: 022, Loss: -0.2329\n",
      "Graph 75: Epoch: 023, Loss: -0.3470\n",
      "Graph 75: Epoch: 024, Loss: -0.5571\n",
      "Graph 75: Epoch: 025, Loss: -0.3519\n",
      "Graph 75: Epoch: 026, Loss: -0.4654\n",
      "Graph 75: Epoch: 027, Loss: -0.6724\n",
      "Graph 75: Epoch: 028, Loss: -0.4717\n",
      "Graph 75: Epoch: 029, Loss: -0.4619\n",
      "Graph 75: Epoch: 030, Loss: -0.3578\n",
      "Graph 75: Epoch: 031, Loss: -0.5858\n",
      "Graph 75: Epoch: 032, Loss: -0.6993\n",
      "Graph 75: Epoch: 033, Loss: -0.4557\n",
      "Graph 75: Epoch: 034, Loss: -0.5917\n",
      "Graph 75: Epoch: 035, Loss: -0.2339\n",
      "Graph 75: Epoch: 036, Loss: -0.5985\n",
      "Graph 75: Epoch: 037, Loss: -0.2400\n",
      "Graph 75: Epoch: 038, Loss: -0.6003\n",
      "Graph 75: Epoch: 039, Loss: -0.1248\n",
      "Graph 75: Epoch: 040, Loss: -0.7148\n",
      "Graph 75: Epoch: 041, Loss: -0.4670\n",
      "Graph 75: Epoch: 042, Loss: -0.4708\n",
      "Graph 75: Epoch: 043, Loss: -0.5873\n",
      "Graph 75: Epoch: 044, Loss: -0.7268\n",
      "Graph 75: Epoch: 045, Loss: -0.4761\n",
      "Graph 75: Epoch: 046, Loss: -0.3635\n",
      "Graph 75: Epoch: 047, Loss: -0.5026\n",
      "Graph 75: Epoch: 048, Loss: -0.2442\n",
      "Graph 75: Epoch: 049, Loss: -0.4866\n",
      "Graph 75: Epoch: 050, Loss: -0.2505\n",
      "Graph 75: Epoch: 051, Loss: -0.1243\n",
      "Graph 75: Epoch: 052, Loss: -0.3659\n",
      "Graph 75: Epoch: 053, Loss: -0.6062\n",
      "Graph 75: Epoch: 054, Loss: -0.5990\n",
      "Graph 75: Epoch: 055, Loss: -0.3708\n",
      "Graph 75: Epoch: 056, Loss: -0.4810\n",
      "Graph 75: Epoch: 057, Loss: -0.2426\n",
      "Graph 75: Epoch: 058, Loss: -0.4775\n",
      "Graph 75: Epoch: 059, Loss: -0.4839\n",
      "Graph 75: Epoch: 060, Loss: -0.6045\n",
      "Graph 75: Epoch: 061, Loss: -0.2468\n",
      "Graph 75: Epoch: 062, Loss: -0.7268\n",
      "Graph 75: Epoch: 063, Loss: -0.2645\n",
      "Graph 75: Epoch: 064, Loss: -0.3611\n",
      "Graph 75: Epoch: 065, Loss: -0.6082\n",
      "Graph 75: Epoch: 066, Loss: -0.7289\n",
      "Graph 75: Epoch: 067, Loss: -0.1262\n",
      "Graph 75: Epoch: 068, Loss: -0.8439\n",
      "Graph 75: Epoch: 069, Loss: -0.4906\n",
      "Graph 75: Epoch: 070, Loss: -0.4888\n",
      "Graph 75: Epoch: 071, Loss: -0.6082\n",
      "Graph 75: Epoch: 072, Loss: -0.8561\n",
      "Graph 75: Epoch: 073, Loss: -0.7873\n",
      "Graph 75: Epoch: 074, Loss: -0.4911\n",
      "Graph 75: Epoch: 075, Loss: -0.7342\n",
      "Graph 75: Epoch: 076, Loss: -0.7340\n",
      "Graph 75: Epoch: 077, Loss: -0.3670\n",
      "Graph 75: Epoch: 078, Loss: -0.4882\n",
      "Graph 75: Epoch: 079, Loss: -0.3701\n",
      "Graph 75: Epoch: 080, Loss: -0.4921\n",
      "Graph 75: Epoch: 081, Loss: -0.4922\n",
      "Graph 75: Epoch: 082, Loss: -0.7361\n",
      "Graph 75: Epoch: 083, Loss: -0.6529\n",
      "Graph 75: Epoch: 084, Loss: -0.3683\n",
      "Graph 75: Epoch: 085, Loss: -0.4943\n",
      "Graph 75: Epoch: 086, Loss: -0.4919\n",
      "Graph 75: Epoch: 087, Loss: -0.3701\n",
      "Graph 75: Epoch: 088, Loss: -0.6074\n",
      "Graph 75: Epoch: 089, Loss: -0.4862\n",
      "Graph 75: Epoch: 090, Loss: -0.6156\n",
      "Graph 75: Epoch: 091, Loss: -0.6149\n",
      "Graph 75: Epoch: 092, Loss: -0.8608\n",
      "Graph 75: Epoch: 093, Loss: -0.4942\n",
      "Graph 75: Epoch: 094, Loss: -0.4219\n",
      "Graph 75: Epoch: 095, Loss: -0.4932\n",
      "Graph 75: Epoch: 096, Loss: -0.8624\n",
      "Graph 75: Epoch: 097, Loss: -0.4937\n",
      "Graph 75: Epoch: 098, Loss: -0.2464\n",
      "Graph 75: Epoch: 099, Loss: -0.6779\n",
      "Graph 75: Epoch: 100, Loss: -0.2493\n",
      "Graph 76: Epoch: 001, Loss: -0.5000\n",
      "Graph 76: Epoch: 002, Loss: -0.4950\n",
      "Graph 76: Epoch: 003, Loss: -0.5047\n",
      "Graph 76: Epoch: 004, Loss: -0.5064\n",
      "Graph 76: Epoch: 005, Loss: -0.4909\n",
      "Graph 76: Epoch: 006, Loss: -0.5098\n",
      "Graph 76: Epoch: 007, Loss: -0.4885\n",
      "Graph 76: Epoch: 008, Loss: -0.4881\n",
      "Graph 76: Epoch: 009, Loss: -0.4886\n",
      "Graph 76: Epoch: 010, Loss: -0.5101\n",
      "Graph 76: Epoch: 011, Loss: -0.5098\n",
      "Graph 76: Epoch: 012, Loss: -0.5102\n",
      "Graph 76: Epoch: 013, Loss: -0.8760\n",
      "Graph 76: Epoch: 014, Loss: -0.4880\n",
      "Graph 76: Epoch: 015, Loss: -0.5119\n",
      "Graph 76: Epoch: 016, Loss: -0.5125\n",
      "Graph 76: Epoch: 017, Loss: -0.4864\n",
      "Graph 76: Epoch: 018, Loss: -0.4860\n",
      "Graph 76: Epoch: 019, Loss: -0.5138\n",
      "Graph 76: Epoch: 020, Loss: -0.5141\n",
      "Graph 76: Epoch: 021, Loss: -0.5150\n",
      "Graph 76: Epoch: 022, Loss: -0.1198\n",
      "Graph 76: Epoch: 023, Loss: -0.4821\n",
      "Graph 76: Epoch: 024, Loss: -0.4814\n",
      "Graph 76: Epoch: 025, Loss: -0.4813\n",
      "Graph 76: Epoch: 026, Loss: -0.5183\n",
      "Graph 76: Epoch: 027, Loss: -0.5184\n",
      "Graph 76: Epoch: 028, Loss: -0.4809\n",
      "Graph 76: Epoch: 029, Loss: -0.5191\n",
      "Graph 76: Epoch: 030, Loss: -0.4803\n",
      "Graph 76: Epoch: 031, Loss: -0.4803\n",
      "Graph 76: Epoch: 032, Loss: -0.5192\n",
      "Graph 76: Epoch: 033, Loss: -0.4808\n",
      "Graph 76: Epoch: 034, Loss: -0.1243\n",
      "Graph 76: Epoch: 035, Loss: -0.4815\n",
      "Graph 76: Epoch: 036, Loss: -0.4822\n",
      "Graph 76: Epoch: 037, Loss: -0.5166\n",
      "Graph 76: Epoch: 038, Loss: -0.5161\n",
      "Graph 76: Epoch: 039, Loss: -0.5161\n",
      "Graph 76: Epoch: 040, Loss: -0.4833\n",
      "Graph 76: Epoch: 041, Loss: -0.1282\n",
      "Graph 76: Epoch: 042, Loss: -0.5169\n",
      "Graph 76: Epoch: 043, Loss: -0.4823\n",
      "Graph 76: Epoch: 044, Loss: -0.4822\n",
      "Graph 76: Epoch: 045, Loss: -0.4826\n",
      "Graph 76: Epoch: 046, Loss: -0.1330\n",
      "Graph 76: Epoch: 047, Loss: -0.5159\n",
      "Graph 76: Epoch: 048, Loss: -0.5160\n",
      "Graph 76: Epoch: 049, Loss: -0.5165\n",
      "Graph 76: Epoch: 050, Loss: -0.4825\n",
      "Graph 76: Epoch: 051, Loss: -0.4821\n",
      "Graph 76: Epoch: 052, Loss: -0.4823\n",
      "Graph 76: Epoch: 053, Loss: -0.1408\n",
      "Graph 76: Epoch: 054, Loss: -0.5167\n",
      "Graph 76: Epoch: 055, Loss: -0.4831\n",
      "Graph 76: Epoch: 056, Loss: -0.5166\n",
      "Graph 76: Epoch: 057, Loss: -0.1458\n",
      "Graph 76: Epoch: 058, Loss: -0.1477\n",
      "Graph 76: Epoch: 059, Loss: -0.4821\n",
      "Graph 76: Epoch: 060, Loss: -0.5180\n",
      "Graph 76: Epoch: 061, Loss: -0.1545\n",
      "Graph 76: Epoch: 062, Loss: -0.5194\n",
      "Graph 76: Epoch: 063, Loss: -0.4793\n",
      "Graph 76: Epoch: 064, Loss: -0.4787\n",
      "Graph 76: Epoch: 065, Loss: -0.5213\n",
      "Graph 76: Epoch: 066, Loss: -0.5219\n",
      "Graph 76: Epoch: 067, Loss: -0.5229\n",
      "Graph 76: Epoch: 068, Loss: -0.1446\n",
      "Graph 76: Epoch: 069, Loss: -0.8542\n",
      "Graph 76: Epoch: 070, Loss: -0.8540\n",
      "Graph 76: Epoch: 071, Loss: -0.4720\n",
      "Graph 76: Epoch: 072, Loss: -0.5287\n",
      "Graph 76: Epoch: 073, Loss: -0.5298\n",
      "Graph 76: Epoch: 074, Loss: -0.4686\n",
      "Graph 76: Epoch: 075, Loss: -0.4677\n",
      "Graph 76: Epoch: 076, Loss: -0.5325\n",
      "Graph 76: Epoch: 077, Loss: -0.4667\n",
      "Graph 76: Epoch: 078, Loss: -0.5334\n",
      "Graph 76: Epoch: 079, Loss: -0.5341\n",
      "Graph 76: Epoch: 080, Loss: -0.5352\n",
      "Graph 76: Epoch: 081, Loss: -0.4632\n",
      "Graph 76: Epoch: 082, Loss: -0.4624\n",
      "Graph 76: Epoch: 083, Loss: -0.5379\n",
      "Graph 76: Epoch: 084, Loss: -0.4614\n",
      "Graph 76: Epoch: 085, Loss: -0.8592\n",
      "Graph 76: Epoch: 086, Loss: -0.4608\n",
      "Graph 76: Epoch: 087, Loss: -0.1395\n",
      "Graph 76: Epoch: 088, Loss: -0.4614\n",
      "Graph 76: Epoch: 089, Loss: -0.4623\n",
      "Graph 76: Epoch: 090, Loss: -0.4636\n",
      "Graph 76: Epoch: 091, Loss: -0.4654\n",
      "Graph 76: Epoch: 092, Loss: -0.1432\n",
      "Graph 76: Epoch: 093, Loss: -0.8550\n",
      "Graph 76: Epoch: 094, Loss: -0.5287\n",
      "Graph 76: Epoch: 095, Loss: -0.5277\n",
      "Graph 76: Epoch: 096, Loss: -0.5274\n",
      "Graph 76: Epoch: 097, Loss: -0.5276\n",
      "Graph 76: Epoch: 098, Loss: -0.4717\n",
      "Graph 76: Epoch: 099, Loss: -0.5285\n",
      "Graph 76: Epoch: 100, Loss: -0.5291\n",
      "Graph 77: Epoch: 001, Loss: -0.1315\n",
      "Graph 77: Epoch: 002, Loss: -0.0631\n",
      "Graph 77: Epoch: 003, Loss: -0.1725\n",
      "Graph 77: Epoch: 004, Loss: -0.1946\n",
      "Graph 77: Epoch: 005, Loss: -0.1607\n",
      "Graph 77: Epoch: 006, Loss: -0.4265\n",
      "Graph 77: Epoch: 007, Loss: -0.2399\n",
      "Graph 77: Epoch: 008, Loss: -0.2655\n",
      "Graph 77: Epoch: 009, Loss: -0.4794\n",
      "Graph 77: Epoch: 010, Loss: -0.1709\n",
      "Graph 77: Epoch: 011, Loss: -0.0439\n",
      "Graph 77: Epoch: 012, Loss: -0.3557\n",
      "Graph 77: Epoch: 013, Loss: -0.2735\n",
      "Graph 77: Epoch: 014, Loss: -0.1616\n",
      "Graph 77: Epoch: 015, Loss: -0.2082\n",
      "Graph 77: Epoch: 016, Loss: -0.5981\n",
      "Graph 77: Epoch: 017, Loss: -0.2273\n",
      "Graph 77: Epoch: 018, Loss: -0.1000\n",
      "Graph 77: Epoch: 019, Loss: -0.1113\n",
      "Graph 77: Epoch: 020, Loss: -0.2820\n",
      "Graph 77: Epoch: 021, Loss: -0.0444\n",
      "Graph 77: Epoch: 022, Loss: -0.5229\n",
      "Graph 77: Epoch: 023, Loss: -0.5010\n",
      "Graph 77: Epoch: 024, Loss: -0.5284\n",
      "Graph 77: Epoch: 025, Loss: -0.2929\n",
      "Graph 77: Epoch: 026, Loss: -0.5572\n",
      "Graph 77: Epoch: 027, Loss: -0.7426\n",
      "Graph 77: Epoch: 028, Loss: -0.3698\n",
      "Graph 77: Epoch: 029, Loss: -0.4263\n",
      "Graph 77: Epoch: 030, Loss: -0.4295\n",
      "Graph 77: Epoch: 031, Loss: -0.5720\n",
      "Graph 77: Epoch: 032, Loss: -0.3620\n",
      "Graph 77: Epoch: 033, Loss: -0.4341\n",
      "Graph 77: Epoch: 034, Loss: -0.4365\n",
      "Graph 77: Epoch: 035, Loss: -0.6700\n",
      "Graph 77: Epoch: 036, Loss: -0.5732\n",
      "Graph 77: Epoch: 037, Loss: -0.3127\n",
      "Graph 77: Epoch: 038, Loss: -0.3157\n",
      "Graph 77: Epoch: 039, Loss: -0.4749\n",
      "Graph 77: Epoch: 040, Loss: -0.0758\n",
      "Graph 77: Epoch: 041, Loss: -0.3362\n",
      "Graph 77: Epoch: 042, Loss: -0.3868\n",
      "Graph 77: Epoch: 043, Loss: -0.1998\n",
      "Graph 77: Epoch: 044, Loss: -0.1817\n",
      "Graph 77: Epoch: 045, Loss: -0.2877\n",
      "Graph 77: Epoch: 046, Loss: -0.2029\n",
      "Graph 77: Epoch: 047, Loss: -0.3960\n",
      "Graph 77: Epoch: 048, Loss: -0.1946\n",
      "Graph 77: Epoch: 049, Loss: -0.5906\n",
      "Graph 77: Epoch: 050, Loss: -0.3653\n",
      "Graph 77: Epoch: 051, Loss: -0.5267\n",
      "Graph 77: Epoch: 052, Loss: -0.5868\n",
      "Graph 77: Epoch: 053, Loss: -0.2590\n",
      "Graph 77: Epoch: 054, Loss: -0.4527\n",
      "Graph 77: Epoch: 055, Loss: -0.4592\n",
      "Graph 77: Epoch: 056, Loss: -0.2621\n",
      "Graph 77: Epoch: 057, Loss: -0.4536\n",
      "Graph 77: Epoch: 058, Loss: -0.8625\n",
      "Graph 77: Epoch: 059, Loss: -0.3469\n",
      "Graph 77: Epoch: 060, Loss: -0.3354\n",
      "Graph 77: Epoch: 061, Loss: -0.8669\n",
      "Graph 77: Epoch: 062, Loss: -0.4182\n",
      "Graph 77: Epoch: 063, Loss: -0.2667\n",
      "Graph 77: Epoch: 064, Loss: -0.6911\n",
      "Graph 77: Epoch: 065, Loss: -0.6777\n",
      "Graph 77: Epoch: 066, Loss: -0.8741\n",
      "Graph 77: Epoch: 067, Loss: -0.4282\n",
      "Graph 77: Epoch: 068, Loss: -0.4302\n",
      "Graph 77: Epoch: 069, Loss: -0.4673\n",
      "Graph 77: Epoch: 070, Loss: -0.4676\n",
      "Graph 77: Epoch: 071, Loss: -0.2306\n",
      "Graph 77: Epoch: 072, Loss: -0.6185\n",
      "Graph 77: Epoch: 073, Loss: -0.4556\n",
      "Graph 77: Epoch: 074, Loss: -0.6394\n",
      "Graph 77: Epoch: 075, Loss: -0.5988\n",
      "Graph 77: Epoch: 076, Loss: -0.2821\n",
      "Graph 77: Epoch: 077, Loss: -0.4594\n",
      "Graph 77: Epoch: 078, Loss: -0.2952\n",
      "Graph 77: Epoch: 079, Loss: -0.2079\n",
      "Graph 77: Epoch: 080, Loss: -0.5596\n",
      "Graph 77: Epoch: 081, Loss: -0.4504\n",
      "Graph 77: Epoch: 082, Loss: -0.2349\n",
      "Graph 77: Epoch: 083, Loss: -0.8103\n",
      "Graph 77: Epoch: 084, Loss: -0.3815\n",
      "Graph 77: Epoch: 085, Loss: -0.2132\n",
      "Graph 77: Epoch: 086, Loss: -0.7374\n",
      "Graph 77: Epoch: 087, Loss: -0.2330\n",
      "Graph 77: Epoch: 088, Loss: -0.5621\n",
      "Graph 77: Epoch: 089, Loss: -0.3088\n",
      "Graph 77: Epoch: 090, Loss: -0.6457\n",
      "Graph 77: Epoch: 091, Loss: -0.6599\n",
      "Graph 77: Epoch: 092, Loss: -0.3656\n",
      "Graph 77: Epoch: 093, Loss: -0.3540\n",
      "Graph 77: Epoch: 094, Loss: -0.3445\n",
      "Graph 77: Epoch: 095, Loss: -0.0416\n",
      "Graph 77: Epoch: 096, Loss: -0.4761\n",
      "Graph 77: Epoch: 097, Loss: -0.7176\n",
      "Graph 77: Epoch: 098, Loss: -0.4787\n",
      "Graph 77: Epoch: 099, Loss: -0.6120\n",
      "Graph 77: Epoch: 100, Loss: -0.6178\n",
      "Graph 78: Epoch: 001, Loss: -0.0169\n",
      "Graph 78: Epoch: 002, Loss: -0.0481\n",
      "Graph 78: Epoch: 003, Loss: -0.1590\n",
      "Graph 78: Epoch: 004, Loss: -0.2334\n",
      "Graph 78: Epoch: 005, Loss: -0.2945\n",
      "Graph 78: Epoch: 006, Loss: -0.3319\n",
      "Graph 78: Epoch: 007, Loss: -0.3872\n",
      "Graph 78: Epoch: 008, Loss: -0.3513\n",
      "Graph 78: Epoch: 009, Loss: -0.3333\n",
      "Graph 78: Epoch: 010, Loss: -0.3442\n",
      "Graph 78: Epoch: 011, Loss: -0.3846\n",
      "Graph 78: Epoch: 012, Loss: -0.3502\n",
      "Graph 78: Epoch: 013, Loss: -0.3746\n",
      "Graph 78: Epoch: 014, Loss: -0.3722\n",
      "Graph 78: Epoch: 015, Loss: -0.3820\n",
      "Graph 78: Epoch: 016, Loss: -0.5005\n",
      "Graph 78: Epoch: 017, Loss: -0.4204\n",
      "Graph 78: Epoch: 018, Loss: -0.4462\n",
      "Graph 78: Epoch: 019, Loss: -0.5362\n",
      "Graph 78: Epoch: 020, Loss: -0.5352\n",
      "Graph 78: Epoch: 021, Loss: -0.5554\n",
      "Graph 78: Epoch: 022, Loss: -0.3876\n",
      "Graph 78: Epoch: 023, Loss: -0.4773\n",
      "Graph 78: Epoch: 024, Loss: -0.4683\n",
      "Graph 78: Epoch: 025, Loss: -0.7743\n",
      "Graph 78: Epoch: 026, Loss: -0.4720\n",
      "Graph 78: Epoch: 027, Loss: -0.7893\n",
      "Graph 78: Epoch: 028, Loss: -0.6416\n",
      "Graph 78: Epoch: 029, Loss: -0.6400\n",
      "Graph 78: Epoch: 030, Loss: -0.3163\n",
      "Graph 78: Epoch: 031, Loss: -0.4055\n",
      "Graph 78: Epoch: 032, Loss: -0.4071\n",
      "Graph 78: Epoch: 033, Loss: -0.3260\n",
      "Graph 78: Epoch: 034, Loss: -0.4074\n",
      "Graph 78: Epoch: 035, Loss: -0.4074\n",
      "Graph 78: Epoch: 036, Loss: -0.3257\n",
      "Graph 78: Epoch: 037, Loss: -0.7337\n",
      "Graph 78: Epoch: 038, Loss: -0.2461\n",
      "Graph 78: Epoch: 039, Loss: -0.5701\n",
      "Graph 78: Epoch: 040, Loss: -0.4065\n",
      "Graph 78: Epoch: 041, Loss: -0.3276\n",
      "Graph 78: Epoch: 042, Loss: -0.4911\n",
      "Graph 78: Epoch: 043, Loss: -0.3279\n",
      "Graph 78: Epoch: 044, Loss: -0.4098\n",
      "Graph 78: Epoch: 045, Loss: -0.5727\n",
      "Graph 78: Epoch: 046, Loss: -0.4844\n",
      "Graph 78: Epoch: 047, Loss: -0.5737\n",
      "Graph 78: Epoch: 048, Loss: -0.5744\n",
      "Graph 78: Epoch: 049, Loss: -0.6503\n",
      "Graph 78: Epoch: 050, Loss: -0.6493\n",
      "Graph 78: Epoch: 051, Loss: -0.2469\n",
      "Graph 78: Epoch: 052, Loss: -0.4927\n",
      "Graph 78: Epoch: 053, Loss: -0.4931\n",
      "Graph 78: Epoch: 054, Loss: -0.8212\n",
      "Graph 78: Epoch: 055, Loss: -0.6577\n",
      "Graph 78: Epoch: 056, Loss: -0.3230\n",
      "Graph 78: Epoch: 057, Loss: -0.6552\n",
      "Graph 78: Epoch: 058, Loss: -0.4936\n",
      "Graph 78: Epoch: 059, Loss: -0.4940\n",
      "Graph 78: Epoch: 060, Loss: -0.4120\n",
      "Graph 78: Epoch: 061, Loss: -0.6588\n",
      "Graph 78: Epoch: 062, Loss: -0.7413\n",
      "Graph 78: Epoch: 063, Loss: -0.4125\n",
      "Graph 78: Epoch: 064, Loss: -0.6537\n",
      "Graph 78: Epoch: 065, Loss: -0.4121\n",
      "Graph 78: Epoch: 066, Loss: -0.4100\n",
      "Graph 78: Epoch: 067, Loss: -0.2476\n",
      "Graph 78: Epoch: 068, Loss: -0.2478\n",
      "Graph 78: Epoch: 069, Loss: -0.4127\n",
      "Graph 78: Epoch: 070, Loss: -0.3304\n",
      "Graph 78: Epoch: 071, Loss: -0.3304\n",
      "Graph 78: Epoch: 072, Loss: -0.3306\n",
      "Graph 78: Epoch: 073, Loss: -0.4930\n",
      "Graph 78: Epoch: 074, Loss: -0.5778\n",
      "Graph 78: Epoch: 075, Loss: -0.4122\n",
      "Graph 78: Epoch: 076, Loss: -0.4954\n",
      "Graph 78: Epoch: 077, Loss: -0.6605\n",
      "Graph 78: Epoch: 078, Loss: -0.7430\n",
      "Graph 78: Epoch: 079, Loss: -0.4959\n",
      "Graph 78: Epoch: 080, Loss: -0.3306\n",
      "Graph 78: Epoch: 081, Loss: -0.4956\n",
      "Graph 78: Epoch: 082, Loss: -0.3308\n",
      "Graph 78: Epoch: 083, Loss: -0.6611\n",
      "Graph 78: Epoch: 084, Loss: -0.6609\n",
      "Graph 78: Epoch: 085, Loss: -0.4135\n",
      "Graph 78: Epoch: 086, Loss: -0.3294\n",
      "Graph 78: Epoch: 087, Loss: -0.5786\n",
      "Graph 78: Epoch: 088, Loss: -0.7441\n",
      "Graph 78: Epoch: 089, Loss: -0.5786\n",
      "Graph 78: Epoch: 090, Loss: -0.4137\n",
      "Graph 78: Epoch: 091, Loss: -0.4138\n",
      "Graph 78: Epoch: 092, Loss: -0.4959\n",
      "Graph 78: Epoch: 093, Loss: -0.2485\n",
      "Graph 78: Epoch: 094, Loss: -0.3313\n",
      "Graph 78: Epoch: 095, Loss: -0.6619\n",
      "Graph 78: Epoch: 096, Loss: -0.2486\n",
      "Graph 78: Epoch: 097, Loss: -0.2486\n",
      "Graph 78: Epoch: 098, Loss: -0.2487\n",
      "Graph 78: Epoch: 099, Loss: -0.6620\n",
      "Graph 78: Epoch: 100, Loss: -0.4139\n",
      "Graph 79: Epoch: 001, Loss: -0.0460\n",
      "Graph 79: Epoch: 002, Loss: -0.1688\n",
      "Graph 79: Epoch: 003, Loss: -0.2602\n",
      "Graph 79: Epoch: 004, Loss: -0.3125\n",
      "Graph 79: Epoch: 005, Loss: -0.2983\n",
      "Graph 79: Epoch: 006, Loss: -0.3761\n",
      "Graph 79: Epoch: 007, Loss: -0.3706\n",
      "Graph 79: Epoch: 008, Loss: -0.3602\n",
      "Graph 79: Epoch: 009, Loss: -0.2776\n",
      "Graph 79: Epoch: 010, Loss: -0.4270\n",
      "Graph 79: Epoch: 011, Loss: -0.4052\n",
      "Graph 79: Epoch: 012, Loss: -0.3522\n",
      "Graph 79: Epoch: 013, Loss: -0.4927\n",
      "Graph 79: Epoch: 014, Loss: -0.4413\n",
      "Graph 79: Epoch: 015, Loss: -0.4422\n",
      "Graph 79: Epoch: 016, Loss: -0.3739\n",
      "Graph 79: Epoch: 017, Loss: -0.3900\n",
      "Graph 79: Epoch: 018, Loss: -0.5356\n",
      "Graph 79: Epoch: 019, Loss: -0.3878\n",
      "Graph 79: Epoch: 020, Loss: -0.3152\n",
      "Graph 79: Epoch: 021, Loss: -0.5382\n",
      "Graph 79: Epoch: 022, Loss: -0.6271\n",
      "Graph 79: Epoch: 023, Loss: -0.3170\n",
      "Graph 79: Epoch: 024, Loss: -0.4798\n",
      "Graph 79: Epoch: 025, Loss: -0.2409\n",
      "Graph 79: Epoch: 026, Loss: -0.3978\n",
      "Graph 79: Epoch: 027, Loss: -0.4713\n",
      "Graph 79: Epoch: 028, Loss: -0.4798\n",
      "Graph 79: Epoch: 029, Loss: -0.3237\n",
      "Graph 79: Epoch: 030, Loss: -0.5634\n",
      "Graph 79: Epoch: 031, Loss: -0.4008\n",
      "Graph 79: Epoch: 032, Loss: -0.6467\n",
      "Graph 79: Epoch: 033, Loss: -0.2443\n",
      "Graph 79: Epoch: 034, Loss: -0.2410\n",
      "Graph 79: Epoch: 035, Loss: -0.4059\n",
      "Graph 79: Epoch: 036, Loss: -0.5679\n",
      "Graph 79: Epoch: 037, Loss: -0.3263\n",
      "Graph 79: Epoch: 038, Loss: -0.2452\n",
      "Graph 79: Epoch: 039, Loss: -0.6514\n",
      "Graph 79: Epoch: 040, Loss: -0.4871\n",
      "Graph 79: Epoch: 041, Loss: -0.5714\n",
      "Graph 79: Epoch: 042, Loss: -0.4854\n",
      "Graph 79: Epoch: 043, Loss: -0.4898\n",
      "Graph 79: Epoch: 044, Loss: -0.7279\n",
      "Graph 79: Epoch: 045, Loss: -0.4867\n",
      "Graph 79: Epoch: 046, Loss: -0.4097\n",
      "Graph 79: Epoch: 047, Loss: -0.3279\n",
      "Graph 79: Epoch: 048, Loss: -0.6519\n",
      "Graph 79: Epoch: 049, Loss: -0.4912\n",
      "Graph 79: Epoch: 050, Loss: -0.4048\n",
      "Graph 79: Epoch: 051, Loss: -0.5741\n",
      "Graph 79: Epoch: 052, Loss: -0.3283\n",
      "Graph 79: Epoch: 053, Loss: -0.4097\n",
      "Graph 79: Epoch: 054, Loss: -0.6472\n",
      "Graph 79: Epoch: 055, Loss: -0.4104\n",
      "Graph 79: Epoch: 056, Loss: -0.8163\n",
      "Graph 79: Epoch: 057, Loss: -0.5038\n",
      "Graph 79: Epoch: 058, Loss: -0.6573\n",
      "Graph 79: Epoch: 059, Loss: -0.4932\n",
      "Graph 79: Epoch: 060, Loss: -0.4933\n",
      "Graph 79: Epoch: 061, Loss: -0.5756\n",
      "Graph 79: Epoch: 062, Loss: -0.4925\n",
      "Graph 79: Epoch: 063, Loss: -0.5751\n",
      "Graph 79: Epoch: 064, Loss: -0.5086\n",
      "Graph 79: Epoch: 065, Loss: -0.1655\n",
      "Graph 79: Epoch: 066, Loss: -0.4943\n",
      "Graph 79: Epoch: 067, Loss: -0.3296\n",
      "Graph 79: Epoch: 068, Loss: -0.5764\n",
      "Graph 79: Epoch: 069, Loss: -0.5769\n",
      "Graph 79: Epoch: 070, Loss: -0.7413\n",
      "Graph 79: Epoch: 071, Loss: -0.3300\n",
      "Graph 79: Epoch: 072, Loss: -0.4946\n",
      "Graph 79: Epoch: 073, Loss: -0.6595\n",
      "Graph 79: Epoch: 074, Loss: -0.4949\n",
      "Graph 79: Epoch: 075, Loss: -0.3304\n",
      "Graph 79: Epoch: 076, Loss: -0.1656\n",
      "Graph 79: Epoch: 077, Loss: -0.4950\n",
      "Graph 79: Epoch: 078, Loss: -0.5775\n",
      "Graph 79: Epoch: 079, Loss: -0.6598\n",
      "Graph 79: Epoch: 080, Loss: -0.5752\n",
      "Graph 79: Epoch: 081, Loss: -0.4948\n",
      "Graph 79: Epoch: 082, Loss: -0.3310\n",
      "Graph 79: Epoch: 083, Loss: -0.4137\n",
      "Graph 79: Epoch: 084, Loss: -0.4956\n",
      "Graph 79: Epoch: 085, Loss: -0.3307\n",
      "Graph 79: Epoch: 086, Loss: -0.6606\n",
      "Graph 79: Epoch: 087, Loss: -0.7434\n",
      "Graph 79: Epoch: 088, Loss: -0.6611\n",
      "Graph 79: Epoch: 089, Loss: -0.3310\n",
      "Graph 79: Epoch: 090, Loss: -0.6605\n",
      "Graph 79: Epoch: 091, Loss: -0.5784\n",
      "Graph 79: Epoch: 092, Loss: -0.1657\n",
      "Graph 79: Epoch: 093, Loss: -0.5784\n",
      "Graph 79: Epoch: 094, Loss: -0.4962\n",
      "Graph 79: Epoch: 095, Loss: -0.2486\n",
      "Graph 79: Epoch: 096, Loss: -0.3313\n",
      "Graph 79: Epoch: 097, Loss: -0.4955\n",
      "Graph 79: Epoch: 098, Loss: -0.2474\n",
      "Graph 79: Epoch: 099, Loss: -0.2486\n",
      "Graph 79: Epoch: 100, Loss: -0.4137\n",
      "Graph 80: Epoch: 001, Loss: -0.0296\n",
      "Graph 80: Epoch: 002, Loss: -0.0929\n",
      "Graph 80: Epoch: 003, Loss: -0.1298\n",
      "Graph 80: Epoch: 004, Loss: -0.2474\n",
      "Graph 80: Epoch: 005, Loss: -0.2536\n",
      "Graph 80: Epoch: 006, Loss: -0.3102\n",
      "Graph 80: Epoch: 007, Loss: -0.3642\n",
      "Graph 80: Epoch: 008, Loss: -0.2390\n",
      "Graph 80: Epoch: 009, Loss: -0.3077\n",
      "Graph 80: Epoch: 010, Loss: -0.2513\n",
      "Graph 80: Epoch: 011, Loss: -0.2764\n",
      "Graph 80: Epoch: 012, Loss: -0.3310\n",
      "Graph 80: Epoch: 013, Loss: -0.3469\n",
      "Graph 80: Epoch: 014, Loss: -0.4175\n",
      "Graph 80: Epoch: 015, Loss: -0.3891\n",
      "Graph 80: Epoch: 016, Loss: -0.3437\n",
      "Graph 80: Epoch: 017, Loss: -0.4126\n",
      "Graph 80: Epoch: 018, Loss: -0.4072\n",
      "Graph 80: Epoch: 019, Loss: -0.4375\n",
      "Graph 80: Epoch: 020, Loss: -0.4721\n",
      "Graph 80: Epoch: 021, Loss: -0.3336\n",
      "Graph 80: Epoch: 022, Loss: -0.4278\n",
      "Graph 80: Epoch: 023, Loss: -0.4579\n",
      "Graph 80: Epoch: 024, Loss: -0.4548\n",
      "Graph 80: Epoch: 025, Loss: -0.3567\n",
      "Graph 80: Epoch: 026, Loss: -0.3689\n",
      "Graph 80: Epoch: 027, Loss: -0.4054\n",
      "Graph 80: Epoch: 028, Loss: -0.4140\n",
      "Graph 80: Epoch: 029, Loss: -0.4137\n",
      "Graph 80: Epoch: 030, Loss: -0.5464\n",
      "Graph 80: Epoch: 031, Loss: -0.4583\n",
      "Graph 80: Epoch: 032, Loss: -0.3414\n",
      "Graph 80: Epoch: 033, Loss: -0.2459\n",
      "Graph 80: Epoch: 034, Loss: -0.4020\n",
      "Graph 80: Epoch: 035, Loss: -0.4638\n",
      "Graph 80: Epoch: 036, Loss: -0.6059\n",
      "Graph 80: Epoch: 037, Loss: -0.3503\n",
      "Graph 80: Epoch: 038, Loss: -0.5037\n",
      "Graph 80: Epoch: 039, Loss: -0.5057\n",
      "Graph 80: Epoch: 040, Loss: -0.5701\n",
      "Graph 80: Epoch: 041, Loss: -0.6252\n",
      "Graph 80: Epoch: 042, Loss: -0.5198\n",
      "Graph 80: Epoch: 043, Loss: -0.4239\n",
      "Graph 80: Epoch: 044, Loss: -0.3248\n",
      "Graph 80: Epoch: 045, Loss: -0.2436\n",
      "Graph 80: Epoch: 046, Loss: -0.4431\n",
      "Graph 80: Epoch: 047, Loss: -0.5249\n",
      "Graph 80: Epoch: 048, Loss: -0.4304\n",
      "Graph 80: Epoch: 049, Loss: -0.3372\n",
      "Graph 80: Epoch: 050, Loss: -0.4018\n",
      "Graph 80: Epoch: 051, Loss: -0.4227\n",
      "Graph 80: Epoch: 052, Loss: -0.5451\n",
      "Graph 80: Epoch: 053, Loss: -0.4349\n",
      "Graph 80: Epoch: 054, Loss: -0.4340\n",
      "Graph 80: Epoch: 055, Loss: -0.6441\n",
      "Graph 80: Epoch: 056, Loss: -0.4544\n",
      "Graph 80: Epoch: 057, Loss: -0.5393\n",
      "Graph 80: Epoch: 058, Loss: -0.4381\n",
      "Graph 80: Epoch: 059, Loss: -0.3325\n",
      "Graph 80: Epoch: 060, Loss: -0.6467\n",
      "Graph 80: Epoch: 061, Loss: -0.6478\n",
      "Graph 80: Epoch: 062, Loss: -0.4343\n",
      "Graph 80: Epoch: 063, Loss: -0.3296\n",
      "Graph 80: Epoch: 064, Loss: -0.4388\n",
      "Graph 80: Epoch: 065, Loss: -0.4342\n",
      "Graph 80: Epoch: 066, Loss: -0.4271\n",
      "Graph 80: Epoch: 067, Loss: -0.3333\n",
      "Graph 80: Epoch: 068, Loss: -0.5410\n",
      "Graph 80: Epoch: 069, Loss: -0.4326\n",
      "Graph 80: Epoch: 070, Loss: -0.5456\n",
      "Graph 80: Epoch: 071, Loss: -0.2239\n",
      "Graph 80: Epoch: 072, Loss: -0.6614\n",
      "Graph 80: Epoch: 073, Loss: -0.3316\n",
      "Graph 80: Epoch: 074, Loss: -0.2233\n",
      "Graph 80: Epoch: 075, Loss: -0.3277\n",
      "Graph 80: Epoch: 076, Loss: -0.5461\n",
      "Graph 80: Epoch: 077, Loss: -0.4379\n",
      "Graph 80: Epoch: 078, Loss: -0.3318\n",
      "Graph 80: Epoch: 079, Loss: -0.4360\n",
      "Graph 80: Epoch: 080, Loss: -0.4395\n",
      "Graph 80: Epoch: 081, Loss: -0.2216\n",
      "Graph 80: Epoch: 082, Loss: -0.4387\n",
      "Graph 80: Epoch: 083, Loss: -0.5383\n",
      "Graph 80: Epoch: 084, Loss: -0.4408\n",
      "Graph 80: Epoch: 085, Loss: -0.2225\n",
      "Graph 80: Epoch: 086, Loss: -0.4391\n",
      "Graph 80: Epoch: 087, Loss: -0.6549\n",
      "Graph 80: Epoch: 088, Loss: -0.2221\n",
      "Graph 80: Epoch: 089, Loss: -0.4323\n",
      "Graph 80: Epoch: 090, Loss: -0.4399\n",
      "Graph 80: Epoch: 091, Loss: -0.5490\n",
      "Graph 80: Epoch: 092, Loss: -0.3303\n",
      "Graph 80: Epoch: 093, Loss: -0.7659\n",
      "Graph 80: Epoch: 094, Loss: -0.4392\n",
      "Graph 80: Epoch: 095, Loss: -0.3543\n",
      "Graph 80: Epoch: 096, Loss: -0.4400\n",
      "Graph 80: Epoch: 097, Loss: -0.7649\n",
      "Graph 80: Epoch: 098, Loss: -0.7697\n",
      "Graph 80: Epoch: 099, Loss: -0.5491\n",
      "Graph 80: Epoch: 100, Loss: -0.3305\n",
      "Graph 81: Epoch: 001, Loss: -0.0127\n",
      "Graph 81: Epoch: 002, Loss: -0.0469\n",
      "Graph 81: Epoch: 003, Loss: -0.1436\n",
      "Graph 81: Epoch: 004, Loss: -0.2323\n",
      "Graph 81: Epoch: 005, Loss: -0.3299\n",
      "Graph 81: Epoch: 006, Loss: -0.3448\n",
      "Graph 81: Epoch: 007, Loss: -0.3868\n",
      "Graph 81: Epoch: 008, Loss: -0.3929\n",
      "Graph 81: Epoch: 009, Loss: -0.4063\n",
      "Graph 81: Epoch: 010, Loss: -0.3713\n",
      "Graph 81: Epoch: 011, Loss: -0.3803\n",
      "Graph 81: Epoch: 012, Loss: -0.3545\n",
      "Graph 81: Epoch: 013, Loss: -0.4790\n",
      "Graph 81: Epoch: 014, Loss: -0.3975\n",
      "Graph 81: Epoch: 015, Loss: -0.4252\n",
      "Graph 81: Epoch: 016, Loss: -0.4264\n",
      "Graph 81: Epoch: 017, Loss: -0.4270\n",
      "Graph 81: Epoch: 018, Loss: -0.4871\n",
      "Graph 81: Epoch: 019, Loss: -0.5457\n",
      "Graph 81: Epoch: 020, Loss: -0.5545\n",
      "Graph 81: Epoch: 021, Loss: -0.4279\n",
      "Graph 81: Epoch: 022, Loss: -0.6771\n",
      "Graph 81: Epoch: 023, Loss: -0.5671\n",
      "Graph 81: Epoch: 024, Loss: -0.5724\n",
      "Graph 81: Epoch: 025, Loss: -0.2635\n",
      "Graph 81: Epoch: 026, Loss: -0.5115\n",
      "Graph 81: Epoch: 027, Loss: -0.5776\n",
      "Graph 81: Epoch: 028, Loss: -0.3268\n",
      "Graph 81: Epoch: 029, Loss: -0.4564\n",
      "Graph 81: Epoch: 030, Loss: -0.5189\n",
      "Graph 81: Epoch: 031, Loss: -0.3910\n",
      "Graph 81: Epoch: 032, Loss: -0.5221\n",
      "Graph 81: Epoch: 033, Loss: -0.2631\n",
      "Graph 81: Epoch: 034, Loss: -0.5871\n",
      "Graph 81: Epoch: 035, Loss: -0.5900\n",
      "Graph 81: Epoch: 036, Loss: -0.5892\n",
      "Graph 81: Epoch: 037, Loss: -0.3281\n",
      "Graph 81: Epoch: 038, Loss: -0.7222\n",
      "Graph 81: Epoch: 039, Loss: -0.4022\n",
      "Graph 81: Epoch: 040, Loss: -0.5255\n",
      "Graph 81: Epoch: 041, Loss: -0.5267\n",
      "Graph 81: Epoch: 042, Loss: -0.5924\n",
      "Graph 81: Epoch: 043, Loss: -0.3297\n",
      "Graph 81: Epoch: 044, Loss: -0.6591\n",
      "Graph 81: Epoch: 045, Loss: -0.6591\n",
      "Graph 81: Epoch: 046, Loss: -0.3956\n",
      "Graph 81: Epoch: 047, Loss: -0.3955\n",
      "Graph 81: Epoch: 048, Loss: -0.5275\n",
      "Graph 81: Epoch: 049, Loss: -0.6595\n",
      "Graph 81: Epoch: 050, Loss: -0.1984\n",
      "Graph 81: Epoch: 051, Loss: -0.3960\n",
      "Graph 81: Epoch: 052, Loss: -0.5279\n",
      "Graph 81: Epoch: 053, Loss: -0.6592\n",
      "Graph 81: Epoch: 054, Loss: -0.5938\n",
      "Graph 81: Epoch: 055, Loss: -0.3965\n",
      "Graph 81: Epoch: 056, Loss: -0.5289\n",
      "Graph 81: Epoch: 057, Loss: -0.3967\n",
      "Graph 81: Epoch: 058, Loss: -0.5288\n",
      "Graph 81: Epoch: 059, Loss: -0.4631\n",
      "Graph 81: Epoch: 060, Loss: -0.5286\n",
      "Graph 81: Epoch: 061, Loss: -0.4631\n",
      "Graph 81: Epoch: 062, Loss: -0.6613\n",
      "Graph 81: Epoch: 063, Loss: -0.5292\n",
      "Graph 81: Epoch: 064, Loss: -0.5953\n",
      "Graph 81: Epoch: 065, Loss: -0.3310\n",
      "Graph 81: Epoch: 066, Loss: -0.7279\n",
      "Graph 81: Epoch: 067, Loss: -0.4634\n",
      "Graph 81: Epoch: 068, Loss: -0.5296\n",
      "Graph 81: Epoch: 069, Loss: -0.3310\n",
      "Graph 81: Epoch: 070, Loss: -0.5296\n",
      "Graph 81: Epoch: 071, Loss: -0.5297\n",
      "Graph 81: Epoch: 072, Loss: -0.5298\n",
      "Graph 81: Epoch: 073, Loss: -0.3973\n",
      "Graph 81: Epoch: 074, Loss: -0.6621\n",
      "Graph 81: Epoch: 075, Loss: -0.3974\n",
      "Graph 81: Epoch: 076, Loss: -0.5299\n",
      "Graph 81: Epoch: 077, Loss: -0.3975\n",
      "Graph 81: Epoch: 078, Loss: -0.7287\n",
      "Graph 81: Epoch: 079, Loss: -0.4637\n",
      "Graph 81: Epoch: 080, Loss: -0.7288\n",
      "Graph 81: Epoch: 081, Loss: -0.3977\n",
      "Graph 81: Epoch: 082, Loss: -0.4640\n",
      "Graph 81: Epoch: 083, Loss: -0.7288\n",
      "Graph 81: Epoch: 084, Loss: -0.6626\n",
      "Graph 81: Epoch: 085, Loss: -0.5302\n",
      "Graph 81: Epoch: 086, Loss: -0.5966\n",
      "Graph 81: Epoch: 087, Loss: -0.5966\n",
      "Graph 81: Epoch: 088, Loss: -0.4642\n",
      "Graph 81: Epoch: 089, Loss: -0.5305\n",
      "Graph 81: Epoch: 090, Loss: -0.4642\n",
      "Graph 81: Epoch: 091, Loss: -0.3317\n",
      "Graph 81: Epoch: 092, Loss: -0.3980\n",
      "Graph 81: Epoch: 093, Loss: -0.2655\n",
      "Graph 81: Epoch: 094, Loss: -0.4644\n",
      "Graph 81: Epoch: 095, Loss: -0.7957\n",
      "Graph 81: Epoch: 096, Loss: -0.4644\n",
      "Graph 81: Epoch: 097, Loss: -0.5970\n",
      "Graph 81: Epoch: 098, Loss: -0.3318\n",
      "Graph 81: Epoch: 099, Loss: -0.5308\n",
      "Graph 81: Epoch: 100, Loss: -0.3981\n",
      "Graph 82: Epoch: 001, Loss: -0.0102\n",
      "Graph 82: Epoch: 002, Loss: -0.1127\n",
      "Graph 82: Epoch: 003, Loss: -0.1993\n",
      "Graph 82: Epoch: 004, Loss: -0.3933\n",
      "Graph 82: Epoch: 005, Loss: -0.4639\n",
      "Graph 82: Epoch: 006, Loss: -0.5002\n",
      "Graph 82: Epoch: 007, Loss: -0.5628\n",
      "Graph 82: Epoch: 008, Loss: -0.4322\n",
      "Graph 82: Epoch: 009, Loss: -0.4592\n",
      "Graph 82: Epoch: 010, Loss: -0.4077\n",
      "Graph 82: Epoch: 011, Loss: -0.4180\n",
      "Graph 82: Epoch: 012, Loss: -0.4697\n",
      "Graph 82: Epoch: 013, Loss: -0.4746\n",
      "Graph 82: Epoch: 014, Loss: -0.3129\n",
      "Graph 82: Epoch: 015, Loss: -0.3706\n",
      "Graph 82: Epoch: 016, Loss: -0.4700\n",
      "Graph 82: Epoch: 017, Loss: -0.4221\n",
      "Graph 82: Epoch: 018, Loss: -0.3751\n",
      "Graph 82: Epoch: 019, Loss: -0.5343\n",
      "Graph 82: Epoch: 020, Loss: -0.5326\n",
      "Graph 82: Epoch: 021, Loss: -0.5359\n",
      "Graph 82: Epoch: 022, Loss: -0.4302\n",
      "Graph 82: Epoch: 023, Loss: -0.4862\n",
      "Graph 82: Epoch: 024, Loss: -0.3240\n",
      "Graph 82: Epoch: 025, Loss: -0.4317\n",
      "Graph 82: Epoch: 026, Loss: -0.4858\n",
      "Graph 82: Epoch: 027, Loss: -0.3800\n",
      "Graph 82: Epoch: 028, Loss: -0.4879\n",
      "Graph 82: Epoch: 029, Loss: -0.5426\n",
      "Graph 82: Epoch: 030, Loss: -0.5436\n",
      "Graph 82: Epoch: 031, Loss: -0.4350\n",
      "Graph 82: Epoch: 032, Loss: -0.6508\n",
      "Graph 82: Epoch: 033, Loss: -0.4358\n",
      "Graph 82: Epoch: 034, Loss: -0.7084\n",
      "Graph 82: Epoch: 035, Loss: -0.6005\n",
      "Graph 82: Epoch: 036, Loss: -0.5464\n",
      "Graph 82: Epoch: 037, Loss: -0.3824\n",
      "Graph 82: Epoch: 038, Loss: -0.4923\n",
      "Graph 82: Epoch: 039, Loss: -0.4376\n",
      "Graph 82: Epoch: 040, Loss: -0.4376\n",
      "Graph 82: Epoch: 041, Loss: -0.4930\n",
      "Graph 82: Epoch: 042, Loss: -0.6573\n",
      "Graph 82: Epoch: 043, Loss: -0.3287\n",
      "Graph 82: Epoch: 044, Loss: -0.7118\n",
      "Graph 82: Epoch: 045, Loss: -0.3839\n",
      "Graph 82: Epoch: 046, Loss: -0.6005\n",
      "Graph 82: Epoch: 047, Loss: -0.5487\n",
      "Graph 82: Epoch: 048, Loss: -0.4385\n",
      "Graph 82: Epoch: 049, Loss: -0.6040\n",
      "Graph 82: Epoch: 050, Loss: -0.3845\n",
      "Graph 82: Epoch: 051, Loss: -0.5495\n",
      "Graph 82: Epoch: 052, Loss: -0.4947\n",
      "Graph 82: Epoch: 053, Loss: -0.2199\n",
      "Graph 82: Epoch: 054, Loss: -0.3851\n",
      "Graph 82: Epoch: 055, Loss: -0.6598\n",
      "Graph 82: Epoch: 056, Loss: -0.6049\n",
      "Graph 82: Epoch: 057, Loss: -0.5500\n",
      "Graph 82: Epoch: 058, Loss: -0.3302\n",
      "Graph 82: Epoch: 059, Loss: -0.7152\n",
      "Graph 82: Epoch: 060, Loss: -0.6604\n",
      "Graph 82: Epoch: 061, Loss: -0.4404\n",
      "Graph 82: Epoch: 062, Loss: -0.3305\n",
      "Graph 82: Epoch: 063, Loss: -0.6609\n",
      "Graph 82: Epoch: 064, Loss: -0.4407\n",
      "Graph 82: Epoch: 065, Loss: -0.3857\n",
      "Graph 82: Epoch: 066, Loss: -0.4960\n",
      "Graph 82: Epoch: 067, Loss: -0.4408\n",
      "Graph 82: Epoch: 068, Loss: -0.3858\n",
      "Graph 82: Epoch: 069, Loss: -0.3844\n",
      "Graph 82: Epoch: 070, Loss: -0.4410\n",
      "Graph 82: Epoch: 071, Loss: -0.6065\n",
      "Graph 82: Epoch: 072, Loss: -0.3861\n",
      "Graph 82: Epoch: 073, Loss: -0.4413\n",
      "Graph 82: Epoch: 074, Loss: -0.4413\n",
      "Graph 82: Epoch: 075, Loss: -0.3861\n",
      "Graph 82: Epoch: 076, Loss: -0.4408\n",
      "Graph 82: Epoch: 077, Loss: -0.3311\n",
      "Graph 82: Epoch: 078, Loss: -0.5517\n",
      "Graph 82: Epoch: 079, Loss: -0.4414\n",
      "Graph 82: Epoch: 080, Loss: -0.3863\n",
      "Graph 82: Epoch: 081, Loss: -0.3863\n",
      "Graph 82: Epoch: 082, Loss: -0.6071\n",
      "Graph 82: Epoch: 083, Loss: -0.3312\n",
      "Graph 82: Epoch: 084, Loss: -0.4968\n",
      "Graph 82: Epoch: 085, Loss: -0.5520\n",
      "Graph 82: Epoch: 086, Loss: -0.5519\n",
      "Graph 82: Epoch: 087, Loss: -0.4417\n",
      "Graph 82: Epoch: 088, Loss: -0.6073\n",
      "Graph 82: Epoch: 089, Loss: -0.4968\n",
      "Graph 82: Epoch: 090, Loss: -0.3313\n",
      "Graph 82: Epoch: 091, Loss: -0.3867\n",
      "Graph 82: Epoch: 092, Loss: -0.6075\n",
      "Graph 82: Epoch: 093, Loss: -0.4417\n",
      "Graph 82: Epoch: 094, Loss: -0.6076\n",
      "Graph 82: Epoch: 095, Loss: -0.5524\n",
      "Graph 82: Epoch: 096, Loss: -0.3868\n",
      "Graph 82: Epoch: 097, Loss: -0.5526\n",
      "Graph 82: Epoch: 098, Loss: -0.4421\n",
      "Graph 82: Epoch: 099, Loss: -0.4974\n",
      "Graph 82: Epoch: 100, Loss: -0.5527\n",
      "Graph 83: Epoch: 001, Loss: -0.0147\n",
      "Graph 83: Epoch: 002, Loss: -0.0497\n",
      "Graph 83: Epoch: 003, Loss: -0.0379\n",
      "Graph 83: Epoch: 004, Loss: -0.1091\n",
      "Graph 83: Epoch: 005, Loss: -0.1365\n",
      "Graph 83: Epoch: 006, Loss: -0.2281\n",
      "Graph 83: Epoch: 007, Loss: -0.2801\n",
      "Graph 83: Epoch: 008, Loss: -0.3228\n",
      "Graph 83: Epoch: 009, Loss: -0.3410\n",
      "Graph 83: Epoch: 010, Loss: -0.3027\n",
      "Graph 83: Epoch: 011, Loss: -0.3860\n",
      "Graph 83: Epoch: 012, Loss: -0.3522\n",
      "Graph 83: Epoch: 013, Loss: -0.2582\n",
      "Graph 83: Epoch: 014, Loss: -0.4115\n",
      "Graph 83: Epoch: 015, Loss: -0.4319\n",
      "Graph 83: Epoch: 016, Loss: -0.4707\n",
      "Graph 83: Epoch: 017, Loss: -0.2846\n",
      "Graph 83: Epoch: 018, Loss: -0.3476\n",
      "Graph 83: Epoch: 019, Loss: -0.2709\n",
      "Graph 83: Epoch: 020, Loss: -0.2788\n",
      "Graph 83: Epoch: 021, Loss: -0.3824\n",
      "Graph 83: Epoch: 022, Loss: -0.4521\n",
      "Graph 83: Epoch: 023, Loss: -0.2075\n",
      "Graph 83: Epoch: 024, Loss: -0.5847\n",
      "Graph 83: Epoch: 025, Loss: -0.5800\n",
      "Graph 83: Epoch: 026, Loss: -0.6321\n",
      "Graph 83: Epoch: 027, Loss: -0.4819\n",
      "Graph 83: Epoch: 028, Loss: -0.3645\n",
      "Graph 83: Epoch: 029, Loss: -0.3849\n",
      "Graph 83: Epoch: 030, Loss: -0.3794\n",
      "Graph 83: Epoch: 031, Loss: -0.3835\n",
      "Graph 83: Epoch: 032, Loss: -0.2875\n",
      "Graph 83: Epoch: 033, Loss: -0.2867\n",
      "Graph 83: Epoch: 034, Loss: -0.4759\n",
      "Graph 83: Epoch: 035, Loss: -0.6530\n",
      "Graph 83: Epoch: 036, Loss: -0.2911\n",
      "Graph 83: Epoch: 037, Loss: -0.6666\n",
      "Graph 83: Epoch: 038, Loss: -0.3818\n",
      "Graph 83: Epoch: 039, Loss: -0.8554\n",
      "Graph 83: Epoch: 040, Loss: -0.3857\n",
      "Graph 83: Epoch: 041, Loss: -0.4800\n",
      "Graph 83: Epoch: 042, Loss: -0.3895\n",
      "Graph 83: Epoch: 043, Loss: -0.4775\n",
      "Graph 83: Epoch: 044, Loss: -0.6494\n",
      "Graph 83: Epoch: 045, Loss: -0.4822\n",
      "Graph 83: Epoch: 046, Loss: -0.4852\n",
      "Graph 83: Epoch: 047, Loss: -0.4875\n",
      "Graph 83: Epoch: 048, Loss: -0.5781\n",
      "Graph 83: Epoch: 049, Loss: -0.4887\n",
      "Graph 83: Epoch: 050, Loss: -0.1980\n",
      "Graph 83: Epoch: 051, Loss: -0.3823\n",
      "Graph 83: Epoch: 052, Loss: -0.2947\n",
      "Graph 83: Epoch: 053, Loss: -0.5832\n",
      "Graph 83: Epoch: 054, Loss: -0.3921\n",
      "Graph 83: Epoch: 055, Loss: -0.4819\n",
      "Graph 83: Epoch: 056, Loss: -0.2949\n",
      "Graph 83: Epoch: 057, Loss: -0.2936\n",
      "Graph 83: Epoch: 058, Loss: -0.7844\n",
      "Graph 83: Epoch: 059, Loss: -0.5893\n",
      "Graph 83: Epoch: 060, Loss: -0.3924\n",
      "Graph 83: Epoch: 061, Loss: -0.5904\n",
      "Graph 83: Epoch: 062, Loss: -0.7864\n",
      "Graph 83: Epoch: 063, Loss: -0.3846\n",
      "Graph 83: Epoch: 064, Loss: -0.5910\n",
      "Graph 83: Epoch: 065, Loss: -0.1978\n",
      "Graph 83: Epoch: 066, Loss: -0.2995\n",
      "Graph 83: Epoch: 067, Loss: -0.4926\n",
      "Graph 83: Epoch: 068, Loss: -0.3911\n",
      "Graph 83: Epoch: 069, Loss: -0.5912\n",
      "Graph 83: Epoch: 070, Loss: -0.5917\n",
      "Graph 83: Epoch: 071, Loss: -0.3946\n",
      "Graph 83: Epoch: 072, Loss: -0.5899\n",
      "Graph 83: Epoch: 073, Loss: -0.1989\n",
      "Graph 83: Epoch: 074, Loss: -0.5164\n",
      "Graph 83: Epoch: 075, Loss: -0.2969\n",
      "Graph 83: Epoch: 076, Loss: -0.3952\n",
      "Graph 83: Epoch: 077, Loss: -0.5924\n",
      "Graph 83: Epoch: 078, Loss: -0.5926\n",
      "Graph 83: Epoch: 079, Loss: -0.4938\n",
      "Graph 83: Epoch: 080, Loss: -0.4909\n",
      "Graph 83: Epoch: 081, Loss: -0.5905\n",
      "Graph 83: Epoch: 082, Loss: -0.5925\n",
      "Graph 83: Epoch: 083, Loss: -0.3176\n",
      "Graph 83: Epoch: 084, Loss: -0.5927\n",
      "Graph 83: Epoch: 085, Loss: -0.6922\n",
      "Graph 83: Epoch: 086, Loss: -0.2552\n",
      "Graph 83: Epoch: 087, Loss: -0.2973\n",
      "Graph 83: Epoch: 088, Loss: -0.4143\n",
      "Graph 83: Epoch: 089, Loss: -0.5933\n",
      "Graph 83: Epoch: 090, Loss: -0.4954\n",
      "Graph 83: Epoch: 091, Loss: -0.4957\n",
      "Graph 83: Epoch: 092, Loss: -0.4142\n",
      "Graph 83: Epoch: 093, Loss: -0.3980\n",
      "Graph 83: Epoch: 094, Loss: -0.2968\n",
      "Graph 83: Epoch: 095, Loss: -0.6271\n",
      "Graph 83: Epoch: 096, Loss: -0.4937\n",
      "Graph 83: Epoch: 097, Loss: -0.4267\n",
      "Graph 83: Epoch: 098, Loss: -0.4944\n",
      "Graph 83: Epoch: 099, Loss: -0.3979\n",
      "Graph 83: Epoch: 100, Loss: -0.3955\n",
      "Graph 84: Epoch: 001, Loss: -0.5000\n",
      "Graph 84: Epoch: 002, Loss: -0.5050\n",
      "Graph 84: Epoch: 003, Loss: -0.4900\n",
      "Graph 84: Epoch: 004, Loss: -0.5113\n",
      "Graph 84: Epoch: 005, Loss: -0.1136\n",
      "Graph 84: Epoch: 006, Loss: -0.4849\n",
      "Graph 84: Epoch: 007, Loss: -0.5151\n",
      "Graph 84: Epoch: 008, Loss: -0.4839\n",
      "Graph 84: Epoch: 009, Loss: -0.5160\n",
      "Graph 84: Epoch: 010, Loss: -0.4833\n",
      "Graph 84: Epoch: 011, Loss: -0.4834\n",
      "Graph 84: Epoch: 012, Loss: -0.8680\n",
      "Graph 84: Epoch: 013, Loss: -0.5145\n",
      "Graph 84: Epoch: 014, Loss: -0.4858\n",
      "Graph 84: Epoch: 015, Loss: -0.5133\n",
      "Graph 84: Epoch: 016, Loss: -0.5132\n",
      "Graph 84: Epoch: 017, Loss: -0.5137\n",
      "Graph 84: Epoch: 018, Loss: -0.4852\n",
      "Graph 84: Epoch: 019, Loss: -0.4848\n",
      "Graph 84: Epoch: 020, Loss: -0.5149\n",
      "Graph 84: Epoch: 021, Loss: -0.4848\n",
      "Graph 84: Epoch: 022, Loss: -0.4851\n",
      "Graph 84: Epoch: 023, Loss: -0.5141\n",
      "Graph 84: Epoch: 024, Loss: -0.4861\n",
      "Graph 84: Epoch: 025, Loss: -0.5132\n",
      "Graph 84: Epoch: 026, Loss: -0.4869\n",
      "Graph 84: Epoch: 027, Loss: -0.4875\n",
      "Graph 84: Epoch: 028, Loss: -0.5114\n",
      "Graph 84: Epoch: 029, Loss: -0.1257\n",
      "Graph 84: Epoch: 030, Loss: -0.5108\n",
      "Graph 84: Epoch: 031, Loss: -0.5112\n",
      "Graph 84: Epoch: 032, Loss: -0.4879\n",
      "Graph 84: Epoch: 033, Loss: -0.4877\n",
      "Graph 84: Epoch: 034, Loss: -0.4880\n",
      "Graph 84: Epoch: 035, Loss: -0.5112\n",
      "Graph 84: Epoch: 036, Loss: -0.5111\n",
      "Graph 84: Epoch: 037, Loss: -0.5114\n",
      "Graph 84: Epoch: 038, Loss: -0.4877\n",
      "Graph 84: Epoch: 039, Loss: -0.4875\n",
      "Graph 84: Epoch: 040, Loss: -0.1311\n",
      "Graph 84: Epoch: 041, Loss: -0.4878\n",
      "Graph 84: Epoch: 042, Loss: -0.4884\n",
      "Graph 84: Epoch: 043, Loss: -0.4894\n",
      "Graph 84: Epoch: 044, Loss: -0.4909\n",
      "Graph 84: Epoch: 045, Loss: -0.5073\n",
      "Graph 84: Epoch: 046, Loss: -0.5062\n",
      "Graph 84: Epoch: 047, Loss: -0.4943\n",
      "Graph 84: Epoch: 048, Loss: -0.5047\n",
      "Graph 84: Epoch: 049, Loss: -0.4956\n",
      "Graph 84: Epoch: 050, Loss: -0.5036\n",
      "Graph 84: Epoch: 051, Loss: -0.4967\n",
      "Graph 84: Epoch: 052, Loss: -0.4974\n",
      "Graph 84: Epoch: 053, Loss: -0.4985\n",
      "Graph 84: Epoch: 054, Loss: -0.4999\n",
      "Graph 84: Epoch: 055, Loss: -0.5010\n",
      "Graph 84: Epoch: 056, Loss: -0.5023\n",
      "Graph 84: Epoch: 057, Loss: -0.4960\n",
      "Graph 84: Epoch: 058, Loss: -0.4950\n",
      "Graph 84: Epoch: 059, Loss: -0.5054\n",
      "Graph 84: Epoch: 060, Loss: -0.4937\n",
      "Graph 84: Epoch: 061, Loss: -0.4934\n",
      "Graph 84: Epoch: 062, Loss: -0.4937\n",
      "Graph 84: Epoch: 063, Loss: -0.5055\n",
      "Graph 84: Epoch: 064, Loss: -0.1326\n",
      "Graph 84: Epoch: 065, Loss: -0.8663\n",
      "Graph 84: Epoch: 066, Loss: -0.4952\n",
      "Graph 84: Epoch: 067, Loss: -0.5042\n",
      "Graph 84: Epoch: 068, Loss: -0.5042\n",
      "Graph 84: Epoch: 069, Loss: -0.4953\n",
      "Graph 84: Epoch: 070, Loss: -0.5046\n",
      "Graph 84: Epoch: 071, Loss: -0.1336\n",
      "Graph 84: Epoch: 072, Loss: -0.4948\n",
      "Graph 84: Epoch: 073, Loss: -0.4952\n",
      "Graph 84: Epoch: 074, Loss: -0.4961\n",
      "Graph 84: Epoch: 075, Loss: -0.8626\n",
      "Graph 84: Epoch: 076, Loss: -0.4983\n",
      "Graph 84: Epoch: 077, Loss: -0.8621\n",
      "Graph 84: Epoch: 078, Loss: -0.5006\n",
      "Graph 84: Epoch: 079, Loss: -0.4980\n",
      "Graph 84: Epoch: 080, Loss: -0.4973\n",
      "Graph 84: Epoch: 081, Loss: -0.5029\n",
      "Graph 84: Epoch: 082, Loss: -0.4965\n",
      "Graph 84: Epoch: 083, Loss: -0.5036\n",
      "Graph 84: Epoch: 084, Loss: -0.5042\n",
      "Graph 84: Epoch: 085, Loss: -0.4948\n",
      "Graph 84: Epoch: 086, Loss: -0.8651\n",
      "Graph 84: Epoch: 087, Loss: -0.4943\n",
      "Graph 84: Epoch: 088, Loss: -0.5053\n",
      "Graph 84: Epoch: 089, Loss: -0.5055\n",
      "Graph 84: Epoch: 090, Loss: -0.4939\n",
      "Graph 84: Epoch: 091, Loss: -0.4938\n",
      "Graph 84: Epoch: 092, Loss: -0.5058\n",
      "Graph 84: Epoch: 093, Loss: -0.4941\n",
      "Graph 84: Epoch: 094, Loss: -0.4945\n",
      "Graph 84: Epoch: 095, Loss: -0.5046\n",
      "Graph 84: Epoch: 096, Loss: -0.5043\n",
      "Graph 84: Epoch: 097, Loss: -0.4955\n",
      "Graph 84: Epoch: 098, Loss: -0.4958\n",
      "Graph 84: Epoch: 099, Loss: -0.4965\n",
      "Graph 84: Epoch: 100, Loss: -0.8747\n",
      "Graph 85: Epoch: 001, Loss: -0.0383\n",
      "Graph 85: Epoch: 002, Loss: -0.1724\n",
      "Graph 85: Epoch: 003, Loss: -0.4249\n",
      "Graph 85: Epoch: 004, Loss: -0.3973\n",
      "Graph 85: Epoch: 005, Loss: -0.3959\n",
      "Graph 85: Epoch: 006, Loss: -0.3550\n",
      "Graph 85: Epoch: 007, Loss: -0.4838\n",
      "Graph 85: Epoch: 008, Loss: -0.4755\n",
      "Graph 85: Epoch: 009, Loss: -0.2565\n",
      "Graph 85: Epoch: 010, Loss: -0.4428\n",
      "Graph 85: Epoch: 011, Loss: -0.5129\n",
      "Graph 85: Epoch: 012, Loss: -0.3280\n",
      "Graph 85: Epoch: 013, Loss: -0.5723\n",
      "Graph 85: Epoch: 014, Loss: -0.4639\n",
      "Graph 85: Epoch: 015, Loss: -0.6016\n",
      "Graph 85: Epoch: 016, Loss: -0.5233\n",
      "Graph 85: Epoch: 017, Loss: -0.2717\n",
      "Graph 85: Epoch: 018, Loss: -0.6736\n",
      "Graph 85: Epoch: 019, Loss: -0.6025\n",
      "Graph 85: Epoch: 020, Loss: -0.5456\n",
      "Graph 85: Epoch: 021, Loss: -0.3425\n",
      "Graph 85: Epoch: 022, Loss: -0.0708\n",
      "Graph 85: Epoch: 023, Loss: -0.4814\n",
      "Graph 85: Epoch: 024, Loss: -0.4126\n",
      "Graph 85: Epoch: 025, Loss: -0.2735\n",
      "Graph 85: Epoch: 026, Loss: -0.4795\n",
      "Graph 85: Epoch: 027, Loss: -0.4842\n",
      "Graph 85: Epoch: 028, Loss: -0.4826\n",
      "Graph 85: Epoch: 029, Loss: -0.3456\n",
      "Graph 85: Epoch: 030, Loss: -0.2085\n",
      "Graph 85: Epoch: 031, Loss: -0.6926\n",
      "Graph 85: Epoch: 032, Loss: -0.4165\n",
      "Graph 85: Epoch: 033, Loss: -0.5555\n",
      "Graph 85: Epoch: 034, Loss: -0.4155\n",
      "Graph 85: Epoch: 035, Loss: -0.5542\n",
      "Graph 85: Epoch: 036, Loss: -0.5577\n",
      "Graph 85: Epoch: 037, Loss: -0.3499\n",
      "Graph 85: Epoch: 038, Loss: -0.4895\n",
      "Graph 85: Epoch: 039, Loss: -0.4200\n",
      "Graph 85: Epoch: 040, Loss: -0.6273\n",
      "Graph 85: Epoch: 041, Loss: -0.7703\n",
      "Graph 85: Epoch: 042, Loss: -0.4895\n",
      "Graph 85: Epoch: 043, Loss: -0.3508\n",
      "Graph 85: Epoch: 044, Loss: -0.5597\n",
      "Graph 85: Epoch: 045, Loss: -0.6318\n",
      "Graph 85: Epoch: 046, Loss: -0.4914\n",
      "Graph 85: Epoch: 047, Loss: -0.2804\n",
      "Graph 85: Epoch: 048, Loss: -0.4221\n",
      "Graph 85: Epoch: 049, Loss: -0.1415\n",
      "Graph 85: Epoch: 050, Loss: -0.4220\n",
      "Graph 85: Epoch: 051, Loss: -0.4225\n",
      "Graph 85: Epoch: 052, Loss: -0.4224\n",
      "Graph 85: Epoch: 053, Loss: -0.7032\n",
      "Graph 85: Epoch: 054, Loss: -0.4228\n",
      "Graph 85: Epoch: 055, Loss: -0.2821\n",
      "Graph 85: Epoch: 056, Loss: -0.4905\n",
      "Graph 85: Epoch: 057, Loss: -0.4233\n",
      "Graph 85: Epoch: 058, Loss: -0.4934\n",
      "Graph 85: Epoch: 059, Loss: -0.3529\n",
      "Graph 85: Epoch: 060, Loss: -0.4927\n",
      "Graph 85: Epoch: 061, Loss: -0.4937\n",
      "Graph 85: Epoch: 062, Loss: -0.6347\n",
      "Graph 85: Epoch: 063, Loss: -0.7757\n",
      "Graph 85: Epoch: 064, Loss: -0.3530\n",
      "Graph 85: Epoch: 065, Loss: -0.6352\n",
      "Graph 85: Epoch: 066, Loss: -0.6356\n",
      "Graph 85: Epoch: 067, Loss: -0.5648\n",
      "Graph 85: Epoch: 068, Loss: -0.4238\n",
      "Graph 85: Epoch: 069, Loss: -0.4242\n",
      "Graph 85: Epoch: 070, Loss: -0.4946\n",
      "Graph 85: Epoch: 071, Loss: -0.7067\n",
      "Graph 85: Epoch: 072, Loss: -0.7774\n",
      "Graph 85: Epoch: 073, Loss: -0.6362\n",
      "Graph 85: Epoch: 074, Loss: -0.4944\n",
      "Graph 85: Epoch: 075, Loss: -0.6361\n",
      "Graph 85: Epoch: 076, Loss: -0.2832\n",
      "Graph 85: Epoch: 077, Loss: -0.3537\n",
      "Graph 85: Epoch: 078, Loss: -0.6366\n",
      "Graph 85: Epoch: 079, Loss: -0.2126\n",
      "Graph 85: Epoch: 080, Loss: -0.4947\n",
      "Graph 85: Epoch: 081, Loss: -0.4950\n",
      "Graph 85: Epoch: 082, Loss: -0.6369\n",
      "Graph 85: Epoch: 083, Loss: -0.6368\n",
      "Graph 85: Epoch: 084, Loss: -0.3543\n",
      "Graph 85: Epoch: 085, Loss: -0.2836\n",
      "Graph 85: Epoch: 086, Loss: -0.4250\n",
      "Graph 85: Epoch: 087, Loss: -0.5668\n",
      "Graph 85: Epoch: 088, Loss: -0.5668\n",
      "Graph 85: Epoch: 089, Loss: -0.5666\n",
      "Graph 85: Epoch: 090, Loss: -0.3544\n",
      "Graph 85: Epoch: 091, Loss: -0.4253\n",
      "Graph 85: Epoch: 092, Loss: -0.2835\n",
      "Graph 85: Epoch: 093, Loss: -0.4958\n",
      "Graph 85: Epoch: 094, Loss: -0.4964\n",
      "Graph 85: Epoch: 095, Loss: -0.6379\n",
      "Graph 85: Epoch: 096, Loss: -0.6381\n",
      "Graph 85: Epoch: 097, Loss: -0.5672\n",
      "Graph 85: Epoch: 098, Loss: -0.6378\n",
      "Graph 85: Epoch: 099, Loss: -0.5672\n",
      "Graph 85: Epoch: 100, Loss: -0.2131\n",
      "Graph 86: Epoch: 001, Loss: -0.0989\n",
      "Graph 86: Epoch: 002, Loss: -0.1787\n",
      "Graph 86: Epoch: 003, Loss: -0.0550\n",
      "Graph 86: Epoch: 004, Loss: -0.1408\n",
      "Graph 86: Epoch: 005, Loss: -0.1337\n",
      "Graph 86: Epoch: 006, Loss: -0.4481\n",
      "Graph 86: Epoch: 007, Loss: -0.2656\n",
      "Graph 86: Epoch: 008, Loss: -0.2353\n",
      "Graph 86: Epoch: 009, Loss: -0.2490\n",
      "Graph 86: Epoch: 010, Loss: -0.3663\n",
      "Graph 86: Epoch: 011, Loss: -0.1808\n",
      "Graph 86: Epoch: 012, Loss: -0.2791\n",
      "Graph 86: Epoch: 013, Loss: -0.4765\n",
      "Graph 86: Epoch: 014, Loss: -0.5318\n",
      "Graph 86: Epoch: 015, Loss: -0.5013\n",
      "Graph 86: Epoch: 016, Loss: -0.3765\n",
      "Graph 86: Epoch: 017, Loss: -0.4542\n",
      "Graph 86: Epoch: 018, Loss: -0.2342\n",
      "Graph 86: Epoch: 019, Loss: -0.3566\n",
      "Graph 86: Epoch: 020, Loss: -0.3966\n",
      "Graph 86: Epoch: 021, Loss: -0.4019\n",
      "Graph 86: Epoch: 022, Loss: -0.4774\n",
      "Graph 86: Epoch: 023, Loss: -0.1399\n",
      "Graph 86: Epoch: 024, Loss: -0.3460\n",
      "Graph 86: Epoch: 025, Loss: -0.3865\n",
      "Graph 86: Epoch: 026, Loss: -0.4759\n",
      "Graph 86: Epoch: 027, Loss: -0.2542\n",
      "Graph 86: Epoch: 028, Loss: -0.3814\n",
      "Graph 86: Epoch: 029, Loss: -0.4057\n",
      "Graph 86: Epoch: 030, Loss: -0.4171\n",
      "Graph 86: Epoch: 031, Loss: -0.2706\n",
      "Graph 86: Epoch: 032, Loss: -0.5395\n",
      "Graph 86: Epoch: 033, Loss: -0.5381\n",
      "Graph 86: Epoch: 034, Loss: -0.2876\n",
      "Graph 86: Epoch: 035, Loss: -0.4473\n",
      "Graph 86: Epoch: 036, Loss: -0.4062\n",
      "Graph 86: Epoch: 037, Loss: -0.5302\n",
      "Graph 86: Epoch: 038, Loss: -0.5921\n",
      "Graph 86: Epoch: 039, Loss: -0.3278\n",
      "Graph 86: Epoch: 040, Loss: -0.1807\n",
      "Graph 86: Epoch: 041, Loss: -0.5270\n",
      "Graph 86: Epoch: 042, Loss: -0.2626\n",
      "Graph 86: Epoch: 043, Loss: -0.5651\n",
      "Graph 86: Epoch: 044, Loss: -0.4624\n",
      "Graph 86: Epoch: 045, Loss: -0.2220\n",
      "Graph 86: Epoch: 046, Loss: -0.3942\n",
      "Graph 86: Epoch: 047, Loss: -0.4670\n",
      "Graph 86: Epoch: 048, Loss: -0.6461\n",
      "Graph 86: Epoch: 049, Loss: -0.3464\n",
      "Graph 86: Epoch: 050, Loss: -0.3507\n",
      "Graph 86: Epoch: 051, Loss: -0.7210\n",
      "Graph 86: Epoch: 052, Loss: -0.2717\n",
      "Graph 86: Epoch: 053, Loss: -0.6071\n",
      "Graph 86: Epoch: 054, Loss: -0.2294\n",
      "Graph 86: Epoch: 055, Loss: -0.3078\n",
      "Graph 86: Epoch: 056, Loss: -0.4354\n",
      "Graph 86: Epoch: 057, Loss: -0.5870\n",
      "Graph 86: Epoch: 058, Loss: -0.3156\n",
      "Graph 86: Epoch: 059, Loss: -0.2542\n",
      "Graph 86: Epoch: 060, Loss: -0.3859\n",
      "Graph 86: Epoch: 061, Loss: -0.3570\n",
      "Graph 86: Epoch: 062, Loss: -0.2804\n",
      "Graph 86: Epoch: 063, Loss: -0.3409\n",
      "Graph 86: Epoch: 064, Loss: -0.3136\n",
      "Graph 86: Epoch: 065, Loss: -0.5599\n",
      "Graph 86: Epoch: 066, Loss: -0.4670\n",
      "Graph 86: Epoch: 067, Loss: -0.5990\n",
      "Graph 86: Epoch: 068, Loss: -0.4601\n",
      "Graph 86: Epoch: 069, Loss: -0.4197\n",
      "Graph 86: Epoch: 070, Loss: -0.7042\n",
      "Graph 86: Epoch: 071, Loss: -0.1961\n",
      "Graph 86: Epoch: 072, Loss: -0.3381\n",
      "Graph 86: Epoch: 073, Loss: -0.1711\n",
      "Graph 86: Epoch: 074, Loss: -0.4508\n",
      "Graph 86: Epoch: 075, Loss: -0.2875\n",
      "Graph 86: Epoch: 076, Loss: -0.6339\n",
      "Graph 86: Epoch: 077, Loss: -0.0483\n",
      "Graph 86: Epoch: 078, Loss: -0.3078\n",
      "Graph 86: Epoch: 079, Loss: -0.4430\n",
      "Graph 86: Epoch: 080, Loss: -0.3180\n",
      "Graph 86: Epoch: 081, Loss: -0.2857\n",
      "Graph 86: Epoch: 082, Loss: -0.1332\n",
      "Graph 86: Epoch: 083, Loss: -0.2546\n",
      "Graph 86: Epoch: 084, Loss: -0.4372\n",
      "Graph 86: Epoch: 085, Loss: -0.6017\n",
      "Graph 86: Epoch: 086, Loss: -0.6118\n",
      "Graph 86: Epoch: 087, Loss: -0.4228\n",
      "Graph 86: Epoch: 088, Loss: -0.1195\n",
      "Graph 86: Epoch: 089, Loss: -0.5872\n",
      "Graph 86: Epoch: 090, Loss: -0.0967\n",
      "Graph 86: Epoch: 091, Loss: -0.7579\n",
      "Graph 86: Epoch: 092, Loss: -0.2936\n",
      "Graph 86: Epoch: 093, Loss: -0.9282\n",
      "Graph 86: Epoch: 094, Loss: -0.0967\n",
      "Graph 86: Epoch: 095, Loss: -0.3423\n",
      "Graph 86: Epoch: 096, Loss: -0.1985\n",
      "Graph 86: Epoch: 097, Loss: -0.2647\n",
      "Graph 86: Epoch: 098, Loss: -0.6569\n",
      "Graph 86: Epoch: 099, Loss: -0.1516\n",
      "Graph 86: Epoch: 100, Loss: -0.3629\n",
      "Graph 87: Epoch: 001, Loss: -0.5000\n",
      "Graph 87: Epoch: 002, Loss: -0.5050\n",
      "Graph 87: Epoch: 003, Loss: -0.5100\n",
      "Graph 87: Epoch: 004, Loss: -0.4850\n",
      "Graph 87: Epoch: 005, Loss: -0.4829\n",
      "Graph 87: Epoch: 006, Loss: -0.4826\n",
      "Graph 87: Epoch: 007, Loss: -0.4833\n",
      "Graph 87: Epoch: 008, Loss: -0.4849\n",
      "Graph 87: Epoch: 009, Loss: -0.5129\n",
      "Graph 87: Epoch: 010, Loss: -0.5119\n",
      "Graph 87: Epoch: 011, Loss: -0.5118\n",
      "Graph 87: Epoch: 012, Loss: -0.4876\n",
      "Graph 87: Epoch: 013, Loss: -0.4877\n",
      "Graph 87: Epoch: 014, Loss: -0.5115\n",
      "Graph 87: Epoch: 015, Loss: -0.5115\n",
      "Graph 87: Epoch: 016, Loss: -0.5121\n",
      "Graph 87: Epoch: 017, Loss: -0.5132\n",
      "Graph 87: Epoch: 018, Loss: -0.4852\n",
      "Graph 87: Epoch: 019, Loss: -0.4844\n",
      "Graph 87: Epoch: 020, Loss: -0.5158\n",
      "Graph 87: Epoch: 021, Loss: -0.5165\n",
      "Graph 87: Epoch: 022, Loss: -0.4823\n",
      "Graph 87: Epoch: 023, Loss: -0.4818\n",
      "Graph 87: Epoch: 024, Loss: -0.4819\n",
      "Graph 87: Epoch: 025, Loss: -0.5174\n",
      "Graph 87: Epoch: 026, Loss: -0.5174\n",
      "Graph 87: Epoch: 027, Loss: -0.4821\n",
      "Graph 87: Epoch: 028, Loss: -0.5178\n",
      "Graph 87: Epoch: 029, Loss: -0.1118\n",
      "Graph 87: Epoch: 030, Loss: -0.8869\n",
      "Graph 87: Epoch: 031, Loss: -0.5189\n",
      "Graph 87: Epoch: 032, Loss: -0.4802\n",
      "Graph 87: Epoch: 033, Loss: -0.1119\n",
      "Graph 87: Epoch: 034, Loss: -0.4799\n",
      "Graph 87: Epoch: 035, Loss: -0.5196\n",
      "Graph 87: Epoch: 036, Loss: -0.4803\n",
      "Graph 87: Epoch: 037, Loss: -0.5192\n",
      "Graph 87: Epoch: 038, Loss: -0.5193\n",
      "Graph 87: Epoch: 039, Loss: -0.5200\n",
      "Graph 87: Epoch: 040, Loss: -0.5210\n",
      "Graph 87: Epoch: 041, Loss: -0.4775\n",
      "Graph 87: Epoch: 042, Loss: -0.4766\n",
      "Graph 87: Epoch: 043, Loss: -0.5236\n",
      "Graph 87: Epoch: 044, Loss: -0.5243\n",
      "Graph 87: Epoch: 045, Loss: -0.4745\n",
      "Graph 87: Epoch: 046, Loss: -0.5260\n",
      "Graph 87: Epoch: 047, Loss: -0.5270\n",
      "Graph 87: Epoch: 048, Loss: -0.5284\n",
      "Graph 87: Epoch: 049, Loss: -0.5302\n",
      "Graph 87: Epoch: 050, Loss: -0.5323\n",
      "Graph 87: Epoch: 051, Loss: -0.4653\n",
      "Graph 87: Epoch: 052, Loss: -0.5363\n",
      "Graph 87: Epoch: 053, Loss: -0.5383\n",
      "Graph 87: Epoch: 054, Loss: -0.4594\n",
      "Graph 87: Epoch: 055, Loss: -0.5422\n",
      "Graph 87: Epoch: 056, Loss: -0.1085\n",
      "Graph 87: Epoch: 057, Loss: -0.4544\n",
      "Graph 87: Epoch: 058, Loss: -0.5465\n",
      "Graph 87: Epoch: 059, Loss: -0.5478\n",
      "Graph 87: Epoch: 060, Loss: -0.5495\n",
      "Graph 87: Epoch: 061, Loss: -0.5515\n",
      "Graph 87: Epoch: 062, Loss: -0.5538\n",
      "Graph 87: Epoch: 063, Loss: -0.4436\n",
      "Graph 87: Epoch: 064, Loss: -0.4418\n",
      "Graph 87: Epoch: 065, Loss: -0.8926\n",
      "Graph 87: Epoch: 066, Loss: -0.1065\n",
      "Graph 87: Epoch: 067, Loss: -0.8936\n",
      "Graph 87: Epoch: 068, Loss: -0.4376\n",
      "Graph 87: Epoch: 069, Loss: -0.5628\n",
      "Graph 87: Epoch: 070, Loss: -0.4363\n",
      "Graph 87: Epoch: 071, Loss: -0.5640\n",
      "Graph 87: Epoch: 072, Loss: -0.8962\n",
      "Graph 87: Epoch: 073, Loss: -0.1028\n",
      "Graph 87: Epoch: 074, Loss: -0.1025\n",
      "Graph 87: Epoch: 075, Loss: -0.1027\n",
      "Graph 87: Epoch: 076, Loss: -0.5667\n",
      "Graph 87: Epoch: 077, Loss: -0.8961\n",
      "Graph 87: Epoch: 078, Loss: -0.4319\n",
      "Graph 87: Epoch: 079, Loss: -0.4317\n",
      "Graph 87: Epoch: 080, Loss: -0.4321\n",
      "Graph 87: Epoch: 081, Loss: -0.5671\n",
      "Graph 87: Epoch: 082, Loss: -0.4332\n",
      "Graph 87: Epoch: 083, Loss: -0.4339\n",
      "Graph 87: Epoch: 084, Loss: -0.8942\n",
      "Graph 87: Epoch: 085, Loss: -0.4360\n",
      "Graph 87: Epoch: 086, Loss: -0.5627\n",
      "Graph 87: Epoch: 087, Loss: -0.5621\n",
      "Graph 87: Epoch: 088, Loss: -0.5620\n",
      "Graph 87: Epoch: 089, Loss: -0.8949\n",
      "Graph 87: Epoch: 090, Loss: -0.4369\n",
      "Graph 87: Epoch: 091, Loss: -0.5631\n",
      "Graph 87: Epoch: 092, Loss: -0.8972\n",
      "Graph 87: Epoch: 093, Loss: -0.5644\n",
      "Graph 87: Epoch: 094, Loss: -0.4345\n",
      "Graph 87: Epoch: 095, Loss: -0.4340\n",
      "Graph 87: Epoch: 096, Loss: -0.5660\n",
      "Graph 87: Epoch: 097, Loss: -0.4336\n",
      "Graph 87: Epoch: 098, Loss: -0.4337\n",
      "Graph 87: Epoch: 099, Loss: -0.0973\n",
      "Graph 87: Epoch: 100, Loss: -0.4350\n",
      "Graph 88: Epoch: 001, Loss: -0.5000\n",
      "Graph 88: Epoch: 002, Loss: -0.4950\n",
      "Graph 88: Epoch: 003, Loss: -0.5047\n",
      "Graph 88: Epoch: 004, Loss: -0.5064\n",
      "Graph 88: Epoch: 005, Loss: -0.4909\n",
      "Graph 88: Epoch: 006, Loss: -0.5098\n",
      "Graph 88: Epoch: 007, Loss: -0.4885\n",
      "Graph 88: Epoch: 008, Loss: -0.5119\n",
      "Graph 88: Epoch: 009, Loss: -0.8752\n",
      "Graph 88: Epoch: 010, Loss: -0.4862\n",
      "Graph 88: Epoch: 011, Loss: -0.1230\n",
      "Graph 88: Epoch: 012, Loss: -0.4863\n",
      "Graph 88: Epoch: 013, Loss: -0.4869\n",
      "Graph 88: Epoch: 014, Loss: -0.5118\n",
      "Graph 88: Epoch: 015, Loss: -0.5114\n",
      "Graph 88: Epoch: 016, Loss: -0.5118\n",
      "Graph 88: Epoch: 017, Loss: -0.8767\n",
      "Graph 88: Epoch: 018, Loss: -0.5132\n",
      "Graph 88: Epoch: 019, Loss: -0.5144\n",
      "Graph 88: Epoch: 020, Loss: -0.5160\n",
      "Graph 88: Epoch: 021, Loss: -0.1236\n",
      "Graph 88: Epoch: 022, Loss: -0.5201\n",
      "Graph 88: Epoch: 023, Loss: -0.5225\n",
      "Graph 88: Epoch: 024, Loss: -0.5253\n",
      "Graph 88: Epoch: 025, Loss: -0.5283\n",
      "Graph 88: Epoch: 026, Loss: -0.5316\n",
      "Graph 88: Epoch: 027, Loss: -0.5351\n",
      "Graph 88: Epoch: 028, Loss: -0.4613\n",
      "Graph 88: Epoch: 029, Loss: -0.4585\n",
      "Graph 88: Epoch: 030, Loss: -0.4567\n",
      "Graph 88: Epoch: 031, Loss: -0.4556\n",
      "Graph 88: Epoch: 032, Loss: -0.4551\n",
      "Graph 88: Epoch: 033, Loss: -0.4552\n",
      "Graph 88: Epoch: 034, Loss: -0.5441\n",
      "Graph 88: Epoch: 035, Loss: -0.1374\n",
      "Graph 88: Epoch: 036, Loss: -0.5443\n",
      "Graph 88: Epoch: 037, Loss: -0.4550\n",
      "Graph 88: Epoch: 038, Loss: -0.4549\n",
      "Graph 88: Epoch: 039, Loss: -0.4553\n",
      "Graph 88: Epoch: 040, Loss: -0.4562\n",
      "Graph 88: Epoch: 041, Loss: -0.5425\n",
      "Graph 88: Epoch: 042, Loss: -0.5418\n",
      "Graph 88: Epoch: 043, Loss: -0.4583\n",
      "Graph 88: Epoch: 044, Loss: -0.4589\n",
      "Graph 88: Epoch: 045, Loss: -0.4600\n",
      "Graph 88: Epoch: 046, Loss: -0.5386\n",
      "Graph 88: Epoch: 047, Loss: -0.5377\n",
      "Graph 88: Epoch: 048, Loss: -0.4625\n",
      "Graph 88: Epoch: 049, Loss: -0.5368\n",
      "Graph 88: Epoch: 050, Loss: -0.5367\n",
      "Graph 88: Epoch: 051, Loss: -0.5371\n",
      "Graph 88: Epoch: 052, Loss: -0.5380\n",
      "Graph 88: Epoch: 053, Loss: -0.8580\n",
      "Graph 88: Epoch: 054, Loss: -0.5403\n",
      "Graph 88: Epoch: 055, Loss: -0.5417\n",
      "Graph 88: Epoch: 056, Loss: -0.5434\n",
      "Graph 88: Epoch: 057, Loss: -0.4545\n",
      "Graph 88: Epoch: 058, Loss: -0.4531\n",
      "Graph 88: Epoch: 059, Loss: -0.5476\n",
      "Graph 88: Epoch: 060, Loss: -0.4513\n",
      "Graph 88: Epoch: 061, Loss: -0.5493\n",
      "Graph 88: Epoch: 062, Loss: -0.1432\n",
      "Graph 88: Epoch: 063, Loss: -0.4486\n",
      "Graph 88: Epoch: 064, Loss: -0.1451\n",
      "Graph 88: Epoch: 065, Loss: -0.4474\n",
      "Graph 88: Epoch: 066, Loss: -0.4472\n",
      "Graph 88: Epoch: 067, Loss: -0.4476\n",
      "Graph 88: Epoch: 068, Loss: -0.5515\n",
      "Graph 88: Epoch: 069, Loss: -0.5512\n",
      "Graph 88: Epoch: 070, Loss: -0.5515\n",
      "Graph 88: Epoch: 071, Loss: -0.5522\n",
      "Graph 88: Epoch: 072, Loss: -0.1528\n",
      "Graph 88: Epoch: 073, Loss: -0.5548\n",
      "Graph 88: Epoch: 074, Loss: -0.4435\n",
      "Graph 88: Epoch: 075, Loss: -0.5576\n",
      "Graph 88: Epoch: 076, Loss: -0.1604\n",
      "Graph 88: Epoch: 077, Loss: -0.5606\n",
      "Graph 88: Epoch: 078, Loss: -0.5626\n",
      "Graph 88: Epoch: 079, Loss: -0.4352\n",
      "Graph 88: Epoch: 080, Loss: -0.4336\n",
      "Graph 88: Epoch: 081, Loss: -0.4328\n",
      "Graph 88: Epoch: 082, Loss: -0.1736\n",
      "Graph 88: Epoch: 083, Loss: -0.5680\n",
      "Graph 88: Epoch: 084, Loss: -0.4310\n",
      "Graph 88: Epoch: 085, Loss: -0.8201\n",
      "Graph 88: Epoch: 086, Loss: -0.8197\n",
      "Graph 88: Epoch: 087, Loss: -0.5692\n",
      "Graph 88: Epoch: 088, Loss: -0.5694\n",
      "Graph 88: Epoch: 089, Loss: -0.8208\n",
      "Graph 88: Epoch: 090, Loss: -0.8218\n",
      "Graph 88: Epoch: 091, Loss: -0.4294\n",
      "Graph 88: Epoch: 092, Loss: -0.5701\n",
      "Graph 88: Epoch: 093, Loss: -0.4299\n",
      "Graph 88: Epoch: 094, Loss: -0.5697\n",
      "Graph 88: Epoch: 095, Loss: -0.4302\n",
      "Graph 88: Epoch: 096, Loss: -0.5694\n",
      "Graph 88: Epoch: 097, Loss: -0.4305\n",
      "Graph 88: Epoch: 098, Loss: -0.4309\n",
      "Graph 88: Epoch: 099, Loss: -0.4317\n",
      "Graph 88: Epoch: 100, Loss: -0.5670\n",
      "Graph 89: Epoch: 001, Loss: -0.0288\n",
      "Graph 89: Epoch: 002, Loss: -0.0862\n",
      "Graph 89: Epoch: 003, Loss: -0.1361\n",
      "Graph 89: Epoch: 004, Loss: -0.1975\n",
      "Graph 89: Epoch: 005, Loss: -0.2095\n",
      "Graph 89: Epoch: 006, Loss: -0.2353\n",
      "Graph 89: Epoch: 007, Loss: -0.3802\n",
      "Graph 89: Epoch: 008, Loss: -0.2387\n",
      "Graph 89: Epoch: 009, Loss: -0.2606\n",
      "Graph 89: Epoch: 010, Loss: -0.3170\n",
      "Graph 89: Epoch: 011, Loss: -0.3763\n",
      "Graph 89: Epoch: 012, Loss: -0.3561\n",
      "Graph 89: Epoch: 013, Loss: -0.3725\n",
      "Graph 89: Epoch: 014, Loss: -0.3482\n",
      "Graph 89: Epoch: 015, Loss: -0.3740\n",
      "Graph 89: Epoch: 016, Loss: -0.3691\n",
      "Graph 89: Epoch: 017, Loss: -0.3648\n",
      "Graph 89: Epoch: 018, Loss: -0.3967\n",
      "Graph 89: Epoch: 019, Loss: -0.2873\n",
      "Graph 89: Epoch: 020, Loss: -0.3688\n",
      "Graph 89: Epoch: 021, Loss: -0.3757\n",
      "Graph 89: Epoch: 022, Loss: -0.3752\n",
      "Graph 89: Epoch: 023, Loss: -0.4157\n",
      "Graph 89: Epoch: 024, Loss: -0.4258\n",
      "Graph 89: Epoch: 025, Loss: -0.3234\n",
      "Graph 89: Epoch: 026, Loss: -0.4324\n",
      "Graph 89: Epoch: 027, Loss: -0.3778\n",
      "Graph 89: Epoch: 028, Loss: -0.3843\n",
      "Graph 89: Epoch: 029, Loss: -0.4403\n",
      "Graph 89: Epoch: 030, Loss: -0.4156\n",
      "Graph 89: Epoch: 031, Loss: -0.4468\n",
      "Graph 89: Epoch: 032, Loss: -0.3334\n",
      "Graph 89: Epoch: 033, Loss: -0.4095\n",
      "Graph 89: Epoch: 034, Loss: -0.3719\n",
      "Graph 89: Epoch: 035, Loss: -0.4206\n",
      "Graph 89: Epoch: 036, Loss: -0.4442\n",
      "Graph 89: Epoch: 037, Loss: -0.5085\n",
      "Graph 89: Epoch: 038, Loss: -0.4139\n",
      "Graph 89: Epoch: 039, Loss: -0.3697\n",
      "Graph 89: Epoch: 040, Loss: -0.4655\n",
      "Graph 89: Epoch: 041, Loss: -0.4719\n",
      "Graph 89: Epoch: 042, Loss: -0.4459\n",
      "Graph 89: Epoch: 043, Loss: -0.4649\n",
      "Graph 89: Epoch: 044, Loss: -0.3692\n",
      "Graph 89: Epoch: 045, Loss: -0.4443\n",
      "Graph 89: Epoch: 046, Loss: -0.5747\n",
      "Graph 89: Epoch: 047, Loss: -0.3320\n",
      "Graph 89: Epoch: 048, Loss: -0.3253\n",
      "Graph 89: Epoch: 049, Loss: -0.5022\n",
      "Graph 89: Epoch: 050, Loss: -0.6006\n",
      "Graph 89: Epoch: 051, Loss: -0.4046\n",
      "Graph 89: Epoch: 052, Loss: -0.2622\n",
      "Graph 89: Epoch: 053, Loss: -0.6117\n",
      "Graph 89: Epoch: 054, Loss: -0.5016\n",
      "Graph 89: Epoch: 055, Loss: -0.4045\n",
      "Graph 89: Epoch: 056, Loss: -0.0870\n",
      "Graph 89: Epoch: 057, Loss: -0.5091\n",
      "Graph 89: Epoch: 058, Loss: -0.5190\n",
      "Graph 89: Epoch: 059, Loss: -0.4270\n",
      "Graph 89: Epoch: 060, Loss: -0.4080\n",
      "Graph 89: Epoch: 061, Loss: -0.3356\n",
      "Graph 89: Epoch: 062, Loss: -0.5447\n",
      "Graph 89: Epoch: 063, Loss: -0.3038\n",
      "Graph 89: Epoch: 064, Loss: -0.5504\n",
      "Graph 89: Epoch: 065, Loss: -0.5332\n",
      "Graph 89: Epoch: 066, Loss: -0.4280\n",
      "Graph 89: Epoch: 067, Loss: -0.3369\n",
      "Graph 89: Epoch: 068, Loss: -0.5362\n",
      "Graph 89: Epoch: 069, Loss: -0.7751\n",
      "Graph 89: Epoch: 070, Loss: -0.2365\n",
      "Graph 89: Epoch: 071, Loss: -0.6380\n",
      "Graph 89: Epoch: 072, Loss: -0.4047\n",
      "Graph 89: Epoch: 073, Loss: -0.5417\n",
      "Graph 89: Epoch: 074, Loss: -0.4332\n",
      "Graph 89: Epoch: 075, Loss: -0.4286\n",
      "Graph 89: Epoch: 076, Loss: -0.3070\n",
      "Graph 89: Epoch: 077, Loss: -0.3301\n",
      "Graph 89: Epoch: 078, Loss: -0.7124\n",
      "Graph 89: Epoch: 079, Loss: -0.1965\n",
      "Graph 89: Epoch: 080, Loss: -0.5476\n",
      "Graph 89: Epoch: 081, Loss: -0.4370\n",
      "Graph 89: Epoch: 082, Loss: -0.6580\n",
      "Graph 89: Epoch: 083, Loss: -0.5395\n",
      "Graph 89: Epoch: 084, Loss: -0.6512\n",
      "Graph 89: Epoch: 085, Loss: -0.4144\n",
      "Graph 89: Epoch: 086, Loss: -0.6465\n",
      "Graph 89: Epoch: 087, Loss: -0.5456\n",
      "Graph 89: Epoch: 088, Loss: -0.4153\n",
      "Graph 89: Epoch: 089, Loss: -0.5472\n",
      "Graph 89: Epoch: 090, Loss: -0.5584\n",
      "Graph 89: Epoch: 091, Loss: -0.5402\n",
      "Graph 89: Epoch: 092, Loss: -0.5448\n",
      "Graph 89: Epoch: 093, Loss: -0.3031\n",
      "Graph 89: Epoch: 094, Loss: -0.1964\n",
      "Graph 89: Epoch: 095, Loss: -0.5615\n",
      "Graph 89: Epoch: 096, Loss: -0.6871\n",
      "Graph 89: Epoch: 097, Loss: -0.2326\n",
      "Graph 89: Epoch: 098, Loss: -0.5511\n",
      "Graph 89: Epoch: 099, Loss: -0.4303\n",
      "Graph 89: Epoch: 100, Loss: -0.5610\n",
      "Graph 90: Epoch: 001, Loss: -0.0307\n",
      "Graph 90: Epoch: 002, Loss: -0.1231\n",
      "Graph 90: Epoch: 003, Loss: -0.2268\n",
      "Graph 90: Epoch: 004, Loss: -0.3169\n",
      "Graph 90: Epoch: 005, Loss: -0.2887\n",
      "Graph 90: Epoch: 006, Loss: -0.3655\n",
      "Graph 90: Epoch: 007, Loss: -0.3680\n",
      "Graph 90: Epoch: 008, Loss: -0.3919\n",
      "Graph 90: Epoch: 009, Loss: -0.3533\n",
      "Graph 90: Epoch: 010, Loss: -0.4099\n",
      "Graph 90: Epoch: 011, Loss: -0.3376\n",
      "Graph 90: Epoch: 012, Loss: -0.3450\n",
      "Graph 90: Epoch: 013, Loss: -0.3627\n",
      "Graph 90: Epoch: 014, Loss: -0.4508\n",
      "Graph 90: Epoch: 015, Loss: -0.4033\n",
      "Graph 90: Epoch: 016, Loss: -0.4402\n",
      "Graph 90: Epoch: 017, Loss: -0.4375\n",
      "Graph 90: Epoch: 018, Loss: -0.4179\n",
      "Graph 90: Epoch: 019, Loss: -0.3751\n",
      "Graph 90: Epoch: 020, Loss: -0.5965\n",
      "Graph 90: Epoch: 021, Loss: -0.1925\n",
      "Graph 90: Epoch: 022, Loss: -0.4775\n",
      "Graph 90: Epoch: 023, Loss: -0.5241\n",
      "Graph 90: Epoch: 024, Loss: -0.5163\n",
      "Graph 90: Epoch: 025, Loss: -0.4787\n",
      "Graph 90: Epoch: 026, Loss: -0.3795\n",
      "Graph 90: Epoch: 027, Loss: -0.5073\n",
      "Graph 90: Epoch: 028, Loss: -0.3710\n",
      "Graph 90: Epoch: 029, Loss: -0.4434\n",
      "Graph 90: Epoch: 030, Loss: -0.6382\n",
      "Graph 90: Epoch: 031, Loss: -0.4433\n",
      "Graph 90: Epoch: 032, Loss: -0.5111\n",
      "Graph 90: Epoch: 033, Loss: -0.5904\n",
      "Graph 90: Epoch: 034, Loss: -0.4480\n",
      "Graph 90: Epoch: 035, Loss: -0.5202\n",
      "Graph 90: Epoch: 036, Loss: -0.5798\n",
      "Graph 90: Epoch: 037, Loss: -0.5019\n",
      "Graph 90: Epoch: 038, Loss: -0.3768\n",
      "Graph 90: Epoch: 039, Loss: -0.5872\n",
      "Graph 90: Epoch: 040, Loss: -0.5259\n",
      "Graph 90: Epoch: 041, Loss: -0.7488\n",
      "Graph 90: Epoch: 042, Loss: -0.6722\n",
      "Graph 90: Epoch: 043, Loss: -0.4528\n",
      "Graph 90: Epoch: 044, Loss: -0.3776\n",
      "Graph 90: Epoch: 045, Loss: -0.2278\n",
      "Graph 90: Epoch: 046, Loss: -0.6036\n",
      "Graph 90: Epoch: 047, Loss: -0.4532\n",
      "Graph 90: Epoch: 048, Loss: -0.7429\n",
      "Graph 90: Epoch: 049, Loss: -0.4530\n",
      "Graph 90: Epoch: 050, Loss: -0.3789\n",
      "Graph 90: Epoch: 051, Loss: -0.3793\n",
      "Graph 90: Epoch: 052, Loss: -0.4537\n",
      "Graph 90: Epoch: 053, Loss: -0.6063\n",
      "Graph 90: Epoch: 054, Loss: -0.4548\n",
      "Graph 90: Epoch: 055, Loss: -0.3797\n",
      "Graph 90: Epoch: 056, Loss: -0.3041\n",
      "Graph 90: Epoch: 057, Loss: -0.4549\n",
      "Graph 90: Epoch: 058, Loss: -0.3812\n",
      "Graph 90: Epoch: 059, Loss: -0.4558\n",
      "Graph 90: Epoch: 060, Loss: -0.6801\n",
      "Graph 90: Epoch: 061, Loss: -0.5313\n",
      "Graph 90: Epoch: 062, Loss: -0.4552\n",
      "Graph 90: Epoch: 063, Loss: -0.7589\n",
      "Graph 90: Epoch: 064, Loss: -0.4609\n",
      "Graph 90: Epoch: 065, Loss: -0.8355\n",
      "Graph 90: Epoch: 066, Loss: -0.6831\n",
      "Graph 90: Epoch: 067, Loss: -0.3049\n",
      "Graph 90: Epoch: 068, Loss: -0.4564\n",
      "Graph 90: Epoch: 069, Loss: -0.6845\n",
      "Graph 90: Epoch: 070, Loss: -0.3050\n",
      "Graph 90: Epoch: 071, Loss: -0.4569\n",
      "Graph 90: Epoch: 072, Loss: -0.5329\n",
      "Graph 90: Epoch: 073, Loss: -0.5330\n",
      "Graph 90: Epoch: 074, Loss: -0.6081\n",
      "Graph 90: Epoch: 075, Loss: -0.4568\n",
      "Graph 90: Epoch: 076, Loss: -0.4574\n",
      "Graph 90: Epoch: 077, Loss: -0.3812\n",
      "Graph 90: Epoch: 078, Loss: -0.5335\n",
      "Graph 90: Epoch: 079, Loss: -0.5337\n",
      "Graph 90: Epoch: 080, Loss: -0.4574\n",
      "Graph 90: Epoch: 081, Loss: -0.5338\n",
      "Graph 90: Epoch: 082, Loss: -0.4577\n",
      "Graph 90: Epoch: 083, Loss: -0.4577\n",
      "Graph 90: Epoch: 084, Loss: -0.3816\n",
      "Graph 90: Epoch: 085, Loss: -0.5336\n",
      "Graph 90: Epoch: 086, Loss: -0.5340\n",
      "Graph 90: Epoch: 087, Loss: -0.4578\n",
      "Graph 90: Epoch: 088, Loss: -0.4580\n",
      "Graph 90: Epoch: 089, Loss: -0.5332\n",
      "Graph 90: Epoch: 090, Loss: -0.5341\n",
      "Graph 90: Epoch: 091, Loss: -0.3151\n",
      "Graph 90: Epoch: 092, Loss: -0.4582\n",
      "Graph 90: Epoch: 093, Loss: -0.4581\n",
      "Graph 90: Epoch: 094, Loss: -0.3057\n",
      "Graph 90: Epoch: 095, Loss: -0.5344\n",
      "Graph 90: Epoch: 096, Loss: -0.3803\n",
      "Graph 90: Epoch: 097, Loss: -0.3819\n",
      "Graph 90: Epoch: 098, Loss: -0.6085\n",
      "Graph 90: Epoch: 099, Loss: -0.3056\n",
      "Graph 90: Epoch: 100, Loss: -0.6101\n",
      "Graph 91: Epoch: 001, Loss: -0.0255\n",
      "Graph 91: Epoch: 002, Loss: -0.1852\n",
      "Graph 91: Epoch: 003, Loss: -0.2754\n",
      "Graph 91: Epoch: 004, Loss: -0.3602\n",
      "Graph 91: Epoch: 005, Loss: -0.3184\n",
      "Graph 91: Epoch: 006, Loss: -0.4039\n",
      "Graph 91: Epoch: 007, Loss: -0.4262\n",
      "Graph 91: Epoch: 008, Loss: -0.4433\n",
      "Graph 91: Epoch: 009, Loss: -0.4806\n",
      "Graph 91: Epoch: 010, Loss: -0.4072\n",
      "Graph 91: Epoch: 011, Loss: -0.5541\n",
      "Graph 91: Epoch: 012, Loss: -0.5008\n",
      "Graph 91: Epoch: 013, Loss: -0.4677\n",
      "Graph 91: Epoch: 014, Loss: -0.3707\n",
      "Graph 91: Epoch: 015, Loss: -0.3157\n",
      "Graph 91: Epoch: 016, Loss: -0.4744\n",
      "Graph 91: Epoch: 017, Loss: -0.3755\n",
      "Graph 91: Epoch: 018, Loss: -0.2699\n",
      "Graph 91: Epoch: 019, Loss: -0.3253\n",
      "Graph 91: Epoch: 020, Loss: -0.5931\n",
      "Graph 91: Epoch: 021, Loss: -0.2712\n",
      "Graph 91: Epoch: 022, Loss: -0.4323\n",
      "Graph 91: Epoch: 023, Loss: -0.3786\n",
      "Graph 91: Epoch: 024, Loss: -0.7556\n",
      "Graph 91: Epoch: 025, Loss: -0.4885\n",
      "Graph 91: Epoch: 026, Loss: -0.7005\n",
      "Graph 91: Epoch: 027, Loss: -0.7065\n",
      "Graph 91: Epoch: 028, Loss: -0.4914\n",
      "Graph 91: Epoch: 029, Loss: -0.3279\n",
      "Graph 91: Epoch: 030, Loss: -0.3828\n",
      "Graph 91: Epoch: 031, Loss: -0.6553\n",
      "Graph 91: Epoch: 032, Loss: -0.3829\n",
      "Graph 91: Epoch: 033, Loss: -0.3289\n",
      "Graph 91: Epoch: 034, Loss: -0.4928\n",
      "Graph 91: Epoch: 035, Loss: -0.5477\n",
      "Graph 91: Epoch: 036, Loss: -0.3841\n",
      "Graph 91: Epoch: 037, Loss: -0.6577\n",
      "Graph 91: Epoch: 038, Loss: -0.7128\n",
      "Graph 91: Epoch: 039, Loss: -0.4392\n",
      "Graph 91: Epoch: 040, Loss: -0.4390\n",
      "Graph 91: Epoch: 041, Loss: -0.4941\n",
      "Graph 91: Epoch: 042, Loss: -0.6593\n",
      "Graph 91: Epoch: 043, Loss: -0.3847\n",
      "Graph 91: Epoch: 044, Loss: -0.6049\n",
      "Graph 91: Epoch: 045, Loss: -0.3302\n",
      "Graph 91: Epoch: 046, Loss: -0.4949\n",
      "Graph 91: Epoch: 047, Loss: -0.5501\n",
      "Graph 91: Epoch: 048, Loss: -0.4404\n",
      "Graph 91: Epoch: 049, Loss: -0.5502\n",
      "Graph 91: Epoch: 050, Loss: -0.4403\n",
      "Graph 91: Epoch: 051, Loss: -0.4955\n",
      "Graph 91: Epoch: 052, Loss: -0.6057\n",
      "Graph 91: Epoch: 053, Loss: -0.5508\n",
      "Graph 91: Epoch: 054, Loss: -0.4408\n",
      "Graph 91: Epoch: 055, Loss: -0.5511\n",
      "Graph 91: Epoch: 056, Loss: -0.6063\n",
      "Graph 91: Epoch: 057, Loss: -0.4962\n",
      "Graph 91: Epoch: 058, Loss: -0.6064\n",
      "Graph 91: Epoch: 059, Loss: -0.5509\n",
      "Graph 91: Epoch: 060, Loss: -0.3860\n",
      "Graph 91: Epoch: 061, Loss: -0.5515\n",
      "Graph 91: Epoch: 062, Loss: -0.4965\n",
      "Graph 91: Epoch: 063, Loss: -0.6065\n",
      "Graph 91: Epoch: 064, Loss: -0.6621\n",
      "Graph 91: Epoch: 065, Loss: -0.3309\n",
      "Graph 91: Epoch: 066, Loss: -0.4967\n",
      "Graph 91: Epoch: 067, Loss: -0.5520\n",
      "Graph 91: Epoch: 068, Loss: -0.4416\n",
      "Graph 91: Epoch: 069, Loss: -0.6625\n",
      "Graph 91: Epoch: 070, Loss: -0.3868\n",
      "Graph 91: Epoch: 071, Loss: -0.3866\n",
      "Graph 91: Epoch: 072, Loss: -0.6074\n",
      "Graph 91: Epoch: 073, Loss: -0.4418\n",
      "Graph 91: Epoch: 074, Loss: -0.6074\n",
      "Graph 91: Epoch: 075, Loss: -0.3865\n",
      "Graph 91: Epoch: 076, Loss: -0.3315\n",
      "Graph 91: Epoch: 077, Loss: -0.4972\n",
      "Graph 91: Epoch: 078, Loss: -0.4419\n",
      "Graph 91: Epoch: 079, Loss: -0.3315\n",
      "Graph 91: Epoch: 080, Loss: -0.4972\n",
      "Graph 91: Epoch: 081, Loss: -0.4421\n",
      "Graph 91: Epoch: 082, Loss: -0.3866\n",
      "Graph 91: Epoch: 083, Loss: -0.4422\n",
      "Graph 91: Epoch: 084, Loss: -0.4974\n",
      "Graph 91: Epoch: 085, Loss: -0.5526\n",
      "Graph 91: Epoch: 086, Loss: -0.4973\n",
      "Graph 91: Epoch: 087, Loss: -0.5527\n",
      "Graph 91: Epoch: 088, Loss: -0.4976\n",
      "Graph 91: Epoch: 089, Loss: -0.6080\n",
      "Graph 91: Epoch: 090, Loss: -0.4976\n",
      "Graph 91: Epoch: 091, Loss: -0.4976\n",
      "Graph 91: Epoch: 092, Loss: -0.5529\n",
      "Graph 91: Epoch: 093, Loss: -0.5529\n",
      "Graph 91: Epoch: 094, Loss: -0.3871\n",
      "Graph 91: Epoch: 095, Loss: -0.5530\n",
      "Graph 91: Epoch: 096, Loss: -0.4425\n",
      "Graph 91: Epoch: 097, Loss: -0.4425\n",
      "Graph 91: Epoch: 098, Loss: -0.4978\n",
      "Graph 91: Epoch: 099, Loss: -0.5528\n",
      "Graph 91: Epoch: 100, Loss: -0.6638\n",
      "Graph 92: Epoch: 001, Loss: -0.0503\n",
      "Graph 92: Epoch: 002, Loss: -0.2297\n",
      "Graph 92: Epoch: 003, Loss: -0.4004\n",
      "Graph 92: Epoch: 004, Loss: -0.3077\n",
      "Graph 92: Epoch: 005, Loss: -0.3818\n",
      "Graph 92: Epoch: 006, Loss: -0.3146\n",
      "Graph 92: Epoch: 007, Loss: -0.3266\n",
      "Graph 92: Epoch: 008, Loss: -0.3140\n",
      "Graph 92: Epoch: 009, Loss: -0.3200\n",
      "Graph 92: Epoch: 010, Loss: -0.6944\n",
      "Graph 92: Epoch: 011, Loss: -0.4523\n",
      "Graph 92: Epoch: 012, Loss: -0.3411\n",
      "Graph 92: Epoch: 013, Loss: -0.2701\n",
      "Graph 92: Epoch: 014, Loss: -0.6029\n",
      "Graph 92: Epoch: 015, Loss: -0.5881\n",
      "Graph 92: Epoch: 016, Loss: -0.3336\n",
      "Graph 92: Epoch: 017, Loss: -0.4715\n",
      "Graph 92: Epoch: 018, Loss: -0.4076\n",
      "Graph 92: Epoch: 019, Loss: -0.5453\n",
      "Graph 92: Epoch: 020, Loss: -0.5471\n",
      "Graph 92: Epoch: 021, Loss: -0.2756\n",
      "Graph 92: Epoch: 022, Loss: -0.5495\n",
      "Graph 92: Epoch: 023, Loss: -0.5510\n",
      "Graph 92: Epoch: 024, Loss: -0.2759\n",
      "Graph 92: Epoch: 025, Loss: -0.5537\n",
      "Graph 92: Epoch: 026, Loss: -0.3471\n",
      "Graph 92: Epoch: 027, Loss: -0.4160\n",
      "Graph 92: Epoch: 028, Loss: -0.4870\n",
      "Graph 92: Epoch: 029, Loss: -0.7642\n",
      "Graph 92: Epoch: 030, Loss: -0.6264\n",
      "Graph 92: Epoch: 031, Loss: -0.6271\n",
      "Graph 92: Epoch: 032, Loss: -0.2798\n",
      "Graph 92: Epoch: 033, Loss: -0.5586\n",
      "Graph 92: Epoch: 034, Loss: -0.4871\n",
      "Graph 92: Epoch: 035, Loss: -0.3496\n",
      "Graph 92: Epoch: 036, Loss: -0.4170\n",
      "Graph 92: Epoch: 037, Loss: -0.4205\n",
      "Graph 92: Epoch: 038, Loss: -0.3508\n",
      "Graph 92: Epoch: 039, Loss: -0.3506\n",
      "Graph 92: Epoch: 040, Loss: -0.7016\n",
      "Graph 92: Epoch: 041, Loss: -0.4918\n",
      "Graph 92: Epoch: 042, Loss: -0.3513\n",
      "Graph 92: Epoch: 043, Loss: -0.4217\n",
      "Graph 92: Epoch: 044, Loss: -0.1412\n",
      "Graph 92: Epoch: 045, Loss: -0.4210\n",
      "Graph 92: Epoch: 046, Loss: -0.4928\n",
      "Graph 92: Epoch: 047, Loss: -0.4227\n",
      "Graph 92: Epoch: 048, Loss: -0.2820\n",
      "Graph 92: Epoch: 049, Loss: -0.4225\n",
      "Graph 92: Epoch: 050, Loss: -0.4935\n",
      "Graph 92: Epoch: 051, Loss: -0.4937\n",
      "Graph 92: Epoch: 052, Loss: -0.4232\n",
      "Graph 92: Epoch: 053, Loss: -0.4936\n",
      "Graph 92: Epoch: 054, Loss: -0.2824\n",
      "Graph 92: Epoch: 055, Loss: -0.4233\n",
      "Graph 92: Epoch: 056, Loss: -0.4941\n",
      "Graph 92: Epoch: 057, Loss: -0.5649\n",
      "Graph 92: Epoch: 058, Loss: -0.4236\n",
      "Graph 92: Epoch: 059, Loss: -0.6336\n",
      "Graph 92: Epoch: 060, Loss: -0.7767\n",
      "Graph 92: Epoch: 061, Loss: -0.4948\n",
      "Graph 92: Epoch: 062, Loss: -0.3536\n",
      "Graph 92: Epoch: 063, Loss: -0.4949\n",
      "Graph 92: Epoch: 064, Loss: -0.3534\n",
      "Graph 92: Epoch: 065, Loss: -0.6364\n",
      "Graph 92: Epoch: 066, Loss: -0.6360\n",
      "Graph 92: Epoch: 067, Loss: -0.4954\n",
      "Graph 92: Epoch: 068, Loss: -0.4243\n",
      "Graph 92: Epoch: 069, Loss: -0.4248\n",
      "Graph 92: Epoch: 070, Loss: -0.4954\n",
      "Graph 92: Epoch: 071, Loss: -0.5660\n",
      "Graph 92: Epoch: 072, Loss: -0.5664\n",
      "Graph 92: Epoch: 073, Loss: -0.7079\n",
      "Graph 92: Epoch: 074, Loss: -0.7789\n",
      "Graph 92: Epoch: 075, Loss: -0.5666\n",
      "Graph 92: Epoch: 076, Loss: -0.3545\n",
      "Graph 92: Epoch: 077, Loss: -0.4244\n",
      "Graph 92: Epoch: 078, Loss: -0.2837\n",
      "Graph 92: Epoch: 079, Loss: -0.4254\n",
      "Graph 92: Epoch: 080, Loss: -0.4963\n",
      "Graph 92: Epoch: 081, Loss: -0.5669\n",
      "Graph 92: Epoch: 082, Loss: -0.7087\n",
      "Graph 92: Epoch: 083, Loss: -0.4256\n",
      "Graph 92: Epoch: 084, Loss: -0.4963\n",
      "Graph 92: Epoch: 085, Loss: -0.3548\n",
      "Graph 92: Epoch: 086, Loss: -0.4965\n",
      "Graph 92: Epoch: 087, Loss: -0.7091\n",
      "Graph 92: Epoch: 088, Loss: -0.4257\n",
      "Graph 92: Epoch: 089, Loss: -0.4968\n",
      "Graph 92: Epoch: 090, Loss: -0.3549\n",
      "Graph 92: Epoch: 091, Loss: -0.5677\n",
      "Graph 92: Epoch: 092, Loss: -0.3550\n",
      "Graph 92: Epoch: 093, Loss: -0.4968\n",
      "Graph 92: Epoch: 094, Loss: -0.4260\n",
      "Graph 92: Epoch: 095, Loss: -0.4968\n",
      "Graph 92: Epoch: 096, Loss: -0.7098\n",
      "Graph 92: Epoch: 097, Loss: -0.5677\n",
      "Graph 92: Epoch: 098, Loss: -0.6389\n",
      "Graph 92: Epoch: 099, Loss: -0.3552\n",
      "Graph 92: Epoch: 100, Loss: -0.6390\n",
      "Graph 93: Epoch: 001, Loss: -0.0095\n",
      "Graph 93: Epoch: 002, Loss: -0.0438\n",
      "Graph 93: Epoch: 003, Loss: -0.1277\n",
      "Graph 93: Epoch: 004, Loss: -0.3067\n",
      "Graph 93: Epoch: 005, Loss: -0.4609\n",
      "Graph 93: Epoch: 006, Loss: -0.1572\n",
      "Graph 93: Epoch: 007, Loss: -0.3807\n",
      "Graph 93: Epoch: 008, Loss: -0.3953\n",
      "Graph 93: Epoch: 009, Loss: -0.3445\n",
      "Graph 93: Epoch: 010, Loss: -0.4549\n",
      "Graph 93: Epoch: 011, Loss: -0.4299\n",
      "Graph 93: Epoch: 012, Loss: -0.3801\n",
      "Graph 93: Epoch: 013, Loss: -0.4343\n",
      "Graph 93: Epoch: 014, Loss: -0.5016\n",
      "Graph 93: Epoch: 015, Loss: -0.5497\n",
      "Graph 93: Epoch: 016, Loss: -0.4352\n",
      "Graph 93: Epoch: 017, Loss: -0.5591\n",
      "Graph 93: Epoch: 018, Loss: -0.3371\n",
      "Graph 93: Epoch: 019, Loss: -0.6211\n",
      "Graph 93: Epoch: 020, Loss: -0.6206\n",
      "Graph 93: Epoch: 021, Loss: -0.5069\n",
      "Graph 93: Epoch: 022, Loss: -0.5559\n",
      "Graph 93: Epoch: 023, Loss: -0.2847\n",
      "Graph 93: Epoch: 024, Loss: -0.4585\n",
      "Graph 93: Epoch: 025, Loss: -0.4010\n",
      "Graph 93: Epoch: 026, Loss: -0.4583\n",
      "Graph 93: Epoch: 027, Loss: -0.6321\n",
      "Graph 93: Epoch: 028, Loss: -0.5747\n",
      "Graph 93: Epoch: 029, Loss: -0.4026\n",
      "Graph 93: Epoch: 030, Loss: -0.3458\n",
      "Graph 93: Epoch: 031, Loss: -0.5760\n",
      "Graph 93: Epoch: 032, Loss: -0.5780\n",
      "Graph 93: Epoch: 033, Loss: -0.4037\n",
      "Graph 93: Epoch: 034, Loss: -0.7482\n",
      "Graph 93: Epoch: 035, Loss: -0.4616\n",
      "Graph 93: Epoch: 036, Loss: -0.7509\n",
      "Graph 93: Epoch: 037, Loss: -0.4628\n",
      "Graph 93: Epoch: 038, Loss: -0.4635\n",
      "Graph 93: Epoch: 039, Loss: -0.4642\n",
      "Graph 93: Epoch: 040, Loss: -0.6949\n",
      "Graph 93: Epoch: 041, Loss: -0.4646\n",
      "Graph 93: Epoch: 042, Loss: -0.5802\n",
      "Graph 93: Epoch: 043, Loss: -0.4064\n",
      "Graph 93: Epoch: 044, Loss: -0.5810\n",
      "Graph 93: Epoch: 045, Loss: -0.4642\n",
      "Graph 93: Epoch: 046, Loss: -0.2911\n",
      "Graph 93: Epoch: 047, Loss: -0.4655\n",
      "Graph 93: Epoch: 048, Loss: -0.4076\n",
      "Graph 93: Epoch: 049, Loss: -0.5821\n",
      "Graph 93: Epoch: 050, Loss: -0.3495\n",
      "Graph 93: Epoch: 051, Loss: -0.4078\n",
      "Graph 93: Epoch: 052, Loss: -0.5160\n",
      "Graph 93: Epoch: 053, Loss: -0.3492\n",
      "Graph 93: Epoch: 054, Loss: -0.6411\n",
      "Graph 93: Epoch: 055, Loss: -0.5827\n",
      "Graph 93: Epoch: 056, Loss: -0.4654\n",
      "Graph 93: Epoch: 057, Loss: -0.3495\n",
      "Graph 93: Epoch: 058, Loss: -0.3499\n",
      "Graph 93: Epoch: 059, Loss: -0.4085\n",
      "Graph 93: Epoch: 060, Loss: -0.4660\n",
      "Graph 93: Epoch: 061, Loss: -0.6412\n",
      "Graph 93: Epoch: 062, Loss: -0.4077\n",
      "Graph 93: Epoch: 063, Loss: -0.3496\n",
      "Graph 93: Epoch: 064, Loss: -0.6996\n",
      "Graph 93: Epoch: 065, Loss: -0.2919\n",
      "Graph 93: Epoch: 066, Loss: -0.8159\n",
      "Graph 93: Epoch: 067, Loss: -0.5253\n",
      "Graph 93: Epoch: 068, Loss: -0.6411\n",
      "Graph 93: Epoch: 069, Loss: -0.4091\n",
      "Graph 93: Epoch: 070, Loss: -0.4670\n",
      "Graph 93: Epoch: 071, Loss: -0.5254\n",
      "Graph 93: Epoch: 072, Loss: -0.4088\n",
      "Graph 93: Epoch: 073, Loss: -0.4674\n",
      "Graph 93: Epoch: 074, Loss: -0.5841\n",
      "Graph 93: Epoch: 075, Loss: -0.4092\n",
      "Graph 93: Epoch: 076, Loss: -0.5260\n",
      "Graph 93: Epoch: 077, Loss: -0.3509\n",
      "Graph 93: Epoch: 078, Loss: -0.4678\n",
      "Graph 93: Epoch: 079, Loss: -0.5847\n",
      "Graph 93: Epoch: 080, Loss: -0.2335\n",
      "Graph 93: Epoch: 081, Loss: -0.4677\n",
      "Graph 93: Epoch: 082, Loss: -0.4095\n",
      "Graph 93: Epoch: 083, Loss: -0.5849\n",
      "Graph 93: Epoch: 084, Loss: -0.3498\n",
      "Graph 93: Epoch: 085, Loss: -0.5265\n",
      "Graph 93: Epoch: 086, Loss: -0.2926\n",
      "Graph 93: Epoch: 087, Loss: -0.6432\n",
      "Graph 93: Epoch: 088, Loss: -0.4096\n",
      "Graph 93: Epoch: 089, Loss: -0.5258\n",
      "Graph 93: Epoch: 090, Loss: -0.3512\n",
      "Graph 93: Epoch: 091, Loss: -0.4098\n",
      "Graph 93: Epoch: 092, Loss: -0.7021\n",
      "Graph 93: Epoch: 093, Loss: -0.7023\n",
      "Graph 93: Epoch: 094, Loss: -0.2343\n",
      "Graph 93: Epoch: 095, Loss: -0.4099\n",
      "Graph 93: Epoch: 096, Loss: -0.3514\n",
      "Graph 93: Epoch: 097, Loss: -0.3511\n",
      "Graph 93: Epoch: 098, Loss: -0.4683\n",
      "Graph 93: Epoch: 099, Loss: -0.5268\n",
      "Graph 93: Epoch: 100, Loss: -0.4099\n",
      "Graph 94: Epoch: 001, Loss: -0.0440\n",
      "Graph 94: Epoch: 002, Loss: -0.1389\n",
      "Graph 94: Epoch: 003, Loss: -0.3210\n",
      "Graph 94: Epoch: 004, Loss: -0.4551\n",
      "Graph 94: Epoch: 005, Loss: -0.1848\n",
      "Graph 94: Epoch: 006, Loss: -0.3605\n",
      "Graph 94: Epoch: 007, Loss: -0.4533\n",
      "Graph 94: Epoch: 008, Loss: -0.5210\n",
      "Graph 94: Epoch: 009, Loss: -0.3724\n",
      "Graph 94: Epoch: 010, Loss: -0.3767\n",
      "Graph 94: Epoch: 011, Loss: -0.2690\n",
      "Graph 94: Epoch: 012, Loss: -0.3857\n",
      "Graph 94: Epoch: 013, Loss: -0.6087\n",
      "Graph 94: Epoch: 014, Loss: -0.4418\n",
      "Graph 94: Epoch: 015, Loss: -0.6124\n",
      "Graph 94: Epoch: 016, Loss: -0.4487\n",
      "Graph 94: Epoch: 017, Loss: -0.4454\n",
      "Graph 94: Epoch: 018, Loss: -0.5648\n",
      "Graph 94: Epoch: 019, Loss: -0.3974\n",
      "Graph 94: Epoch: 020, Loss: -0.3967\n",
      "Graph 94: Epoch: 021, Loss: -0.3969\n",
      "Graph 94: Epoch: 022, Loss: -0.3427\n",
      "Graph 94: Epoch: 023, Loss: -0.4547\n",
      "Graph 94: Epoch: 024, Loss: -0.3378\n",
      "Graph 94: Epoch: 025, Loss: -0.4595\n",
      "Graph 94: Epoch: 026, Loss: -0.4017\n",
      "Graph 94: Epoch: 027, Loss: -0.4602\n",
      "Graph 94: Epoch: 028, Loss: -0.6910\n",
      "Graph 94: Epoch: 029, Loss: -0.3462\n",
      "Graph 94: Epoch: 030, Loss: -0.2844\n",
      "Graph 94: Epoch: 031, Loss: -0.5755\n",
      "Graph 94: Epoch: 032, Loss: -0.5764\n",
      "Graph 94: Epoch: 033, Loss: -0.5204\n",
      "Graph 94: Epoch: 034, Loss: -0.4057\n",
      "Graph 94: Epoch: 035, Loss: -0.6374\n",
      "Graph 94: Epoch: 036, Loss: -0.6942\n",
      "Graph 94: Epoch: 037, Loss: -0.4056\n",
      "Graph 94: Epoch: 038, Loss: -0.5219\n",
      "Graph 94: Epoch: 039, Loss: -0.4059\n",
      "Graph 94: Epoch: 040, Loss: -0.5789\n",
      "Graph 94: Epoch: 041, Loss: -0.5807\n",
      "Graph 94: Epoch: 042, Loss: -0.2908\n",
      "Graph 94: Epoch: 043, Loss: -0.3485\n",
      "Graph 94: Epoch: 044, Loss: -0.6395\n",
      "Graph 94: Epoch: 045, Loss: -0.4070\n",
      "Graph 94: Epoch: 046, Loss: -0.3490\n",
      "Graph 94: Epoch: 047, Loss: -0.6983\n",
      "Graph 94: Epoch: 048, Loss: -0.5233\n",
      "Graph 94: Epoch: 049, Loss: -0.4658\n",
      "Graph 94: Epoch: 050, Loss: -0.4655\n",
      "Graph 94: Epoch: 051, Loss: -0.5236\n",
      "Graph 94: Epoch: 052, Loss: -0.4660\n",
      "Graph 94: Epoch: 053, Loss: -0.5826\n",
      "Graph 94: Epoch: 054, Loss: -0.4662\n",
      "Graph 94: Epoch: 055, Loss: -0.4665\n",
      "Graph 94: Epoch: 056, Loss: -0.4662\n",
      "Graph 94: Epoch: 057, Loss: -0.3501\n",
      "Graph 94: Epoch: 058, Loss: -0.4666\n",
      "Graph 94: Epoch: 059, Loss: -0.4667\n",
      "Graph 94: Epoch: 060, Loss: -0.5835\n",
      "Graph 94: Epoch: 061, Loss: -0.3502\n",
      "Graph 94: Epoch: 062, Loss: -0.5835\n",
      "Graph 94: Epoch: 063, Loss: -0.6420\n",
      "Graph 94: Epoch: 064, Loss: -0.4670\n",
      "Graph 94: Epoch: 065, Loss: -0.4669\n",
      "Graph 94: Epoch: 066, Loss: -0.4672\n",
      "Graph 94: Epoch: 067, Loss: -0.4672\n",
      "Graph 94: Epoch: 068, Loss: -0.5257\n",
      "Graph 94: Epoch: 069, Loss: -0.3506\n",
      "Graph 94: Epoch: 070, Loss: -0.4086\n",
      "Graph 94: Epoch: 071, Loss: -0.4078\n",
      "Graph 94: Epoch: 072, Loss: -0.4674\n",
      "Graph 94: Epoch: 073, Loss: -0.3508\n",
      "Graph 94: Epoch: 074, Loss: -0.5843\n",
      "Graph 94: Epoch: 075, Loss: -0.7599\n",
      "Graph 94: Epoch: 076, Loss: -0.4673\n",
      "Graph 94: Epoch: 077, Loss: -0.5262\n",
      "Graph 94: Epoch: 078, Loss: -0.5257\n",
      "Graph 94: Epoch: 079, Loss: -0.7017\n",
      "Graph 94: Epoch: 080, Loss: -0.4093\n",
      "Graph 94: Epoch: 081, Loss: -0.3510\n",
      "Graph 94: Epoch: 082, Loss: -0.4095\n",
      "Graph 94: Epoch: 083, Loss: -0.5850\n",
      "Graph 94: Epoch: 084, Loss: -0.6435\n",
      "Graph 94: Epoch: 085, Loss: -0.6435\n",
      "Graph 94: Epoch: 086, Loss: -0.7020\n",
      "Graph 94: Epoch: 087, Loss: -0.5266\n",
      "Graph 94: Epoch: 088, Loss: -0.5849\n",
      "Graph 94: Epoch: 089, Loss: -0.6436\n",
      "Graph 94: Epoch: 090, Loss: -0.2927\n",
      "Graph 94: Epoch: 091, Loss: -0.5267\n",
      "Graph 94: Epoch: 092, Loss: -0.5268\n",
      "Graph 94: Epoch: 093, Loss: -0.2342\n",
      "Graph 94: Epoch: 094, Loss: -0.3513\n",
      "Graph 94: Epoch: 095, Loss: -0.1757\n",
      "Graph 94: Epoch: 096, Loss: -0.2928\n",
      "Graph 94: Epoch: 097, Loss: -0.4684\n",
      "Graph 94: Epoch: 098, Loss: -0.5854\n",
      "Graph 94: Epoch: 099, Loss: -0.3514\n",
      "Graph 94: Epoch: 100, Loss: -0.7612\n",
      "Graph 95: Epoch: 001, Loss: -0.0471\n",
      "Graph 95: Epoch: 002, Loss: -0.1507\n",
      "Graph 95: Epoch: 003, Loss: -0.3221\n",
      "Graph 95: Epoch: 004, Loss: -0.3812\n",
      "Graph 95: Epoch: 005, Loss: -0.3095\n",
      "Graph 95: Epoch: 006, Loss: -0.3616\n",
      "Graph 95: Epoch: 007, Loss: -0.3950\n",
      "Graph 95: Epoch: 008, Loss: -0.3109\n",
      "Graph 95: Epoch: 009, Loss: -0.3254\n",
      "Graph 95: Epoch: 010, Loss: -0.4079\n",
      "Graph 95: Epoch: 011, Loss: -0.5372\n",
      "Graph 95: Epoch: 012, Loss: -0.3466\n",
      "Graph 95: Epoch: 013, Loss: -0.4438\n",
      "Graph 95: Epoch: 014, Loss: -0.4045\n",
      "Graph 95: Epoch: 015, Loss: -0.5393\n",
      "Graph 95: Epoch: 016, Loss: -0.5340\n",
      "Graph 95: Epoch: 017, Loss: -0.6122\n",
      "Graph 95: Epoch: 018, Loss: -0.8014\n",
      "Graph 95: Epoch: 019, Loss: -0.6364\n",
      "Graph 95: Epoch: 020, Loss: -0.6466\n",
      "Graph 95: Epoch: 021, Loss: -0.6529\n",
      "Graph 95: Epoch: 022, Loss: -0.4694\n",
      "Graph 95: Epoch: 023, Loss: -0.5273\n",
      "Graph 95: Epoch: 024, Loss: -0.2885\n",
      "Graph 95: Epoch: 025, Loss: -0.6609\n",
      "Graph 95: Epoch: 026, Loss: -0.1921\n",
      "Graph 95: Epoch: 027, Loss: -0.8555\n",
      "Graph 95: Epoch: 028, Loss: -0.2873\n",
      "Graph 95: Epoch: 029, Loss: -0.4764\n",
      "Graph 95: Epoch: 030, Loss: -0.4593\n",
      "Graph 95: Epoch: 031, Loss: -0.6710\n",
      "Graph 95: Epoch: 032, Loss: -0.7659\n",
      "Graph 95: Epoch: 033, Loss: -0.5773\n",
      "Graph 95: Epoch: 034, Loss: -0.5743\n",
      "Graph 95: Epoch: 035, Loss: -0.6765\n",
      "Graph 95: Epoch: 036, Loss: -0.1949\n",
      "Graph 95: Epoch: 037, Loss: -0.3857\n",
      "Graph 95: Epoch: 038, Loss: -0.3889\n",
      "Graph 95: Epoch: 039, Loss: -0.3882\n",
      "Graph 95: Epoch: 040, Loss: -0.5829\n",
      "Graph 95: Epoch: 041, Loss: -0.6814\n",
      "Graph 95: Epoch: 042, Loss: -0.3856\n",
      "Graph 95: Epoch: 043, Loss: -0.4870\n",
      "Graph 95: Epoch: 044, Loss: -0.3897\n",
      "Graph 95: Epoch: 045, Loss: -0.4875\n",
      "Graph 95: Epoch: 046, Loss: -0.5862\n",
      "Graph 95: Epoch: 047, Loss: -0.6843\n",
      "Graph 95: Epoch: 048, Loss: -0.5816\n",
      "Graph 95: Epoch: 049, Loss: -0.3922\n",
      "Graph 95: Epoch: 050, Loss: -0.1964\n",
      "Graph 95: Epoch: 051, Loss: -0.4870\n",
      "Graph 95: Epoch: 052, Loss: -0.2941\n",
      "Graph 95: Epoch: 053, Loss: -0.5888\n",
      "Graph 95: Epoch: 054, Loss: -0.6866\n",
      "Graph 95: Epoch: 055, Loss: -0.3932\n",
      "Graph 95: Epoch: 056, Loss: -0.3932\n",
      "Graph 95: Epoch: 057, Loss: -0.2949\n",
      "Graph 95: Epoch: 058, Loss: -0.5884\n",
      "Graph 95: Epoch: 059, Loss: -0.7841\n",
      "Graph 95: Epoch: 060, Loss: -0.6842\n",
      "Graph 95: Epoch: 061, Loss: -0.3941\n",
      "Graph 95: Epoch: 062, Loss: -0.4922\n",
      "Graph 95: Epoch: 063, Loss: -0.7851\n",
      "Graph 95: Epoch: 064, Loss: -0.3943\n",
      "Graph 95: Epoch: 065, Loss: -0.4928\n",
      "Graph 95: Epoch: 066, Loss: -0.3945\n",
      "Graph 95: Epoch: 067, Loss: -0.3947\n",
      "Graph 95: Epoch: 068, Loss: -0.1979\n",
      "Graph 95: Epoch: 069, Loss: -0.5913\n",
      "Graph 95: Epoch: 070, Loss: -0.2964\n",
      "Graph 95: Epoch: 071, Loss: -0.5881\n",
      "Graph 95: Epoch: 072, Loss: -0.6902\n",
      "Graph 95: Epoch: 073, Loss: -0.3949\n",
      "Graph 95: Epoch: 074, Loss: -0.5902\n",
      "Graph 95: Epoch: 075, Loss: -0.3952\n",
      "Graph 95: Epoch: 076, Loss: -0.6905\n",
      "Graph 95: Epoch: 077, Loss: -0.5926\n",
      "Graph 95: Epoch: 078, Loss: -0.7899\n",
      "Graph 95: Epoch: 079, Loss: -0.1987\n",
      "Graph 95: Epoch: 080, Loss: -0.1983\n",
      "Graph 95: Epoch: 081, Loss: -0.4920\n",
      "Graph 95: Epoch: 082, Loss: -0.4944\n",
      "Graph 95: Epoch: 083, Loss: -0.6906\n",
      "Graph 95: Epoch: 084, Loss: -0.4947\n",
      "Graph 95: Epoch: 085, Loss: -0.4267\n",
      "Graph 95: Epoch: 086, Loss: -0.5932\n",
      "Graph 95: Epoch: 087, Loss: -0.4952\n",
      "Graph 95: Epoch: 088, Loss: -0.2970\n",
      "Graph 95: Epoch: 089, Loss: -0.6921\n",
      "Graph 95: Epoch: 090, Loss: -0.4121\n",
      "Graph 95: Epoch: 091, Loss: -0.3954\n",
      "Graph 95: Epoch: 092, Loss: -0.5927\n",
      "Graph 95: Epoch: 093, Loss: -0.4942\n",
      "Graph 95: Epoch: 094, Loss: -0.3965\n",
      "Graph 95: Epoch: 095, Loss: -0.1984\n",
      "Graph 95: Epoch: 096, Loss: -0.2305\n",
      "Graph 95: Epoch: 097, Loss: -0.4413\n",
      "Graph 95: Epoch: 098, Loss: -0.7913\n",
      "Graph 95: Epoch: 099, Loss: -0.2929\n",
      "Graph 95: Epoch: 100, Loss: -0.4935\n",
      "Graph 96: Epoch: 001, Loss: -0.0161\n",
      "Graph 96: Epoch: 002, Loss: -0.0780\n",
      "Graph 96: Epoch: 003, Loss: -0.0339\n",
      "Graph 96: Epoch: 004, Loss: -0.2271\n",
      "Graph 96: Epoch: 005, Loss: -0.1083\n",
      "Graph 96: Epoch: 006, Loss: -0.2115\n",
      "Graph 96: Epoch: 007, Loss: -0.1826\n",
      "Graph 96: Epoch: 008, Loss: -0.3274\n",
      "Graph 96: Epoch: 009, Loss: -0.3443\n",
      "Graph 96: Epoch: 010, Loss: -0.2171\n",
      "Graph 96: Epoch: 011, Loss: -0.3885\n",
      "Graph 96: Epoch: 012, Loss: -0.2943\n",
      "Graph 96: Epoch: 013, Loss: -0.3126\n",
      "Graph 96: Epoch: 014, Loss: -0.3061\n",
      "Graph 96: Epoch: 015, Loss: -0.4351\n",
      "Graph 96: Epoch: 016, Loss: -0.4720\n",
      "Graph 96: Epoch: 017, Loss: -0.1807\n",
      "Graph 96: Epoch: 018, Loss: -0.3728\n",
      "Graph 96: Epoch: 019, Loss: -0.4575\n",
      "Graph 96: Epoch: 020, Loss: -0.3775\n",
      "Graph 96: Epoch: 021, Loss: -0.2528\n",
      "Graph 96: Epoch: 022, Loss: -0.3764\n",
      "Graph 96: Epoch: 023, Loss: -0.4779\n",
      "Graph 96: Epoch: 024, Loss: -0.1641\n",
      "Graph 96: Epoch: 025, Loss: -0.4797\n",
      "Graph 96: Epoch: 026, Loss: -0.2787\n",
      "Graph 96: Epoch: 027, Loss: -0.5141\n",
      "Graph 96: Epoch: 028, Loss: -0.4025\n",
      "Graph 96: Epoch: 029, Loss: -0.8292\n",
      "Graph 96: Epoch: 030, Loss: -0.3192\n",
      "Graph 96: Epoch: 031, Loss: -0.5250\n",
      "Graph 96: Epoch: 032, Loss: -0.6522\n",
      "Graph 96: Epoch: 033, Loss: -0.4770\n",
      "Graph 96: Epoch: 034, Loss: -0.7924\n",
      "Graph 96: Epoch: 035, Loss: -0.6646\n",
      "Graph 96: Epoch: 036, Loss: -0.7977\n",
      "Graph 96: Epoch: 037, Loss: -0.8328\n",
      "Graph 96: Epoch: 038, Loss: -0.4258\n",
      "Graph 96: Epoch: 039, Loss: -0.4155\n",
      "Graph 96: Epoch: 040, Loss: -0.4491\n",
      "Graph 96: Epoch: 041, Loss: -0.4092\n",
      "Graph 96: Epoch: 042, Loss: -0.3356\n",
      "Graph 96: Epoch: 043, Loss: -0.2730\n",
      "Graph 96: Epoch: 044, Loss: -0.6867\n",
      "Graph 96: Epoch: 045, Loss: -0.1440\n",
      "Graph 96: Epoch: 046, Loss: -0.5453\n",
      "Graph 96: Epoch: 047, Loss: -0.3959\n",
      "Graph 96: Epoch: 048, Loss: -0.5502\n",
      "Graph 96: Epoch: 049, Loss: -0.4175\n",
      "Graph 96: Epoch: 050, Loss: -0.5804\n",
      "Graph 96: Epoch: 051, Loss: -0.4169\n",
      "Graph 96: Epoch: 052, Loss: -0.1785\n",
      "Graph 96: Epoch: 053, Loss: -0.1444\n",
      "Graph 96: Epoch: 054, Loss: -0.5862\n",
      "Graph 96: Epoch: 055, Loss: -0.5800\n",
      "Graph 96: Epoch: 056, Loss: -0.2855\n",
      "Graph 96: Epoch: 057, Loss: -0.2812\n",
      "Graph 96: Epoch: 058, Loss: -0.7987\n",
      "Graph 96: Epoch: 059, Loss: -0.3307\n",
      "Graph 96: Epoch: 060, Loss: -0.4498\n",
      "Graph 96: Epoch: 061, Loss: -0.0037\n",
      "Graph 96: Epoch: 062, Loss: -0.0702\n",
      "Graph 96: Epoch: 063, Loss: -0.5594\n",
      "Graph 96: Epoch: 064, Loss: -0.6978\n",
      "Graph 96: Epoch: 065, Loss: -0.8262\n",
      "Graph 96: Epoch: 066, Loss: -0.4153\n",
      "Graph 96: Epoch: 067, Loss: -0.5677\n",
      "Graph 96: Epoch: 068, Loss: -0.4259\n",
      "Graph 96: Epoch: 069, Loss: -0.5575\n",
      "Graph 96: Epoch: 070, Loss: -0.8326\n",
      "Graph 96: Epoch: 071, Loss: -0.4218\n",
      "Graph 96: Epoch: 072, Loss: -0.2801\n",
      "Graph 96: Epoch: 073, Loss: -0.3173\n",
      "Graph 96: Epoch: 074, Loss: -0.4656\n",
      "Graph 96: Epoch: 075, Loss: -0.4202\n",
      "Graph 96: Epoch: 076, Loss: -0.9771\n",
      "Graph 96: Epoch: 077, Loss: -0.4653\n",
      "Graph 96: Epoch: 078, Loss: -0.4211\n",
      "Graph 96: Epoch: 079, Loss: -0.4556\n",
      "Graph 96: Epoch: 080, Loss: -0.5579\n",
      "Graph 96: Epoch: 081, Loss: -0.4245\n",
      "Graph 96: Epoch: 082, Loss: -0.5428\n",
      "Graph 96: Epoch: 083, Loss: -0.4230\n",
      "Graph 96: Epoch: 084, Loss: -0.5624\n",
      "Graph 96: Epoch: 085, Loss: -0.4161\n",
      "Graph 96: Epoch: 086, Loss: -0.5624\n",
      "Graph 96: Epoch: 087, Loss: -0.1733\n",
      "Graph 96: Epoch: 088, Loss: -0.5637\n",
      "Graph 96: Epoch: 089, Loss: -0.4246\n",
      "Graph 96: Epoch: 090, Loss: -0.4813\n",
      "Graph 96: Epoch: 091, Loss: -0.2827\n",
      "Graph 96: Epoch: 092, Loss: -0.7045\n",
      "Graph 96: Epoch: 093, Loss: -0.4818\n",
      "Graph 96: Epoch: 094, Loss: -0.4235\n",
      "Graph 96: Epoch: 095, Loss: -0.4982\n",
      "Graph 96: Epoch: 096, Loss: -0.5554\n",
      "Graph 96: Epoch: 097, Loss: -0.2812\n",
      "Graph 96: Epoch: 098, Loss: -0.6568\n",
      "Graph 96: Epoch: 099, Loss: -0.5490\n",
      "Graph 96: Epoch: 100, Loss: -0.4250\n",
      "Graph 97: Epoch: 001, Loss: -0.5000\n",
      "Graph 97: Epoch: 002, Loss: -0.5050\n",
      "Graph 97: Epoch: 003, Loss: -0.4900\n",
      "Graph 97: Epoch: 004, Loss: -0.5113\n",
      "Graph 97: Epoch: 005, Loss: -0.8864\n",
      "Graph 97: Epoch: 006, Loss: -0.5162\n",
      "Graph 97: Epoch: 007, Loss: -0.5193\n",
      "Graph 97: Epoch: 008, Loss: -0.4772\n",
      "Graph 97: Epoch: 009, Loss: -0.4753\n",
      "Graph 97: Epoch: 010, Loss: -0.5255\n",
      "Graph 97: Epoch: 011, Loss: -0.4731\n",
      "Graph 97: Epoch: 012, Loss: -0.5273\n",
      "Graph 97: Epoch: 013, Loss: -0.5285\n",
      "Graph 97: Epoch: 014, Loss: -0.4699\n",
      "Graph 97: Epoch: 015, Loss: -0.5309\n",
      "Graph 97: Epoch: 016, Loss: -0.4678\n",
      "Graph 97: Epoch: 017, Loss: -0.5327\n",
      "Graph 97: Epoch: 018, Loss: -0.4662\n",
      "Graph 97: Epoch: 019, Loss: -0.4659\n",
      "Graph 97: Epoch: 020, Loss: -0.4662\n",
      "Graph 97: Epoch: 021, Loss: -0.4670\n",
      "Graph 97: Epoch: 022, Loss: -0.5317\n",
      "Graph 97: Epoch: 023, Loss: -0.4689\n",
      "Graph 97: Epoch: 024, Loss: -0.5300\n",
      "Graph 97: Epoch: 025, Loss: -0.4704\n",
      "Graph 97: Epoch: 026, Loss: -0.5287\n",
      "Graph 97: Epoch: 027, Loss: -0.4716\n",
      "Graph 97: Epoch: 028, Loss: -0.4724\n",
      "Graph 97: Epoch: 029, Loss: -0.0999\n",
      "Graph 97: Epoch: 030, Loss: -0.1009\n",
      "Graph 97: Epoch: 031, Loss: -0.5237\n",
      "Graph 97: Epoch: 032, Loss: -0.4770\n",
      "Graph 97: Epoch: 033, Loss: -0.5218\n",
      "Graph 97: Epoch: 034, Loss: -0.4787\n",
      "Graph 97: Epoch: 035, Loss: -0.8928\n",
      "Graph 97: Epoch: 036, Loss: -0.4804\n",
      "Graph 97: Epoch: 037, Loss: -0.4816\n",
      "Graph 97: Epoch: 038, Loss: -0.5169\n",
      "Graph 97: Epoch: 039, Loss: -0.5160\n",
      "Graph 97: Epoch: 040, Loss: -0.5157\n",
      "Graph 97: Epoch: 041, Loss: -0.4840\n",
      "Graph 97: Epoch: 042, Loss: -0.5158\n",
      "Graph 97: Epoch: 043, Loss: -0.5161\n",
      "Graph 97: Epoch: 044, Loss: -0.4832\n",
      "Graph 97: Epoch: 045, Loss: -0.4830\n",
      "Graph 97: Epoch: 046, Loss: -0.4833\n",
      "Graph 97: Epoch: 047, Loss: -0.4842\n",
      "Graph 97: Epoch: 048, Loss: -0.5145\n",
      "Graph 97: Epoch: 049, Loss: -0.5139\n",
      "Graph 97: Epoch: 050, Loss: -0.8902\n",
      "Graph 97: Epoch: 051, Loss: -0.4860\n",
      "Graph 97: Epoch: 052, Loss: -0.5137\n",
      "Graph 97: Epoch: 053, Loss: -0.4862\n",
      "Graph 97: Epoch: 054, Loss: -0.5135\n",
      "Graph 97: Epoch: 055, Loss: -0.5137\n",
      "Graph 97: Epoch: 056, Loss: -0.1066\n",
      "Graph 97: Epoch: 057, Loss: -0.1067\n",
      "Graph 97: Epoch: 058, Loss: -0.5150\n",
      "Graph 97: Epoch: 059, Loss: -0.4843\n",
      "Graph 97: Epoch: 060, Loss: -0.5158\n",
      "Graph 97: Epoch: 061, Loss: -0.5164\n",
      "Graph 97: Epoch: 062, Loss: -0.8911\n",
      "Graph 97: Epoch: 063, Loss: -0.4813\n",
      "Graph 97: Epoch: 064, Loss: -0.4807\n",
      "Graph 97: Epoch: 065, Loss: -0.4808\n",
      "Graph 97: Epoch: 066, Loss: -0.4813\n",
      "Graph 97: Epoch: 067, Loss: -0.4824\n",
      "Graph 97: Epoch: 068, Loss: -0.8911\n",
      "Graph 97: Epoch: 069, Loss: -0.5150\n",
      "Graph 97: Epoch: 070, Loss: -0.4854\n",
      "Graph 97: Epoch: 071, Loss: -0.1083\n",
      "Graph 97: Epoch: 072, Loss: -0.4875\n",
      "Graph 97: Epoch: 073, Loss: -0.5110\n",
      "Graph 97: Epoch: 074, Loss: -0.5102\n",
      "Graph 97: Epoch: 075, Loss: -0.8899\n",
      "Graph 97: Epoch: 076, Loss: -0.8904\n",
      "Graph 97: Epoch: 077, Loss: -0.4897\n",
      "Graph 97: Epoch: 078, Loss: -0.5100\n",
      "Graph 97: Epoch: 079, Loss: -0.8929\n",
      "Graph 97: Epoch: 080, Loss: -0.5106\n",
      "Graph 97: Epoch: 081, Loss: -0.4884\n",
      "Graph 97: Epoch: 082, Loss: -0.4882\n",
      "Graph 97: Epoch: 083, Loss: -0.4884\n",
      "Graph 97: Epoch: 084, Loss: -0.1025\n",
      "Graph 97: Epoch: 085, Loss: -0.5099\n",
      "Graph 97: Epoch: 086, Loss: -0.4904\n",
      "Graph 97: Epoch: 087, Loss: -0.1027\n",
      "Graph 97: Epoch: 088, Loss: -0.5079\n",
      "Graph 97: Epoch: 089, Loss: -0.5076\n",
      "Graph 97: Epoch: 090, Loss: -0.5079\n",
      "Graph 97: Epoch: 091, Loss: -0.5087\n",
      "Graph 97: Epoch: 092, Loss: -0.5100\n",
      "Graph 97: Epoch: 093, Loss: -0.4884\n",
      "Graph 97: Epoch: 094, Loss: -0.8971\n",
      "Graph 97: Epoch: 095, Loss: -0.4863\n",
      "Graph 97: Epoch: 096, Loss: -0.5141\n",
      "Graph 97: Epoch: 097, Loss: -0.4850\n",
      "Graph 97: Epoch: 098, Loss: -0.4847\n",
      "Graph 97: Epoch: 099, Loss: -0.4850\n",
      "Graph 97: Epoch: 100, Loss: -0.5142\n",
      "Graph 98: Epoch: 001, Loss: -0.0205\n",
      "Graph 98: Epoch: 002, Loss: -0.2007\n",
      "Graph 98: Epoch: 003, Loss: -0.3742\n",
      "Graph 98: Epoch: 004, Loss: -0.4304\n",
      "Graph 98: Epoch: 005, Loss: -0.4709\n",
      "Graph 98: Epoch: 006, Loss: -0.4055\n",
      "Graph 98: Epoch: 007, Loss: -0.2870\n",
      "Graph 98: Epoch: 008, Loss: -0.6808\n",
      "Graph 98: Epoch: 009, Loss: -0.3879\n",
      "Graph 98: Epoch: 010, Loss: -0.5153\n",
      "Graph 98: Epoch: 011, Loss: -0.6307\n",
      "Graph 98: Epoch: 012, Loss: -0.3480\n",
      "Graph 98: Epoch: 013, Loss: -0.4602\n",
      "Graph 98: Epoch: 014, Loss: -0.3890\n",
      "Graph 98: Epoch: 015, Loss: -0.4168\n",
      "Graph 98: Epoch: 016, Loss: -0.4694\n",
      "Graph 98: Epoch: 017, Loss: -0.3545\n",
      "Graph 98: Epoch: 018, Loss: -0.5394\n",
      "Graph 98: Epoch: 019, Loss: -0.3580\n",
      "Graph 98: Epoch: 020, Loss: -0.6329\n",
      "Graph 98: Epoch: 021, Loss: -0.7174\n",
      "Graph 98: Epoch: 022, Loss: -0.1822\n",
      "Graph 98: Epoch: 023, Loss: -0.5433\n",
      "Graph 98: Epoch: 024, Loss: -0.5462\n",
      "Graph 98: Epoch: 025, Loss: -0.4260\n",
      "Graph 98: Epoch: 026, Loss: -0.3659\n",
      "Graph 98: Epoch: 027, Loss: -0.4266\n",
      "Graph 98: Epoch: 028, Loss: -0.3621\n",
      "Graph 98: Epoch: 029, Loss: -0.6109\n",
      "Graph 98: Epoch: 030, Loss: -0.7331\n",
      "Graph 98: Epoch: 031, Loss: -0.3645\n",
      "Graph 98: Epoch: 032, Loss: -0.3667\n",
      "Graph 98: Epoch: 033, Loss: -0.4894\n",
      "Graph 98: Epoch: 034, Loss: -0.7937\n",
      "Graph 98: Epoch: 035, Loss: -0.4293\n",
      "Graph 98: Epoch: 036, Loss: -0.5513\n",
      "Graph 98: Epoch: 037, Loss: -0.6142\n",
      "Graph 98: Epoch: 038, Loss: -0.6132\n",
      "Graph 98: Epoch: 039, Loss: -0.3084\n",
      "Graph 98: Epoch: 040, Loss: -0.6141\n",
      "Graph 98: Epoch: 041, Loss: -0.4922\n",
      "Graph 98: Epoch: 042, Loss: -0.3699\n",
      "Graph 98: Epoch: 043, Loss: -0.3086\n",
      "Graph 98: Epoch: 044, Loss: -0.4314\n",
      "Graph 98: Epoch: 045, Loss: -0.5551\n",
      "Graph 98: Epoch: 046, Loss: -0.6169\n",
      "Graph 98: Epoch: 047, Loss: -0.6172\n",
      "Graph 98: Epoch: 048, Loss: -0.6785\n",
      "Graph 98: Epoch: 049, Loss: -0.5554\n",
      "Graph 98: Epoch: 050, Loss: -0.6178\n",
      "Graph 98: Epoch: 051, Loss: -0.4946\n",
      "Graph 98: Epoch: 052, Loss: -0.5549\n",
      "Graph 98: Epoch: 053, Loss: -0.5561\n",
      "Graph 98: Epoch: 054, Loss: -0.6799\n",
      "Graph 98: Epoch: 055, Loss: -0.4329\n",
      "Graph 98: Epoch: 056, Loss: -0.5567\n",
      "Graph 98: Epoch: 057, Loss: -0.7417\n",
      "Graph 98: Epoch: 058, Loss: -0.6181\n",
      "Graph 98: Epoch: 059, Loss: -0.6186\n",
      "Graph 98: Epoch: 060, Loss: -0.5569\n",
      "Graph 98: Epoch: 061, Loss: -0.4954\n",
      "Graph 98: Epoch: 062, Loss: -0.6812\n",
      "Graph 98: Epoch: 063, Loss: -0.4957\n",
      "Graph 98: Epoch: 064, Loss: -0.4329\n",
      "Graph 98: Epoch: 065, Loss: -0.4339\n",
      "Graph 98: Epoch: 066, Loss: -0.4959\n",
      "Graph 98: Epoch: 067, Loss: -0.6191\n",
      "Graph 98: Epoch: 068, Loss: -0.5578\n",
      "Graph 98: Epoch: 069, Loss: -0.4339\n",
      "Graph 98: Epoch: 070, Loss: -0.7438\n",
      "Graph 98: Epoch: 071, Loss: -0.1863\n",
      "Graph 98: Epoch: 072, Loss: -0.4960\n",
      "Graph 98: Epoch: 073, Loss: -0.4963\n",
      "Graph 98: Epoch: 074, Loss: -0.4962\n",
      "Graph 98: Epoch: 075, Loss: -0.4342\n",
      "Graph 98: Epoch: 076, Loss: -0.4964\n",
      "Graph 98: Epoch: 077, Loss: -0.2483\n",
      "Graph 98: Epoch: 078, Loss: -0.1864\n",
      "Graph 98: Epoch: 079, Loss: -0.5585\n",
      "Graph 98: Epoch: 080, Loss: -0.5585\n",
      "Graph 98: Epoch: 081, Loss: -0.4964\n",
      "Graph 98: Epoch: 082, Loss: -0.6208\n",
      "Graph 98: Epoch: 083, Loss: -0.2485\n",
      "Graph 98: Epoch: 084, Loss: -0.4967\n",
      "Graph 98: Epoch: 085, Loss: -0.6829\n",
      "Graph 98: Epoch: 086, Loss: -0.6209\n",
      "Graph 98: Epoch: 087, Loss: -0.3107\n",
      "Graph 98: Epoch: 088, Loss: -0.3727\n",
      "Graph 98: Epoch: 089, Loss: -0.4967\n",
      "Graph 98: Epoch: 090, Loss: -0.4345\n",
      "Graph 98: Epoch: 091, Loss: -0.7452\n",
      "Graph 98: Epoch: 092, Loss: -0.6211\n",
      "Graph 98: Epoch: 093, Loss: -0.6831\n",
      "Graph 98: Epoch: 094, Loss: -0.3728\n",
      "Graph 98: Epoch: 095, Loss: -0.4350\n",
      "Graph 98: Epoch: 096, Loss: -0.3729\n",
      "Graph 98: Epoch: 097, Loss: -0.4971\n",
      "Graph 98: Epoch: 098, Loss: -0.6832\n",
      "Graph 98: Epoch: 099, Loss: -0.5593\n",
      "Graph 98: Epoch: 100, Loss: -0.2488\n",
      "Graph 99: Epoch: 001, Loss: -0.0349\n",
      "Graph 99: Epoch: 002, Loss: -0.1442\n",
      "Graph 99: Epoch: 003, Loss: -0.2987\n",
      "Graph 99: Epoch: 004, Loss: -0.3239\n",
      "Graph 99: Epoch: 005, Loss: -0.3556\n",
      "Graph 99: Epoch: 006, Loss: -0.3442\n",
      "Graph 99: Epoch: 007, Loss: -0.3957\n",
      "Graph 99: Epoch: 008, Loss: -0.3449\n",
      "Graph 99: Epoch: 009, Loss: -0.3955\n",
      "Graph 99: Epoch: 010, Loss: -0.4004\n",
      "Graph 99: Epoch: 011, Loss: -0.3901\n",
      "Graph 99: Epoch: 012, Loss: -0.4209\n",
      "Graph 99: Epoch: 013, Loss: -0.3791\n",
      "Graph 99: Epoch: 014, Loss: -0.6157\n",
      "Graph 99: Epoch: 015, Loss: -0.5059\n",
      "Graph 99: Epoch: 016, Loss: -0.4758\n",
      "Graph 99: Epoch: 017, Loss: -0.5070\n",
      "Graph 99: Epoch: 018, Loss: -0.5867\n",
      "Graph 99: Epoch: 019, Loss: -0.4606\n",
      "Graph 99: Epoch: 020, Loss: -0.4730\n",
      "Graph 99: Epoch: 021, Loss: -0.5391\n",
      "Graph 99: Epoch: 022, Loss: -0.5445\n",
      "Graph 99: Epoch: 023, Loss: -0.4079\n",
      "Graph 99: Epoch: 024, Loss: -0.6838\n",
      "Graph 99: Epoch: 025, Loss: -0.4827\n",
      "Graph 99: Epoch: 026, Loss: -0.6221\n",
      "Graph 99: Epoch: 027, Loss: -0.6214\n",
      "Graph 99: Epoch: 028, Loss: -0.6938\n",
      "Graph 99: Epoch: 029, Loss: -0.8339\n",
      "Graph 99: Epoch: 030, Loss: -0.6973\n",
      "Graph 99: Epoch: 031, Loss: -0.6978\n",
      "Graph 99: Epoch: 032, Loss: -0.5591\n",
      "Graph 99: Epoch: 033, Loss: -0.4206\n",
      "Graph 99: Epoch: 034, Loss: -0.4198\n",
      "Graph 99: Epoch: 035, Loss: -0.3507\n",
      "Graph 99: Epoch: 036, Loss: -0.3509\n",
      "Graph 99: Epoch: 037, Loss: -0.3509\n",
      "Graph 99: Epoch: 038, Loss: -0.6312\n",
      "Graph 99: Epoch: 039, Loss: -0.4216\n",
      "Graph 99: Epoch: 040, Loss: -0.2815\n",
      "Graph 99: Epoch: 041, Loss: -0.4915\n",
      "Graph 99: Epoch: 042, Loss: -0.2817\n",
      "Graph 99: Epoch: 043, Loss: -0.5625\n",
      "Graph 99: Epoch: 044, Loss: -0.4926\n",
      "Graph 99: Epoch: 045, Loss: -0.4224\n",
      "Graph 99: Epoch: 046, Loss: -0.4930\n",
      "Graph 99: Epoch: 047, Loss: -0.7040\n",
      "Graph 99: Epoch: 048, Loss: -0.4929\n",
      "Graph 99: Epoch: 049, Loss: -0.4229\n",
      "Graph 99: Epoch: 050, Loss: -0.4232\n",
      "Graph 99: Epoch: 051, Loss: -0.6339\n",
      "Graph 99: Epoch: 052, Loss: -0.6346\n",
      "Graph 99: Epoch: 053, Loss: -0.5644\n",
      "Graph 99: Epoch: 054, Loss: -0.2118\n",
      "Graph 99: Epoch: 055, Loss: -0.5647\n",
      "Graph 99: Epoch: 056, Loss: -0.2825\n",
      "Graph 99: Epoch: 057, Loss: -0.5649\n",
      "Graph 99: Epoch: 058, Loss: -0.4233\n",
      "Graph 99: Epoch: 059, Loss: -0.6354\n",
      "Graph 99: Epoch: 060, Loss: -0.2826\n",
      "Graph 99: Epoch: 061, Loss: -0.4241\n",
      "Graph 99: Epoch: 062, Loss: -0.4945\n",
      "Graph 99: Epoch: 063, Loss: -0.5656\n",
      "Graph 99: Epoch: 064, Loss: -0.2124\n",
      "Graph 99: Epoch: 065, Loss: -0.4951\n",
      "Graph 99: Epoch: 066, Loss: -0.4951\n",
      "Graph 99: Epoch: 067, Loss: -0.5660\n",
      "Graph 99: Epoch: 068, Loss: -0.5649\n",
      "Graph 99: Epoch: 069, Loss: -0.4954\n",
      "Graph 99: Epoch: 070, Loss: -0.4248\n",
      "Graph 99: Epoch: 071, Loss: -0.7785\n",
      "Graph 99: Epoch: 072, Loss: -0.3540\n",
      "Graph 99: Epoch: 073, Loss: -0.5661\n",
      "Graph 99: Epoch: 074, Loss: -0.7786\n",
      "Graph 99: Epoch: 075, Loss: -0.6362\n",
      "Graph 99: Epoch: 076, Loss: -0.4958\n",
      "Graph 99: Epoch: 077, Loss: -0.4958\n",
      "Graph 99: Epoch: 078, Loss: -0.5648\n",
      "Graph 99: Epoch: 079, Loss: -0.5664\n",
      "Graph 99: Epoch: 080, Loss: -0.4250\n",
      "Graph 99: Epoch: 081, Loss: -0.5668\n",
      "Graph 99: Epoch: 082, Loss: -0.4958\n",
      "Graph 99: Epoch: 083, Loss: -0.4961\n",
      "Graph 99: Epoch: 084, Loss: -0.3544\n",
      "Graph 99: Epoch: 085, Loss: -0.6377\n",
      "Graph 99: Epoch: 086, Loss: -0.4963\n",
      "Graph 99: Epoch: 087, Loss: -0.5670\n",
      "Graph 99: Epoch: 088, Loss: -0.4964\n",
      "Graph 99: Epoch: 089, Loss: -0.5668\n",
      "Graph 99: Epoch: 090, Loss: -0.7087\n",
      "Graph 99: Epoch: 091, Loss: -0.5675\n",
      "Graph 99: Epoch: 092, Loss: -0.7091\n",
      "Graph 99: Epoch: 093, Loss: -0.3547\n",
      "Graph 99: Epoch: 094, Loss: -0.7091\n",
      "Graph 99: Epoch: 095, Loss: -0.4256\n",
      "Graph 99: Epoch: 096, Loss: -0.4967\n",
      "Graph 99: Epoch: 097, Loss: -0.4259\n",
      "Graph 99: Epoch: 098, Loss: -0.4967\n",
      "Graph 99: Epoch: 099, Loss: -0.6386\n",
      "Graph 99: Epoch: 100, Loss: -0.4262\n",
      "Graph 100: Epoch: 001, Loss: -0.0269\n",
      "Graph 100: Epoch: 002, Loss: -0.1298\n",
      "Graph 100: Epoch: 003, Loss: -0.2827\n",
      "Graph 100: Epoch: 004, Loss: -0.3727\n",
      "Graph 100: Epoch: 005, Loss: -0.4666\n",
      "Graph 100: Epoch: 006, Loss: -0.3938\n",
      "Graph 100: Epoch: 007, Loss: -0.4613\n",
      "Graph 100: Epoch: 008, Loss: -0.4909\n",
      "Graph 100: Epoch: 009, Loss: -0.4521\n",
      "Graph 100: Epoch: 010, Loss: -0.3859\n",
      "Graph 100: Epoch: 011, Loss: -0.5567\n",
      "Graph 100: Epoch: 012, Loss: -0.4680\n",
      "Graph 100: Epoch: 013, Loss: -0.2670\n",
      "Graph 100: Epoch: 014, Loss: -0.5234\n",
      "Graph 100: Epoch: 015, Loss: -0.7457\n",
      "Graph 100: Epoch: 016, Loss: -0.7999\n",
      "Graph 100: Epoch: 017, Loss: -0.3221\n",
      "Graph 100: Epoch: 018, Loss: -0.4842\n",
      "Graph 100: Epoch: 019, Loss: -0.4318\n",
      "Graph 100: Epoch: 020, Loss: -0.5405\n",
      "Graph 100: Epoch: 021, Loss: -0.5951\n",
      "Graph 100: Epoch: 022, Loss: -0.4880\n",
      "Graph 100: Epoch: 023, Loss: -0.7572\n",
      "Graph 100: Epoch: 024, Loss: -0.2714\n",
      "Graph 100: Epoch: 025, Loss: -0.5427\n",
      "Graph 100: Epoch: 026, Loss: -0.5997\n",
      "Graph 100: Epoch: 027, Loss: -0.2183\n",
      "Graph 100: Epoch: 028, Loss: -0.2728\n",
      "Graph 100: Epoch: 029, Loss: -0.4368\n",
      "Graph 100: Epoch: 030, Loss: -0.4917\n",
      "Graph 100: Epoch: 031, Loss: -0.4919\n",
      "Graph 100: Epoch: 032, Loss: -0.4374\n",
      "Graph 100: Epoch: 033, Loss: -0.7111\n",
      "Graph 100: Epoch: 034, Loss: -0.4380\n",
      "Graph 100: Epoch: 035, Loss: -0.5476\n",
      "Graph 100: Epoch: 036, Loss: -0.4933\n",
      "Graph 100: Epoch: 037, Loss: -0.2194\n",
      "Graph 100: Epoch: 038, Loss: -0.6033\n",
      "Graph 100: Epoch: 039, Loss: -0.5486\n",
      "Graph 100: Epoch: 040, Loss: -0.3293\n",
      "Graph 100: Epoch: 041, Loss: -0.2746\n",
      "Graph 100: Epoch: 042, Loss: -0.4944\n",
      "Graph 100: Epoch: 043, Loss: -0.3296\n",
      "Graph 100: Epoch: 044, Loss: -0.6045\n",
      "Graph 100: Epoch: 045, Loss: -0.3850\n",
      "Graph 100: Epoch: 046, Loss: -0.4399\n",
      "Graph 100: Epoch: 047, Loss: -0.5497\n",
      "Graph 100: Epoch: 048, Loss: -0.5500\n",
      "Graph 100: Epoch: 049, Loss: -0.3303\n",
      "Graph 100: Epoch: 050, Loss: -0.3304\n",
      "Graph 100: Epoch: 051, Loss: -0.5505\n",
      "Graph 100: Epoch: 052, Loss: -0.4406\n",
      "Graph 100: Epoch: 053, Loss: -0.6055\n",
      "Graph 100: Epoch: 054, Loss: -0.5507\n",
      "Graph 100: Epoch: 055, Loss: -0.7711\n",
      "Graph 100: Epoch: 056, Loss: -0.8261\n",
      "Graph 100: Epoch: 057, Loss: -0.4961\n",
      "Graph 100: Epoch: 058, Loss: -0.4409\n",
      "Graph 100: Epoch: 059, Loss: -0.6615\n",
      "Graph 100: Epoch: 060, Loss: -0.4411\n",
      "Graph 100: Epoch: 061, Loss: -0.3307\n",
      "Graph 100: Epoch: 062, Loss: -0.4412\n",
      "Graph 100: Epoch: 063, Loss: -0.5516\n",
      "Graph 100: Epoch: 064, Loss: -0.6068\n",
      "Graph 100: Epoch: 065, Loss: -0.6067\n",
      "Graph 100: Epoch: 066, Loss: -0.3862\n",
      "Graph 100: Epoch: 067, Loss: -0.4967\n",
      "Graph 100: Epoch: 068, Loss: -0.4967\n",
      "Graph 100: Epoch: 069, Loss: -0.4968\n",
      "Graph 100: Epoch: 070, Loss: -0.4417\n",
      "Graph 100: Epoch: 071, Loss: -0.4417\n",
      "Graph 100: Epoch: 072, Loss: -0.4968\n",
      "Graph 100: Epoch: 073, Loss: -0.4970\n",
      "Graph 100: Epoch: 074, Loss: -0.6074\n",
      "Graph 100: Epoch: 075, Loss: -0.5520\n",
      "Graph 100: Epoch: 076, Loss: -0.4419\n",
      "Graph 100: Epoch: 077, Loss: -0.5523\n",
      "Graph 100: Epoch: 078, Loss: -0.3867\n",
      "Graph 100: Epoch: 079, Loss: -0.4419\n",
      "Graph 100: Epoch: 080, Loss: -0.6077\n",
      "Graph 100: Epoch: 081, Loss: -0.4421\n",
      "Graph 100: Epoch: 082, Loss: -0.4973\n",
      "Graph 100: Epoch: 083, Loss: -0.6079\n",
      "Graph 100: Epoch: 084, Loss: -0.7736\n",
      "Graph 100: Epoch: 085, Loss: -0.4422\n",
      "Graph 100: Epoch: 086, Loss: -0.4975\n",
      "Graph 100: Epoch: 087, Loss: -0.6080\n",
      "Graph 100: Epoch: 088, Loss: -0.4423\n",
      "Graph 100: Epoch: 089, Loss: -0.5526\n",
      "Graph 100: Epoch: 090, Loss: -0.4423\n",
      "Graph 100: Epoch: 091, Loss: -0.6634\n",
      "Graph 100: Epoch: 092, Loss: -0.3871\n",
      "Graph 100: Epoch: 093, Loss: -0.6082\n",
      "Graph 100: Epoch: 094, Loss: -0.4423\n",
      "Graph 100: Epoch: 095, Loss: -0.4424\n",
      "Graph 100: Epoch: 096, Loss: -0.5530\n",
      "Graph 100: Epoch: 097, Loss: -0.5530\n",
      "Graph 100: Epoch: 098, Loss: -0.4978\n",
      "Graph 100: Epoch: 099, Loss: -0.6084\n",
      "Graph 100: Epoch: 100, Loss: -0.4425\n"
     ]
    }
   ],
   "source": [
    "# 그래프 데이터로 모델 학습\n",
    "for graph_idx, (graph, x, adj_tensor) in enumerate(graphs):\n",
    "    # Initialize the GAT model for the current graph\n",
    "    in_features = x.shape[1]\n",
    "    n_heads = adj_tensor.shape[1]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = 2 * n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).to(device)\n",
    "\n",
    "    # Set the optimizer and loss function\n",
    "    optimizer = optim.Adam(gat_model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.NLLLoss().to(device)\n",
    "\n",
    "    # Move the feature matrix and adjacency tensor to the GPU\n",
    "    x = x.to(device)\n",
    "    adj_tensor = adj_tensor.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = gat_model(x, adj_tensor)\n",
    "\n",
    "        # Generate random labels for the current graph\n",
    "        num_nodes = x.shape[0]\n",
    "        labels = torch.tensor([random.randint(0, 1) for _ in range(num_nodes)]).to(device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Graph {}: Epoch: {:03d}, Loss: {:.4f}\".format(graph_idx+1, epoch+1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
