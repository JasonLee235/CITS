{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "from graphviz import Graph\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        adj = adj.repeat(1, 1, self.n_heads)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            attn_res = attn_res.mean(dim=1)\n",
    "            return attn_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.phi1 = torch.nn.Linear(d_h, 1)\n",
    "        self.phi2 = torch.nn.Linear(d_h, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.C = torch.nn.Parameter(torch.randn(1)) # constant C\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_i, v_j):\n",
    "        v_i = v_i.unsqueeze(0)\n",
    "        phi1_v_i = torch.matmul(v_i, self.phi1.state_dict()['weight']) # phi1_v_i 의 사이즈 ()\n",
    "        print(\"phi_v_i.size()\")\n",
    "        print(phi1_v_i.size())\n",
    "\n",
    "        phi2_v_j = torch.matmul(v_j, self.phi2.state_dict()['weight']) # phi2_neighbors 의 사이즈 ()\n",
    "        print(\"phi_v_j.size()\")\n",
    "        print(phi2_v_j.size())\n",
    "        \n",
    "        attn_input = torch.matmul(phi1_v_i, phi2_v_j.transpose(0,1)) / (self.d_h ** 0.5) # (1,n) 의 크기를 갖는 attn_input\n",
    "        # attn_input = attn_input.squeeze(0)  # Remove the extra dimension\n",
    "        attn_input = attn_input\n",
    "\n",
    "        attn_output = self.C * self.activation(attn_input)\n",
    "        print(\"attn_output.size\")\n",
    "        print(attn_output.size())\n",
    "        # v_j의 크기를 (n, 1)로 변형하여 크기를 맞춤\n",
    "        # v_j = v_j.unsqueeze(1)\n",
    "        # masked_attn_output = attn_output.masked_fill(v_j == 0, float('-inf'))\n",
    "        # masked_attn_output = masked_attn_output.squeeze(1)\n",
    "        masked_attn_output = torch.where(v_j == 0, float('-inf'), attn_output)\n",
    "        print(\"masked_attn_\")\n",
    "        print(masked_attn_output.size())\n",
    "\n",
    "        masked_attn_output= masked_attn_output[0]\n",
    "        masked_attn_output = masked_attn_output.unsqueeze(0)\n",
    "        print(\"masked_attn_1\")\n",
    "        print(masked_attn_output.size())\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "        # attn_weights = self.softmax(attn_output)        \n",
    "        print(\"attn_weights.size\")\n",
    "        print(attn_weights.size())\n",
    "\n",
    "\n",
    "        output = torch.matmul(attn_weights, x)\n",
    "        print(\"output_size\")\n",
    "        print(output.size())\n",
    "        output = output.squeeze(0)\n",
    "        print(\"output_size1\")\n",
    "        print(output.size())\n",
    "        return permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads, is_concat = True, dropout = dropout)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, 1, is_concat = False, dropout = dropout)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(in_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        output = F.softmax(x, dim=0)\n",
    "        output = torch.mean(output, dim=1)\n",
    "        output = output.unsqueeze(0)\n",
    "        output = output.transpose(0,1)\n",
    "        return output\n",
    "    \n",
    "    def decode(self, output, v_i, v_j):\n",
    "        return self.decoder(output, v_i, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features =  1\n",
    "n_heads = 4\n",
    "\n",
    "def generate_random_weighted_graph(num_nodes, num_edges, max_weight=10):\n",
    "    # 방향 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 노드 추가\n",
    "    nodes = range(num_nodes)\n",
    "    graph.add_nodes_from(nodes)\n",
    "    \n",
    "    # 노드에 가중치 할당 및 노드 특징 벡터 생성\n",
    "    x = torch.zeros(num_nodes, in_features)\n",
    "    for node in graph.nodes:\n",
    "        weight = random.randint(1, max_weight)\n",
    "        graph.nodes[node]['weight'] = weight\n",
    "        x[node] = weight\n",
    "\n",
    "    # 간선 추가\n",
    "    edges = []\n",
    "    for i in range(num_edges):\n",
    "        # 임의의 출발 노드와 도착 노드 선택\n",
    "        source = random.choice(nodes)\n",
    "        target = random.choice(nodes)\n",
    "        \n",
    "        # 출발 노드와 도착 노드가 같은 경우 건너뜀\n",
    "        if source == target:\n",
    "            continue\n",
    "        \n",
    "        # 간선 추가\n",
    "        edges.append((source, target))\n",
    "\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    graph_original = graph\n",
    "\n",
    "        # Generate v_prev tensor\n",
    "    j = random.randint(0, num_nodes-1)\n",
    "\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    adj_matrix_original = torch.Tensor(adj_matrix.todense())\n",
    "    \n",
    "    adj_matrix = adj_matrix + sp.eye(adj_matrix.shape[0]) # Add self-loop\n",
    "    adj_tensor = torch.Tensor(adj_matrix.todense())\n",
    "\n",
    "    adj_tensor = adj_tensor.unsqueeze(2) # adj_tensor (num_nodes, num_nodes, n_heads)\n",
    "    # adj_tensor = adj_tensor.repeat(1, 1, n_heads) #\n",
    "    \n",
    "    return graph, x, adj_tensor, adj_matrix_original, graph_original, j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 100\n",
    "output_file = 'random_undirected_graphs.pkl'\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for _ in range(num_graphs):\n",
    "    num_nodes, num_edges, max_weight = np.random.randint(1,20), np.random.randint(1,30), np.random.randint(1,30)\n",
    "    graph, x, adj_tensor, adj_matrix_original, graph_original, j= generate_random_weighted_graph(num_nodes, num_edges, max_weight)\n",
    "    graphs.append((x, adj_tensor, j, adj_matrix_original))\n",
    "\n",
    "\n",
    "# 그래프를 pickle 파일로 저장\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일에서 그래프 데이터 로드\n",
    "with open('random_undirected_graphs.pkl', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "torch.Size([4, 1])\n",
      "tensor([[0.2426],\n",
      "        [0.2580],\n",
      "        [0.2497],\n",
      "        [0.2497]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.2426, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "\n",
      "v_j\n",
      "tensor([[0.2426],\n",
      "        [0.2580],\n",
      "        [0.2497],\n",
      "        [0.2497]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 1 - Decode Output:\n",
      "tensor([0.2500], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 2 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3149],\n",
      "        [0.3165],\n",
      "        [0.3686]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3149, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.3149],\n",
      "        [0.3165],\n",
      "        [0.3686]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 2 - Decode Output:\n",
      "tensor([0.3333], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3333, 0.3333, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 3 - Output:\n",
      "torch.Size([4, 1])\n",
      "tensor([[0.2892],\n",
      "        [0.2376],\n",
      "        [0.2353],\n",
      "        [0.2379]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.2353, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "\n",
      "v_j\n",
      "tensor([[0.2892],\n",
      "        [0.2376],\n",
      "        [0.2353],\n",
      "        [0.2379]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 3 - Decode Output:\n",
      "tensor([0.2500], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 4 - Output:\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.5000],\n",
      "        [0.5000]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.5000, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 4 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 5 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1387],\n",
      "        [0.1071],\n",
      "        [0.1071],\n",
      "        [0.1068],\n",
      "        [0.1094],\n",
      "        [0.1071],\n",
      "        [0.1075],\n",
      "        [0.1062],\n",
      "        [0.1100]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1068, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1387],\n",
      "        [0.1071],\n",
      "        [0.1071],\n",
      "        [0.1068],\n",
      "        [0.1094],\n",
      "        [0.1071],\n",
      "        [0.1075],\n",
      "        [0.1062],\n",
      "        [0.1100]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 5 - Decode Output:\n",
      "tensor([0.1111], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 6 - Output:\n",
      "torch.Size([14, 1])\n",
      "tensor([[0.0724],\n",
      "        [0.0710],\n",
      "        [0.0715],\n",
      "        [0.0710],\n",
      "        [0.0714],\n",
      "        [0.0709],\n",
      "        [0.0710],\n",
      "        [0.0712],\n",
      "        [0.0718],\n",
      "        [0.0711],\n",
      "        [0.0722],\n",
      "        [0.0715],\n",
      "        [0.0713],\n",
      "        [0.0715]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0715, device='cuda:0', grad_fn=<SelectBackward0>) 13\n",
      "\n",
      "torch.Size([14, 14])\n",
      "torch.Size([14, 14])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 6 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 7 - Output:\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.4779],\n",
      "        [0.5221]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.5221, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 7 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 8 - Output:\n",
      "torch.Size([6, 1])\n",
      "tensor([[0.1653],\n",
      "        [0.1653],\n",
      "        [0.1693],\n",
      "        [0.1683],\n",
      "        [0.1657],\n",
      "        [0.1661]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1653, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([6, 16])\n",
      "attn_output.size\n",
      "torch.Size([6])\n",
      "masked_attn_\n",
      "torch.Size([6, 6])\n",
      "masked_attn_1\n",
      "torch.Size([1, 6])\n",
      "attn_weights.size\n",
      "torch.Size([1, 6])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 8 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 9 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0524],\n",
      "        [0.0525],\n",
      "        [0.0536],\n",
      "        [0.0523],\n",
      "        [0.0530],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0525],\n",
      "        [0.0523],\n",
      "        [0.0535],\n",
      "        [0.0532],\n",
      "        [0.0525],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0526],\n",
      "        [0.0530],\n",
      "        [0.0524],\n",
      "        [0.0523]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0536, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 9 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 10 - Output:\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.5000],\n",
      "        [0.5000]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.5000, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 10 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 11 - Output:\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.1944],\n",
      "        [0.2190],\n",
      "        [0.1936],\n",
      "        [0.1981],\n",
      "        [0.1949]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1949, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 11 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 12 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0526],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0529],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0528],\n",
      "        [0.0526],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0532],\n",
      "        [0.0529]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 12 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 13 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3519],\n",
      "        [0.3241],\n",
      "        [0.3241]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3241, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.3519],\n",
      "        [0.3241],\n",
      "        [0.3241]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 13 - Decode Output:\n",
      "tensor([0.3333], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3334, 0.3333, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 14 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.0946],\n",
      "        [0.0960],\n",
      "        [0.0961],\n",
      "        [0.1334],\n",
      "        [0.0941],\n",
      "        [0.0976],\n",
      "        [0.0982],\n",
      "        [0.0942],\n",
      "        [0.0951],\n",
      "        [0.1005]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0960, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 14 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 15 - Output:\n",
      "torch.Size([16, 1])\n",
      "tensor([[0.0622],\n",
      "        [0.0622],\n",
      "        [0.0628],\n",
      "        [0.0623],\n",
      "        [0.0621],\n",
      "        [0.0642],\n",
      "        [0.0622],\n",
      "        [0.0622],\n",
      "        [0.0636],\n",
      "        [0.0621],\n",
      "        [0.0624],\n",
      "        [0.0622],\n",
      "        [0.0628],\n",
      "        [0.0621],\n",
      "        [0.0624],\n",
      "        [0.0622]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0623, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 16])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 15 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 16 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3168],\n",
      "        [0.3168],\n",
      "        [0.3665]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3665, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 16 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 17 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0526],\n",
      "        [0.0538],\n",
      "        [0.0528],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0525],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0524],\n",
      "        [0.0528],\n",
      "        [0.0525],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0530],\n",
      "        [0.0524],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0524]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0524, device='cuda:0', grad_fn=<SelectBackward0>) 15\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 17 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 18 - Output:\n",
      "torch.Size([14, 1])\n",
      "tensor([[0.0723],\n",
      "        [0.0710],\n",
      "        [0.0710],\n",
      "        [0.0714],\n",
      "        [0.0729],\n",
      "        [0.0722],\n",
      "        [0.0722],\n",
      "        [0.0708],\n",
      "        [0.0709],\n",
      "        [0.0712],\n",
      "        [0.0709],\n",
      "        [0.0708],\n",
      "        [0.0709],\n",
      "        [0.0714]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0729, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([14, 14])\n",
      "torch.Size([14, 14])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 18 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 19 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3706],\n",
      "        [0.3147],\n",
      "        [0.3147]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3147, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.3706],\n",
      "        [0.3147],\n",
      "        [0.3147]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 19 - Decode Output:\n",
      "tensor([0.3333], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3334, 0.3333, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 20 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0840],\n",
      "        [0.0863],\n",
      "        [0.0822],\n",
      "        [0.0839],\n",
      "        [0.0842],\n",
      "        [0.0820],\n",
      "        [0.0841],\n",
      "        [0.0820],\n",
      "        [0.0832],\n",
      "        [0.0835],\n",
      "        [0.0821],\n",
      "        [0.0824]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0822, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 20 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 21 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0551],\n",
      "        [0.0552],\n",
      "        [0.0555],\n",
      "        [0.0578],\n",
      "        [0.0554],\n",
      "        [0.0554],\n",
      "        [0.0575],\n",
      "        [0.0552],\n",
      "        [0.0553],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0552],\n",
      "        [0.0550],\n",
      "        [0.0550],\n",
      "        [0.0559],\n",
      "        [0.0552],\n",
      "        [0.0555],\n",
      "        [0.0556]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0552, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 21 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 22 - Output:\n",
      "torch.Size([1, 1])\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "\n",
      "v_j\n",
      "tensor([[0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 22 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 23 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3176],\n",
      "        [0.3649],\n",
      "        [0.3176]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3649, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.3176],\n",
      "        [0.3649],\n",
      "        [0.3176]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 23 - Decode Output:\n",
      "tensor([0.3333], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3333, 0.3334, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 24 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0526],\n",
      "        [0.0528],\n",
      "        [0.0523],\n",
      "        [0.0530],\n",
      "        [0.0524],\n",
      "        [0.0526],\n",
      "        [0.0523],\n",
      "        [0.0538],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0530],\n",
      "        [0.0524],\n",
      "        [0.0534],\n",
      "        [0.0525],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0523]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0523, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 24 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 25 - Output:\n",
      "torch.Size([8, 1])\n",
      "tensor([[0.1188],\n",
      "        [0.1055],\n",
      "        [0.1602],\n",
      "        [0.1145],\n",
      "        [0.1054],\n",
      "        [0.1679],\n",
      "        [0.1203],\n",
      "        [0.1074]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1679, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1188],\n",
      "        [0.1055],\n",
      "        [0.1602],\n",
      "        [0.1145],\n",
      "        [0.1054],\n",
      "        [0.1679],\n",
      "        [0.1203],\n",
      "        [0.1074]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 25 - Decode Output:\n",
      "tensor([0.1250], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 26 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1307],\n",
      "        [0.1422],\n",
      "        [0.1318],\n",
      "        [0.1405],\n",
      "        [0.1493],\n",
      "        [0.1614],\n",
      "        [0.1440]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1307, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 26 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 27 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0796],\n",
      "        [0.0803],\n",
      "        [0.0798],\n",
      "        [0.0795],\n",
      "        [0.0830],\n",
      "        [0.0845],\n",
      "        [0.0835],\n",
      "        [0.0797],\n",
      "        [0.0808],\n",
      "        [0.0809],\n",
      "        [0.0794],\n",
      "        [0.1090]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0794, device='cuda:0', grad_fn=<SelectBackward0>) 10\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 27 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 28 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0831],\n",
      "        [0.0829],\n",
      "        [0.0825],\n",
      "        [0.0828],\n",
      "        [0.0827],\n",
      "        [0.0825],\n",
      "        [0.0895],\n",
      "        [0.0831],\n",
      "        [0.0824],\n",
      "        [0.0828],\n",
      "        [0.0824],\n",
      "        [0.0832]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0827, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 28 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 29 - Output:\n",
      "torch.Size([13, 1])\n",
      "tensor([[0.0756],\n",
      "        [0.0757],\n",
      "        [0.0756],\n",
      "        [0.0759],\n",
      "        [0.0760],\n",
      "        [0.0889],\n",
      "        [0.0757],\n",
      "        [0.0765],\n",
      "        [0.0760],\n",
      "        [0.0759],\n",
      "        [0.0760],\n",
      "        [0.0756],\n",
      "        [0.0765]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0757, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([13, 13])\n",
      "torch.Size([13, 13])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 29 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 30 - Output:\n",
      "torch.Size([8, 1])\n",
      "tensor([[0.1178],\n",
      "        [0.1065],\n",
      "        [0.1064],\n",
      "        [0.1543],\n",
      "        [0.1392],\n",
      "        [0.1351],\n",
      "        [0.1301],\n",
      "        [0.1106]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1543, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1178],\n",
      "        [0.1065],\n",
      "        [0.1064],\n",
      "        [0.1543],\n",
      "        [0.1392],\n",
      "        [0.1351],\n",
      "        [0.1301],\n",
      "        [0.1106]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 30 - Decode Output:\n",
      "tensor([0.1250], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 31 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.0956],\n",
      "        [0.1089],\n",
      "        [0.1178],\n",
      "        [0.1056],\n",
      "        [0.0913],\n",
      "        [0.1114],\n",
      "        [0.0937],\n",
      "        [0.0911],\n",
      "        [0.0921],\n",
      "        [0.0926]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1178, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 31 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 32 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1158],\n",
      "        [0.1102],\n",
      "        [0.1102],\n",
      "        [0.1105],\n",
      "        [0.1103],\n",
      "        [0.1107],\n",
      "        [0.1117],\n",
      "        [0.1106],\n",
      "        [0.1101]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1105, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 32 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 33 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1103],\n",
      "        [0.1108],\n",
      "        [0.1110],\n",
      "        [0.1126],\n",
      "        [0.1126],\n",
      "        [0.1101],\n",
      "        [0.1113],\n",
      "        [0.1110],\n",
      "        [0.1103]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1113, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 33 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 34 - Output:\n",
      "torch.Size([14, 1])\n",
      "tensor([[0.0712],\n",
      "        [0.0716],\n",
      "        [0.0714],\n",
      "        [0.0711],\n",
      "        [0.0711],\n",
      "        [0.0726],\n",
      "        [0.0711],\n",
      "        [0.0721],\n",
      "        [0.0711],\n",
      "        [0.0718],\n",
      "        [0.0715],\n",
      "        [0.0711],\n",
      "        [0.0711],\n",
      "        [0.0714]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0711, device='cuda:0', grad_fn=<SelectBackward0>) 12\n",
      "\n",
      "torch.Size([14, 14])\n",
      "torch.Size([14, 14])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 34 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 35 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0552],\n",
      "        [0.0552],\n",
      "        [0.0554],\n",
      "        [0.0554],\n",
      "        [0.0553],\n",
      "        [0.0558],\n",
      "        [0.0552],\n",
      "        [0.0552],\n",
      "        [0.0556],\n",
      "        [0.0552],\n",
      "        [0.0557],\n",
      "        [0.0558],\n",
      "        [0.0573],\n",
      "        [0.0556],\n",
      "        [0.0557],\n",
      "        [0.0557],\n",
      "        [0.0554],\n",
      "        [0.0552]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0557, device='cuda:0', grad_fn=<SelectBackward0>) 14\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 35 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 36 - Output:\n",
      "torch.Size([13, 1])\n",
      "tensor([[0.0738],\n",
      "        [0.0744],\n",
      "        [0.0759],\n",
      "        [0.0742],\n",
      "        [0.0741],\n",
      "        [0.0738],\n",
      "        [0.0746],\n",
      "        [0.0740],\n",
      "        [0.0747],\n",
      "        [0.0751],\n",
      "        [0.0737],\n",
      "        [0.1071],\n",
      "        [0.0745]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([13, 13])\n",
      "torch.Size([13, 13])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 36 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 37 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1420],\n",
      "        [0.1431],\n",
      "        [0.1425],\n",
      "        [0.1427],\n",
      "        [0.1423],\n",
      "        [0.1448],\n",
      "        [0.1425]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1427, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 37 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 38 - Output:\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.1951],\n",
      "        [0.1993],\n",
      "        [0.2055],\n",
      "        [0.2008],\n",
      "        [0.1993]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1993, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 38 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 39 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0842],\n",
      "        [0.0830],\n",
      "        [0.0836],\n",
      "        [0.0830],\n",
      "        [0.0845],\n",
      "        [0.0830],\n",
      "        [0.0833],\n",
      "        [0.0832],\n",
      "        [0.0835],\n",
      "        [0.0829],\n",
      "        [0.0829],\n",
      "        [0.0830]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0833, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 39 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 40 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.1011],\n",
      "        [0.0988],\n",
      "        [0.1001],\n",
      "        [0.1012],\n",
      "        [0.0990],\n",
      "        [0.0987],\n",
      "        [0.1006],\n",
      "        [0.1020],\n",
      "        [0.0988],\n",
      "        [0.0998]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1012, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 40 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 41 - Output:\n",
      "torch.Size([14, 1])\n",
      "tensor([[0.0705],\n",
      "        [0.0687],\n",
      "        [0.0684],\n",
      "        [0.0696],\n",
      "        [0.0684],\n",
      "        [0.0690],\n",
      "        [0.0796],\n",
      "        [0.0690],\n",
      "        [0.0705],\n",
      "        [0.0683],\n",
      "        [0.0900],\n",
      "        [0.0689],\n",
      "        [0.0694],\n",
      "        [0.0697]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0690, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([14, 14])\n",
      "torch.Size([14, 14])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 41 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 42 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1105],\n",
      "        [0.1125],\n",
      "        [0.1108],\n",
      "        [0.1119],\n",
      "        [0.1135],\n",
      "        [0.1099],\n",
      "        [0.1098],\n",
      "        [0.1098],\n",
      "        [0.1112]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1125, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1105],\n",
      "        [0.1125],\n",
      "        [0.1108],\n",
      "        [0.1119],\n",
      "        [0.1135],\n",
      "        [0.1099],\n",
      "        [0.1098],\n",
      "        [0.1098],\n",
      "        [0.1112]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 42 - Decode Output:\n",
      "tensor([0.1111], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 43 - Output:\n",
      "torch.Size([13, 1])\n",
      "tensor([[0.0761],\n",
      "        [0.0777],\n",
      "        [0.0757],\n",
      "        [0.0766],\n",
      "        [0.0750],\n",
      "        [0.0752],\n",
      "        [0.0858],\n",
      "        [0.0765],\n",
      "        [0.0758],\n",
      "        [0.0753],\n",
      "        [0.0768],\n",
      "        [0.0783],\n",
      "        [0.0751]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0765, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([13, 13])\n",
      "torch.Size([13, 13])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 43 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 44 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0523],\n",
      "        [0.0526],\n",
      "        [0.0530],\n",
      "        [0.0526],\n",
      "        [0.0523],\n",
      "        [0.0529],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0523],\n",
      "        [0.0527],\n",
      "        [0.0535],\n",
      "        [0.0529],\n",
      "        [0.0524],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0531],\n",
      "        [0.0525],\n",
      "        [0.0524],\n",
      "        [0.0528]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0523, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 44 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 45 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.1159],\n",
      "        [0.0937],\n",
      "        [0.1064],\n",
      "        [0.0980],\n",
      "        [0.0934],\n",
      "        [0.0934],\n",
      "        [0.0997],\n",
      "        [0.0972],\n",
      "        [0.0989],\n",
      "        [0.1034]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0934, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 45 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 46 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1055],\n",
      "        [0.1198],\n",
      "        [0.1072],\n",
      "        [0.1135],\n",
      "        [0.1086],\n",
      "        [0.1069],\n",
      "        [0.1190],\n",
      "        [0.1059],\n",
      "        [0.1137]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1069, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 46 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 47 - Output:\n",
      "torch.Size([13, 1])\n",
      "tensor([[0.0765],\n",
      "        [0.0767],\n",
      "        [0.0762],\n",
      "        [0.0782],\n",
      "        [0.0780],\n",
      "        [0.0759],\n",
      "        [0.0759],\n",
      "        [0.0759],\n",
      "        [0.0762],\n",
      "        [0.0762],\n",
      "        [0.0804],\n",
      "        [0.0760],\n",
      "        [0.0779]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0759, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([13, 13])\n",
      "torch.Size([13, 13])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 47 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 48 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1034],\n",
      "        [0.1088],\n",
      "        [0.1052],\n",
      "        [0.1105],\n",
      "        [0.1561],\n",
      "        [0.1041],\n",
      "        [0.1044],\n",
      "        [0.1032],\n",
      "        [0.1042]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1032, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 48 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 49 - Output:\n",
      "torch.Size([17, 1])\n",
      "tensor([[0.0596],\n",
      "        [0.0585],\n",
      "        [0.0592],\n",
      "        [0.0586],\n",
      "        [0.0585],\n",
      "        [0.0602],\n",
      "        [0.0584],\n",
      "        [0.0585],\n",
      "        [0.0585],\n",
      "        [0.0586],\n",
      "        [0.0589],\n",
      "        [0.0586],\n",
      "        [0.0587],\n",
      "        [0.0585],\n",
      "        [0.0595],\n",
      "        [0.0586],\n",
      "        [0.0585]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0586, device='cuda:0', grad_fn=<SelectBackward0>) 15\n",
      "\n",
      "torch.Size([17, 17])\n",
      "torch.Size([17, 17])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 49 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 50 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0553],\n",
      "        [0.0553],\n",
      "        [0.0552],\n",
      "        [0.0557],\n",
      "        [0.0553],\n",
      "        [0.0552],\n",
      "        [0.0553],\n",
      "        [0.0560],\n",
      "        [0.0552],\n",
      "        [0.0552],\n",
      "        [0.0557],\n",
      "        [0.0579],\n",
      "        [0.0554],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0552],\n",
      "        [0.0555],\n",
      "        [0.0553]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0552, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 50 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 51 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1357],\n",
      "        [0.1401],\n",
      "        [0.1403],\n",
      "        [0.1483],\n",
      "        [0.1349],\n",
      "        [0.1700],\n",
      "        [0.1307]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1700, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 51 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 52 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3656],\n",
      "        [0.3171],\n",
      "        [0.3173]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3171, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.3656],\n",
      "        [0.3171],\n",
      "        [0.3173]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 52 - Decode Output:\n",
      "tensor([0.3333], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3334, 0.3333, 0.3333]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 53 - Output:\n",
      "torch.Size([17, 1])\n",
      "tensor([[0.0585],\n",
      "        [0.0588],\n",
      "        [0.0585],\n",
      "        [0.0585],\n",
      "        [0.0585],\n",
      "        [0.0586],\n",
      "        [0.0585],\n",
      "        [0.0606],\n",
      "        [0.0585],\n",
      "        [0.0587],\n",
      "        [0.0586],\n",
      "        [0.0586],\n",
      "        [0.0588],\n",
      "        [0.0589],\n",
      "        [0.0603],\n",
      "        [0.0585],\n",
      "        [0.0585]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0585, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([17, 17])\n",
      "torch.Size([17, 17])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 53 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 54 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0553],\n",
      "        [0.0554],\n",
      "        [0.0554],\n",
      "        [0.0554],\n",
      "        [0.0553],\n",
      "        [0.0561],\n",
      "        [0.0553],\n",
      "        [0.0554],\n",
      "        [0.0553],\n",
      "        [0.0555],\n",
      "        [0.0565],\n",
      "        [0.0553],\n",
      "        [0.0554],\n",
      "        [0.0561],\n",
      "        [0.0556],\n",
      "        [0.0558],\n",
      "        [0.0555],\n",
      "        [0.0554]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0555, device='cuda:0', grad_fn=<SelectBackward0>) 9\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 54 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 55 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0836],\n",
      "        [0.0880],\n",
      "        [0.0825],\n",
      "        [0.0835],\n",
      "        [0.0825],\n",
      "        [0.0825],\n",
      "        [0.0827],\n",
      "        [0.0834],\n",
      "        [0.0826],\n",
      "        [0.0827],\n",
      "        [0.0827],\n",
      "        [0.0834]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0825, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 55 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 56 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0526],\n",
      "        [0.0520],\n",
      "        [0.0529],\n",
      "        [0.0528],\n",
      "        [0.0522],\n",
      "        [0.0521],\n",
      "        [0.0518],\n",
      "        [0.0538],\n",
      "        [0.0523],\n",
      "        [0.0531],\n",
      "        [0.0518],\n",
      "        [0.0517],\n",
      "        [0.0561],\n",
      "        [0.0522],\n",
      "        [0.0522],\n",
      "        [0.0553],\n",
      "        [0.0516],\n",
      "        [0.0516],\n",
      "        [0.0520]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0522, device='cuda:0', grad_fn=<SelectBackward0>) 13\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 56 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 57 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1389],\n",
      "        [0.1576],\n",
      "        [0.1473],\n",
      "        [0.1390],\n",
      "        [0.1391],\n",
      "        [0.1403],\n",
      "        [0.1377]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1389, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1389],\n",
      "        [0.1576],\n",
      "        [0.1473],\n",
      "        [0.1390],\n",
      "        [0.1391],\n",
      "        [0.1403],\n",
      "        [0.1377]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 57 - Decode Output:\n",
      "tensor([0.1429], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 58 - Output:\n",
      "torch.Size([4, 1])\n",
      "tensor([[0.2560],\n",
      "        [0.2507],\n",
      "        [0.2463],\n",
      "        [0.2470]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.2463, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 4])\n",
      "\n",
      "v_j\n",
      "tensor([[0.2560],\n",
      "        [0.2507],\n",
      "        [0.2463],\n",
      "        [0.2470]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 58 - Decode Output:\n",
      "tensor([0.2500], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2500, 0.2500, 0.2500, 0.2500]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 59 - Output:\n",
      "torch.Size([1, 1])\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "\n",
      "v_j\n",
      "tensor([[0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 59 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 60 - Output:\n",
      "torch.Size([16, 1])\n",
      "tensor([[0.0618],\n",
      "        [0.0617],\n",
      "        [0.0642],\n",
      "        [0.0613],\n",
      "        [0.0611],\n",
      "        [0.0613],\n",
      "        [0.0728],\n",
      "        [0.0615],\n",
      "        [0.0615],\n",
      "        [0.0614],\n",
      "        [0.0611],\n",
      "        [0.0614],\n",
      "        [0.0625],\n",
      "        [0.0632],\n",
      "        [0.0613],\n",
      "        [0.0618]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0611, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 16])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 60 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 61 - Output:\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.4807],\n",
      "        [0.5193]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.5193, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 61 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 62 - Output:\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.1913],\n",
      "        [0.1898],\n",
      "        [0.2203],\n",
      "        [0.1995],\n",
      "        [0.1992]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1995, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 62 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 63 - Output:\n",
      "torch.Size([1, 1])\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "\n",
      "v_j\n",
      "tensor([[0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 63 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 64 - Output:\n",
      "torch.Size([16, 1])\n",
      "tensor([[0.0622],\n",
      "        [0.0633],\n",
      "        [0.0636],\n",
      "        [0.0630],\n",
      "        [0.0629],\n",
      "        [0.0618],\n",
      "        [0.0631],\n",
      "        [0.0626],\n",
      "        [0.0621],\n",
      "        [0.0620],\n",
      "        [0.0618],\n",
      "        [0.0618],\n",
      "        [0.0620],\n",
      "        [0.0640],\n",
      "        [0.0618],\n",
      "        [0.0621]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0629, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 16])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 64 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 65 - Output:\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.2025],\n",
      "        [0.1865],\n",
      "        [0.1829],\n",
      "        [0.1912],\n",
      "        [0.2369]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.2369, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 65 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 66 - Output:\n",
      "torch.Size([17, 1])\n",
      "tensor([[0.0587],\n",
      "        [0.0590],\n",
      "        [0.0586],\n",
      "        [0.0586],\n",
      "        [0.0585],\n",
      "        [0.0586],\n",
      "        [0.0586],\n",
      "        [0.0599],\n",
      "        [0.0586],\n",
      "        [0.0596],\n",
      "        [0.0585],\n",
      "        [0.0587],\n",
      "        [0.0590],\n",
      "        [0.0595],\n",
      "        [0.0584],\n",
      "        [0.0589],\n",
      "        [0.0585]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0584, device='cuda:0', grad_fn=<SelectBackward0>) 14\n",
      "\n",
      "torch.Size([17, 17])\n",
      "torch.Size([17, 17])\n",
      "\n",
      "v_j\n",
      "tensor([[0.0587],\n",
      "        [0.0590],\n",
      "        [0.0586],\n",
      "        [0.0586],\n",
      "        [0.0585],\n",
      "        [0.0586],\n",
      "        [0.0586],\n",
      "        [0.0599],\n",
      "        [0.0586],\n",
      "        [0.0596],\n",
      "        [0.0585],\n",
      "        [0.0587],\n",
      "        [0.0590],\n",
      "        [0.0595],\n",
      "        [0.0584],\n",
      "        [0.0589],\n",
      "        [0.0585]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 66 - Decode Output:\n",
      "tensor([0.0588], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588,\n",
      "         0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588, 0.0588]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 67 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.0956],\n",
      "        [0.1014],\n",
      "        [0.0959],\n",
      "        [0.0958],\n",
      "        [0.0958],\n",
      "        [0.0981],\n",
      "        [0.0966],\n",
      "        [0.1119],\n",
      "        [0.1015],\n",
      "        [0.1071]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0959, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 67 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 68 - Output:\n",
      "torch.Size([6, 1])\n",
      "tensor([[0.1664],\n",
      "        [0.1664],\n",
      "        [0.1676],\n",
      "        [0.1667],\n",
      "        [0.1664],\n",
      "        [0.1665]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1665, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([6, 16])\n",
      "attn_output.size\n",
      "torch.Size([6])\n",
      "masked_attn_\n",
      "torch.Size([6, 6])\n",
      "masked_attn_1\n",
      "torch.Size([1, 6])\n",
      "attn_weights.size\n",
      "torch.Size([1, 6])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 68 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 69 - Output:\n",
      "torch.Size([5, 1])\n",
      "tensor([[0.2474],\n",
      "        [0.1999],\n",
      "        [0.1817],\n",
      "        [0.1801],\n",
      "        [0.1908]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1801, device='cuda:0', grad_fn=<SelectBackward0>) 3\n",
      "\n",
      "torch.Size([5, 5])\n",
      "torch.Size([5, 5])\n",
      "\n",
      "v_j\n",
      "tensor([[0.2474],\n",
      "        [0.1999],\n",
      "        [0.1817],\n",
      "        [0.1801],\n",
      "        [0.1908]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 69 - Decode Output:\n",
      "tensor([0.2000], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2000, 0.2000, 0.2000, 0.2000, 0.2000]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 70 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1103],\n",
      "        [0.1108],\n",
      "        [0.1104],\n",
      "        [0.1142],\n",
      "        [0.1102],\n",
      "        [0.1119],\n",
      "        [0.1116],\n",
      "        [0.1104],\n",
      "        [0.1102]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1104, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 70 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 71 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3201],\n",
      "        [0.3350],\n",
      "        [0.3450]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3450, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 71 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 72 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1402],\n",
      "        [0.1434],\n",
      "        [0.1399],\n",
      "        [0.1460],\n",
      "        [0.1403],\n",
      "        [0.1512],\n",
      "        [0.1391]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1402, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 72 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 73 - Output:\n",
      "torch.Size([8, 1])\n",
      "tensor([[0.1195],\n",
      "        [0.1211],\n",
      "        [0.1210],\n",
      "        [0.1199],\n",
      "        [0.1531],\n",
      "        [0.1210],\n",
      "        [0.1243],\n",
      "        [0.1200]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1531, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1195],\n",
      "        [0.1211],\n",
      "        [0.1210],\n",
      "        [0.1199],\n",
      "        [0.1531],\n",
      "        [0.1210],\n",
      "        [0.1243],\n",
      "        [0.1200]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 73 - Decode Output:\n",
      "tensor([0.1250], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 74 - Output:\n",
      "torch.Size([15, 1])\n",
      "tensor([[0.0667],\n",
      "        [0.0665],\n",
      "        [0.0691],\n",
      "        [0.0662],\n",
      "        [0.0659],\n",
      "        [0.0663],\n",
      "        [0.0694],\n",
      "        [0.0659],\n",
      "        [0.0659],\n",
      "        [0.0665],\n",
      "        [0.0659],\n",
      "        [0.0670],\n",
      "        [0.0666],\n",
      "        [0.0663],\n",
      "        [0.0659]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0691, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 74 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 75 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0819],\n",
      "        [0.0842],\n",
      "        [0.0872],\n",
      "        [0.0854],\n",
      "        [0.0846],\n",
      "        [0.0818],\n",
      "        [0.0814],\n",
      "        [0.0845],\n",
      "        [0.0814],\n",
      "        [0.0818],\n",
      "        [0.0828],\n",
      "        [0.0831]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0872, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 75 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 76 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1099],\n",
      "        [0.1104],\n",
      "        [0.1096],\n",
      "        [0.1095],\n",
      "        [0.1100],\n",
      "        [0.1108],\n",
      "        [0.1111],\n",
      "        [0.1118],\n",
      "        [0.1170]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1099, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 76 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 77 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0551],\n",
      "        [0.0539],\n",
      "        [0.0545],\n",
      "        [0.0588],\n",
      "        [0.0545],\n",
      "        [0.0552],\n",
      "        [0.0606],\n",
      "        [0.0541],\n",
      "        [0.0539],\n",
      "        [0.0539],\n",
      "        [0.0585],\n",
      "        [0.0544],\n",
      "        [0.0540],\n",
      "        [0.0542],\n",
      "        [0.0583],\n",
      "        [0.0543],\n",
      "        [0.0553],\n",
      "        [0.0565]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0545, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 77 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 78 - Output:\n",
      "torch.Size([8, 1])\n",
      "tensor([[0.1168],\n",
      "        [0.1158],\n",
      "        [0.1215],\n",
      "        [0.1228],\n",
      "        [0.1311],\n",
      "        [0.1451],\n",
      "        [0.1278],\n",
      "        [0.1191]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1215, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 78 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 79 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0828],\n",
      "        [0.0836],\n",
      "        [0.0830],\n",
      "        [0.0828],\n",
      "        [0.0836],\n",
      "        [0.0829],\n",
      "        [0.0839],\n",
      "        [0.0832],\n",
      "        [0.0833],\n",
      "        [0.0839],\n",
      "        [0.0842],\n",
      "        [0.0828]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0836, device='cuda:0', grad_fn=<SelectBackward0>) 4\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 79 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 80 - Output:\n",
      "torch.Size([18, 1])\n",
      "tensor([[0.0552],\n",
      "        [0.0550],\n",
      "        [0.0553],\n",
      "        [0.0545],\n",
      "        [0.0546],\n",
      "        [0.0560],\n",
      "        [0.0547],\n",
      "        [0.0551],\n",
      "        [0.0585],\n",
      "        [0.0551],\n",
      "        [0.0545],\n",
      "        [0.0546],\n",
      "        [0.0559],\n",
      "        [0.0578],\n",
      "        [0.0558],\n",
      "        [0.0546],\n",
      "        [0.0545],\n",
      "        [0.0582]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0550, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([18, 18])\n",
      "torch.Size([18, 18])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 80 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 81 - Output:\n",
      "torch.Size([9, 1])\n",
      "tensor([[0.1045],\n",
      "        [0.1048],\n",
      "        [0.1170],\n",
      "        [0.1059],\n",
      "        [0.1338],\n",
      "        [0.1045],\n",
      "        [0.1150],\n",
      "        [0.1054],\n",
      "        [0.1092]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1045, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([9, 9])\n",
      "torch.Size([9, 9])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1045],\n",
      "        [0.1048],\n",
      "        [0.1170],\n",
      "        [0.1059],\n",
      "        [0.1338],\n",
      "        [0.1045],\n",
      "        [0.1150],\n",
      "        [0.1054],\n",
      "        [0.1092]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 81 - Decode Output:\n",
      "tensor([0.1111], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 82 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0525],\n",
      "        [0.0530],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0527],\n",
      "        [0.0526],\n",
      "        [0.0533],\n",
      "        [0.0528],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0524],\n",
      "        [0.0529],\n",
      "        [0.0525],\n",
      "        [0.0529],\n",
      "        [0.0525],\n",
      "        [0.0524],\n",
      "        [0.0524]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0525, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 82 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 83 - Output:\n",
      "torch.Size([11, 1])\n",
      "tensor([[0.0888],\n",
      "        [0.1301],\n",
      "        [0.0851],\n",
      "        [0.0849],\n",
      "        [0.0844],\n",
      "        [0.0851],\n",
      "        [0.0845],\n",
      "        [0.0869],\n",
      "        [0.0838],\n",
      "        [0.0979],\n",
      "        [0.0886]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0845, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 11])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 83 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 84 - Output:\n",
      "torch.Size([3, 1])\n",
      "tensor([[0.3290],\n",
      "        [0.3339],\n",
      "        [0.3370]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.3370, device='cuda:0', grad_fn=<SelectBackward0>) 2\n",
      "\n",
      "torch.Size([3, 3])\n",
      "torch.Size([3, 3])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 84 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 85 - Output:\n",
      "torch.Size([15, 1])\n",
      "tensor([[0.0665],\n",
      "        [0.0669],\n",
      "        [0.0666],\n",
      "        [0.0672],\n",
      "        [0.0666],\n",
      "        [0.0665],\n",
      "        [0.0668],\n",
      "        [0.0666],\n",
      "        [0.0666],\n",
      "        [0.0671],\n",
      "        [0.0665],\n",
      "        [0.0666],\n",
      "        [0.0666],\n",
      "        [0.0665],\n",
      "        [0.0665]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0666, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 85 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 86 - Output:\n",
      "torch.Size([11, 1])\n",
      "tensor([[0.0905],\n",
      "        [0.0899],\n",
      "        [0.0900],\n",
      "        [0.0904],\n",
      "        [0.0899],\n",
      "        [0.0907],\n",
      "        [0.0941],\n",
      "        [0.0911],\n",
      "        [0.0920],\n",
      "        [0.0916],\n",
      "        [0.0899]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0920, device='cuda:0', grad_fn=<SelectBackward0>) 8\n",
      "\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 11])\n",
      "\n",
      "v_j\n",
      "tensor([[0.0905],\n",
      "        [0.0899],\n",
      "        [0.0900],\n",
      "        [0.0904],\n",
      "        [0.0899],\n",
      "        [0.0907],\n",
      "        [0.0941],\n",
      "        [0.0911],\n",
      "        [0.0920],\n",
      "        [0.0916],\n",
      "        [0.0899]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 86 - Decode Output:\n",
      "tensor([0.0909], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909, 0.0909,\n",
      "         0.0909, 0.0909]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 87 - Output:\n",
      "torch.Size([1, 1])\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(1., device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1, 1])\n",
      "\n",
      "v_j\n",
      "tensor([[0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 87 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 88 - Output:\n",
      "torch.Size([8, 1])\n",
      "tensor([[0.1191],\n",
      "        [0.1264],\n",
      "        [0.1200],\n",
      "        [0.1192],\n",
      "        [0.1230],\n",
      "        [0.1319],\n",
      "        [0.1397],\n",
      "        [0.1207]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1264, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([8, 8])\n",
      "torch.Size([8, 8])\n",
      "\n",
      "v_j\n",
      "tensor([[0.1191],\n",
      "        [0.1264],\n",
      "        [0.1200],\n",
      "        [0.1192],\n",
      "        [0.1230],\n",
      "        [0.1319],\n",
      "        [0.1397],\n",
      "        [0.1207]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 88 - Decode Output:\n",
      "tensor([0.1250], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 89 - Output:\n",
      "torch.Size([11, 1])\n",
      "tensor([[0.0909],\n",
      "        [0.0906],\n",
      "        [0.0904],\n",
      "        [0.0915],\n",
      "        [0.0906],\n",
      "        [0.0911],\n",
      "        [0.0909],\n",
      "        [0.0913],\n",
      "        [0.0907],\n",
      "        [0.0903],\n",
      "        [0.0916]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0913, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 11])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 89 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 90 - Output:\n",
      "torch.Size([10, 1])\n",
      "tensor([[0.0992],\n",
      "        [0.0992],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.1026],\n",
      "        [0.0992],\n",
      "        [0.1009],\n",
      "        [0.1001],\n",
      "        [0.0997],\n",
      "        [0.1009]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1001, device='cuda:0', grad_fn=<SelectBackward0>) 7\n",
      "\n",
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n",
      "\n",
      "v_j\n",
      "tensor([[0.0992],\n",
      "        [0.0992],\n",
      "        [0.0991],\n",
      "        [0.0991],\n",
      "        [0.1026],\n",
      "        [0.0992],\n",
      "        [0.1009],\n",
      "        [0.1001],\n",
      "        [0.0997],\n",
      "        [0.1009]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 90 - Decode Output:\n",
      "tensor([0.1000], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 91 - Output:\n",
      "torch.Size([13, 1])\n",
      "tensor([[0.0754],\n",
      "        [0.0744],\n",
      "        [0.0758],\n",
      "        [0.0754],\n",
      "        [0.0743],\n",
      "        [0.0741],\n",
      "        [0.1020],\n",
      "        [0.0742],\n",
      "        [0.0752],\n",
      "        [0.0752],\n",
      "        [0.0746],\n",
      "        [0.0749],\n",
      "        [0.0744]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0744, device='cuda:0', grad_fn=<SelectBackward0>) 1\n",
      "\n",
      "torch.Size([13, 13])\n",
      "torch.Size([13, 13])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 91 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 92 - Output:\n",
      "torch.Size([11, 1])\n",
      "tensor([[0.0907],\n",
      "        [0.0896],\n",
      "        [0.0896],\n",
      "        [0.0984],\n",
      "        [0.0910],\n",
      "        [0.0903],\n",
      "        [0.0895],\n",
      "        [0.0902],\n",
      "        [0.0896],\n",
      "        [0.0914],\n",
      "        [0.0897]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0907, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([11, 11])\n",
      "torch.Size([11, 11])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 92 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 93 - Output:\n",
      "torch.Size([15, 1])\n",
      "tensor([[0.0660],\n",
      "        [0.0656],\n",
      "        [0.0655],\n",
      "        [0.0657],\n",
      "        [0.0659],\n",
      "        [0.0661],\n",
      "        [0.0660],\n",
      "        [0.0716],\n",
      "        [0.0664],\n",
      "        [0.0654],\n",
      "        [0.0669],\n",
      "        [0.0666],\n",
      "        [0.0697],\n",
      "        [0.0655],\n",
      "        [0.0670]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0655, device='cuda:0', grad_fn=<SelectBackward0>) 13\n",
      "\n",
      "torch.Size([15, 15])\n",
      "torch.Size([15, 15])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 93 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 94 - Output:\n",
      "torch.Size([6, 1])\n",
      "tensor([[0.1533],\n",
      "        [0.1593],\n",
      "        [0.1612],\n",
      "        [0.1701],\n",
      "        [0.1959],\n",
      "        [0.1601]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1533, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([6, 6])\n",
      "torch.Size([6, 6])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([6, 16])\n",
      "attn_output.size\n",
      "torch.Size([6])\n",
      "masked_attn_\n",
      "torch.Size([6, 6])\n",
      "masked_attn_1\n",
      "torch.Size([1, 6])\n",
      "attn_weights.size\n",
      "torch.Size([1, 6])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 94 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 95 - Output:\n",
      "torch.Size([2, 1])\n",
      "tensor([[0.5018],\n",
      "        [0.4982]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.5018, device='cuda:0', grad_fn=<SelectBackward0>) 0\n",
      "\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n",
      "\n",
      "v_j\n",
      "tensor([[0.5018],\n",
      "        [0.4982]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 95 - Decode Output:\n",
      "tensor([0.5000], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.5000, 0.5000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 96 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0832],\n",
      "        [0.0839],\n",
      "        [0.0834],\n",
      "        [0.0832],\n",
      "        [0.0829],\n",
      "        [0.0831],\n",
      "        [0.0844],\n",
      "        [0.0829],\n",
      "        [0.0833],\n",
      "        [0.0829],\n",
      "        [0.0834],\n",
      "        [0.0833]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0831, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 96 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 97 - Output:\n",
      "torch.Size([12, 1])\n",
      "tensor([[0.0831],\n",
      "        [0.0828],\n",
      "        [0.0849],\n",
      "        [0.0832],\n",
      "        [0.0827],\n",
      "        [0.0850],\n",
      "        [0.0833],\n",
      "        [0.0838],\n",
      "        [0.0830],\n",
      "        [0.0830],\n",
      "        [0.0826],\n",
      "        [0.0827]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0827, device='cuda:0', grad_fn=<SelectBackward0>) 11\n",
      "\n",
      "torch.Size([12, 12])\n",
      "torch.Size([12, 12])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 97 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 98 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0555],\n",
      "        [0.0526],\n",
      "        [0.0521],\n",
      "        [0.0527],\n",
      "        [0.0527],\n",
      "        [0.0523],\n",
      "        [0.0521],\n",
      "        [0.0529],\n",
      "        [0.0522],\n",
      "        [0.0521],\n",
      "        [0.0526],\n",
      "        [0.0522],\n",
      "        [0.0530],\n",
      "        [0.0520],\n",
      "        [0.0539],\n",
      "        [0.0526],\n",
      "        [0.0520],\n",
      "        [0.0521],\n",
      "        [0.0524]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0523, device='cuda:0', grad_fn=<SelectBackward0>) 5\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.0555],\n",
      "        [0.0526],\n",
      "        [0.0521],\n",
      "        [0.0527],\n",
      "        [0.0527],\n",
      "        [0.0523],\n",
      "        [0.0521],\n",
      "        [0.0529],\n",
      "        [0.0522],\n",
      "        [0.0521],\n",
      "        [0.0526],\n",
      "        [0.0522],\n",
      "        [0.0530],\n",
      "        [0.0520],\n",
      "        [0.0539],\n",
      "        [0.0526],\n",
      "        [0.0520],\n",
      "        [0.0521],\n",
      "        [0.0524]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 98 - Decode Output:\n",
      "tensor([0.0526], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526,\n",
      "         0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526, 0.0526,\n",
      "         0.0526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 99 - Output:\n",
      "torch.Size([7, 1])\n",
      "tensor([[0.1402],\n",
      "        [0.1380],\n",
      "        [0.1432],\n",
      "        [0.1383],\n",
      "        [0.1416],\n",
      "        [0.1606],\n",
      "        [0.1381]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.1381, device='cuda:0', grad_fn=<SelectBackward0>) 6\n",
      "\n",
      "torch.Size([7, 7])\n",
      "torch.Size([7, 7])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 99 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 100 - Output:\n",
      "torch.Size([19, 1])\n",
      "tensor([[0.0523],\n",
      "        [0.0525],\n",
      "        [0.0523],\n",
      "        [0.0528],\n",
      "        [0.0530],\n",
      "        [0.0526],\n",
      "        [0.0526],\n",
      "        [0.0523],\n",
      "        [0.0524],\n",
      "        [0.0525],\n",
      "        [0.0526],\n",
      "        [0.0539],\n",
      "        [0.0537],\n",
      "        [0.0524],\n",
      "        [0.0529],\n",
      "        [0.0523],\n",
      "        [0.0523],\n",
      "        [0.0522],\n",
      "        [0.0524]], device='cuda:0', grad_fn=<TransposeBackward0>)\n",
      "v_i\n",
      "tensor(0.0523, device='cuda:0', grad_fn=<SelectBackward0>) 16\n",
      "\n",
      "torch.Size([19, 19])\n",
      "torch.Size([19, 19])\n",
      "\n",
      "v_j\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "phi_v_i.size()\n",
      "torch.Size([16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 1])\n",
      "output_size1\n",
      "torch.Size([1])\n",
      "Graph 100 - Decode Output:\n",
      "tensor([nan], device='cuda:0', grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_features = 4 * n_heads\n",
    "out_features = n_heads\n",
    "d_h = 4 * n_heads\n",
    "dropout = 0.6\n",
    "gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "gat_models = []\n",
    "for graph_idx, (x, adj_tensor, j, adj_matrix_original) in enumerate(graphs):\n",
    "    gat_models.append(gat_model)\n",
    "    x = x.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(x, adj_tensor)\n",
    "    print(f\"Graph {graph_idx+1} - Output:\")\n",
    "    print(output.shape)\n",
    "    print(output)\n",
    "\n",
    "    v_i = output[j,0]\n",
    "    print(\"v_i\")\n",
    "    print(v_i, j)\n",
    "    # Generate neighbors tensor\n",
    "    print()\n",
    "    print(adj_matrix_original.size())\n",
    "    print(adj_tensor.squeeze(2).size())\n",
    "    v_j = adj_matrix_original[-1,j].cuda() * output\n",
    "    print(\"\")\n",
    "    print(\"v_j\")\n",
    "    print(v_j)\n",
    "\n",
    "\n",
    "    output, decode_output, attn_weights = gat_model.decode(output, v_i, v_j)\n",
    "    print(f\"Graph {graph_idx+1} - Decode Output:\")\n",
    "    print(decode_output)\n",
    "    print(\"Attention Weights:\")\n",
    "    print(attn_weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "https://chioni.github.io/posts/gat/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
