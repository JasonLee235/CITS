{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "from graphviz import Graph\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6942],\n",
      "        [-1.0504],\n",
      "        [ 0.8351]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6942, -1.0504,  0.8351]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,1)\n",
    "print(a)\n",
    "b = a.transpose(0,1)\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['weight', 'bias'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inear = torch.nn.Linear(2, 1)\n",
    "b = inear.state_dict().keys()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "        self.in_features = in_features\n",
    "\n",
    "        self.phi1 = torch.nn.Linear(d_h, 1)\n",
    "        self.phi2 = torch.nn.Linear(d_h, 1)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.C = torch.nn.Parameter(torch.randn(1)) # constant C\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_i, v_j):\n",
    "        v_i = v_i.unsqueeze(0)\n",
    "        phi1_v_i = torch.matmul(v_i, self.phi1.state_dict()['weight']) # phi1_v_i 의 사이즈 ()\n",
    "        print(\"phi_v_i.size()\")\n",
    "        print(phi1_v_i.size())\n",
    "\n",
    "        phi2_v_j = torch.matmul(v_j, self.phi2.state_dict()['weight']) # phi2_neighbors 의 사이즈 ()\n",
    "        print(\"phi_v_j.size()\")\n",
    "        print(phi2_v_j.size())\n",
    "        \n",
    "        attn_input = torch.matmul(phi1_v_i, phi2_v_j.transpose(0,1)) / (self.d_h ** 0.5) # (1,n) 의 크기를 갖는 attn_input\n",
    "        # attn_input = attn_input.squeeze(0)  # Remove the extra dimension\n",
    "        attn_input = attn_input\n",
    "\n",
    "        attn_output = self.C * self.activation(attn_input)\n",
    "        print(\"attn_output.size\")\n",
    "        print(attn_output.size())\n",
    "        # v_j의 크기를 (n, 1)로 변형하여 크기를 맞춤\n",
    "        # v_j = v_j.unsqueeze(1)\n",
    "        # masked_attn_output = attn_output.masked_fill(v_j == 0, float('-inf'))\n",
    "        # masked_attn_output = masked_attn_output.squeeze(1)\n",
    "        masked_attn_output = torch.where(v_j == 0, float('-inf'), attn_output)\n",
    "        print(\"masked_attn_\")\n",
    "        print(masked_attn_output.size())\n",
    "\n",
    "        masked_attn_output= masked_attn_output[0]\n",
    "        masked_attn_output = masked_attn_output.unsqueeze(0)\n",
    "        print(\"masked_attn_1\")\n",
    "        print(masked_attn_output.size())\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "        # attn_weights = self.softmax(attn_output)        \n",
    "        print(\"attn_weights.size\")\n",
    "        print(attn_weights.size())\n",
    "\n",
    "\n",
    "        output = torch.matmul(attn_weights, x)\n",
    "        print(\"output_size\")\n",
    "        print(output.size())\n",
    "        output = output.squeeze(0)\n",
    "        print(\"output_size1\")\n",
    "        print(output.size())\n",
    "        return x, output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, n_heads)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(in_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, v_i, v_j):\n",
    "        return self.decoder(x, v_i, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weighted_graph(num_nodes, num_edges, max_weight=10):\n",
    "    # 방향 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 노드 추가\n",
    "    nodes = range(num_nodes)\n",
    "    graph.add_nodes_from(nodes)\n",
    "    \n",
    "    # 간선 추가\n",
    "    edges = []\n",
    "    for i in range(num_edges):\n",
    "        # 임의의 출발 노드와 도착 노드 선택\n",
    "        source = random.choice(nodes)\n",
    "        target = random.choice(nodes)\n",
    "        \n",
    "        # 출발 노드와 도착 노드가 같은 경우 건너뜀\n",
    "        if source == target:\n",
    "            continue\n",
    "        \n",
    "        # 가중치 랜덤 생성\n",
    "        weight = random.randint(1, max_weight)\n",
    "        \n",
    "        # 간선 추가\n",
    "        edges.append((source, target, weight))\n",
    "\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    adj_matrix = adj_matrix + sp.eye(adj_matrix.shape[0]) # Add self-loop\n",
    "    adj_tensor = torch.Tensor(adj_matrix.todense())\n",
    "    \n",
    "    in_features =  1\n",
    "    x = torch.randn(num_nodes, in_features)\n",
    "    n_heads = 4\n",
    "    \n",
    "    degrees = np.sum(adj_matrix.toarray(), axis=1)  # 행 또는 열의 합 계산\n",
    "\n",
    "    Y = degrees  # 각 노드별 인접한 노드의 개수\n",
    "\n",
    "\n",
    "    adj_tensor = adj_tensor.unsqueeze(2) # adj_tensor (num_nodes, num_nodes, n_heads)\n",
    "    adj_tensor = adj_tensor.repeat(1, 1, n_heads) #\n",
    "    \n",
    "    graph.add_weighted_edges_from(edges)\n",
    "    \n",
    "    return graph, x, adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 100\n",
    "output_file = 'random_undirected_graphs.pkl'\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for _ in range(num_graphs):\n",
    "    num_nodes, num_edges, max_weight = np.random.randint(1,20), np.random.randint(1,30), np.random.randint(1,30)\n",
    "    graph, x, adj_tensor = generate_random_weighted_graph(num_nodes, num_edges, max_weight)\n",
    "    graphs.append((x, adj_tensor))\n",
    "\n",
    "\n",
    "# 그래프를 pickle 파일로 저장\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일에서 그래프 데이터 로드\n",
    "with open('random_undirected_graphs.pkl', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 1 - Decode Output:\n",
      "tensor([0.0712, 0.0684, 0.3577, 0.5027], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0790, 0.0771, 0.0794, 0.0772, 0.0750, 0.0783, 0.0745, 0.0769, 0.0751,\n",
      "         0.0772, 0.0791, 0.0756, 0.0754]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 2 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 2 - Decode Output:\n",
      "tensor([0.0728, 0.0582, 0.4820, 0.3870], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0823, 0.0840, 0.0822, 0.0836, 0.0832, 0.0842, 0.0832, 0.0826, 0.0836,\n",
      "         0.0829, 0.0842, 0.0841]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 3 - Output:\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 3 - Decode Output:\n",
      "tensor([0.1678, 0.1747, 0.2993, 0.3582], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1415, 0.1403, 0.1475, 0.1407, 0.1415, 0.1432, 0.1454]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 4 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 4 - Decode Output:\n",
      "tensor([0.3614, 0.0669, 0.4015, 0.1702], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3289, 0.3313, 0.3397]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 5 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 5 - Decode Output:\n",
      "tensor([0.0892, 0.0545, 0.4224, 0.4338], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0854, 0.0803, 0.0825, 0.0866, 0.0836, 0.0808, 0.0872, 0.0821, 0.0819,\n",
      "         0.0837, 0.0811, 0.0847]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 6 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 6 - Decode Output:\n",
      "tensor([0.4035, 0.4423, 0.0889, 0.0653], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1091, 0.1117, 0.1113, 0.1097, 0.1120, 0.1122, 0.1126, 0.1116, 0.1097]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 7 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 7 - Decode Output:\n",
      "tensor([0.0932, 0.0483, 0.3901, 0.4683], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0564, 0.0552, 0.0551, 0.0552, 0.0556, 0.0552, 0.0554, 0.0554, 0.0556,\n",
      "         0.0567, 0.0560, 0.0558, 0.0546, 0.0557, 0.0554, 0.0559, 0.0551, 0.0558]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 8 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 8 - Decode Output:\n",
      "tensor([0.3301, 0.4942, 0.1162, 0.0595], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0772, 0.0773, 0.0764, 0.0769, 0.0770, 0.0764, 0.0756, 0.0772, 0.0782,\n",
      "         0.0769, 0.0769, 0.0766, 0.0774]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 9 - Output:\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 9 - Decode Output:\n",
      "tensor([0.0695, 0.0827, 0.6074, 0.2405], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2503, 0.2453, 0.2557, 0.2487]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 10 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 10 - Decode Output:\n",
      "tensor([0.5493, 0.3082, 0.0503, 0.0922], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0911, 0.0912, 0.0904, 0.0905, 0.0910, 0.0903, 0.0912, 0.0904, 0.0912,\n",
      "         0.0916, 0.0910]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 11 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 11 - Decode Output:\n",
      "tensor([0.0844, 0.0928, 0.3427, 0.4801], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0555, 0.0556, 0.0556, 0.0555, 0.0556, 0.0556, 0.0556, 0.0556, 0.0557,\n",
      "         0.0555, 0.0555, 0.0555, 0.0555, 0.0556, 0.0556, 0.0556, 0.0555, 0.0555]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 12 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 12 - Decode Output:\n",
      "tensor([0.3154, 0.1933, 0.0648, 0.4265], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0776, 0.0765, 0.0773, 0.0773, 0.0769, 0.0766, 0.0765, 0.0765, 0.0770,\n",
      "         0.0766, 0.0771, 0.0772, 0.0768]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 13 - Output:\n",
      "torch.Size([17, 4])\n",
      "torch.Size([17, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 13 - Decode Output:\n",
      "tensor([0.3938, 0.4781, 0.0725, 0.0556], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0590, 0.0596, 0.0573, 0.0590, 0.0586, 0.0590, 0.0589, 0.0576, 0.0577,\n",
      "         0.0603, 0.0595, 0.0593, 0.0592, 0.0584, 0.0587, 0.0586, 0.0593]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 14 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 14 - Decode Output:\n",
      "tensor([0.5673, 0.2856, 0.0728, 0.0743], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0556, 0.0555, 0.0555, 0.0555, 0.0556, 0.0555, 0.0555, 0.0556, 0.0556,\n",
      "         0.0556, 0.0556, 0.0556, 0.0556, 0.0555, 0.0555, 0.0555, 0.0555, 0.0555]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 15 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 15 - Decode Output:\n",
      "tensor([0.1003, 0.4153, 0.4210, 0.0633], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0993, 0.1023, 0.1013, 0.1003, 0.1000, 0.0991, 0.0999, 0.1001, 0.0985,\n",
      "         0.0991]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 16 - Output:\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 16 - Decode Output:\n",
      "tensor([0.4897, 0.2766, 0.0338, 0.1999], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 17 - Output:\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 17 - Decode Output:\n",
      "tensor([0.4437, 0.4139, 0.0605, 0.0818], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1972, 0.1932, 0.1994, 0.2018, 0.2084]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 18 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 18 - Decode Output:\n",
      "tensor([0.1203, 0.0478, 0.2624, 0.5695], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0520, 0.0524, 0.0525, 0.0529, 0.0527, 0.0530, 0.0530, 0.0522, 0.0526,\n",
      "         0.0517, 0.0530, 0.0521, 0.0528, 0.0537, 0.0532, 0.0522, 0.0526, 0.0526,\n",
      "         0.0529]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 19 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 19 - Decode Output:\n",
      "tensor([0.0675, 0.0604, 0.3548, 0.5174], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0556, 0.0555, 0.0555, 0.0555, 0.0556, 0.0556, 0.0556, 0.0555, 0.0555,\n",
      "         0.0555, 0.0555, 0.0556, 0.0554, 0.0556, 0.0557, 0.0556, 0.0555, 0.0556]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 20 - Output:\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 20 - Decode Output:\n",
      "tensor([0.3226, 0.3226, 0.0322, 0.3226], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 21 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 21 - Decode Output:\n",
      "tensor([0.3222, 0.2874, 0.2733, 0.1170], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1007, 0.0997, 0.1001, 0.1003, 0.1001, 0.1000, 0.0987, 0.1001, 0.0994,\n",
      "         0.1009]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 22 - Output:\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 22 - Decode Output:\n",
      "tensor([0.0458, 0.2179, 0.6343, 0.1020], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2507, 0.2496, 0.2497, 0.2500]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 23 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 23 - Decode Output:\n",
      "tensor([0.1320, 0.0684, 0.3051, 0.4945], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0537, 0.0520, 0.0492, 0.0536, 0.0566, 0.0550, 0.0519, 0.0505, 0.0489,\n",
      "         0.0562, 0.0485, 0.0532, 0.0530, 0.0516, 0.0524, 0.0551, 0.0503, 0.0535,\n",
      "         0.0549]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 24 - Output:\n",
      "torch.Size([4, 4])\n",
      "torch.Size([4, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([4, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 4])\n",
      "masked_attn_\n",
      "torch.Size([4, 4])\n",
      "masked_attn_1\n",
      "torch.Size([1, 4])\n",
      "attn_weights.size\n",
      "torch.Size([1, 4])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 24 - Decode Output:\n",
      "tensor([0.3034, 0.3523, 0.0978, 0.2465], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2485, 0.2506, 0.2511, 0.2498]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 25 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 25 - Decode Output:\n",
      "tensor([0.0606, 0.0772, 0.4614, 0.4007], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1118, 0.1111, 0.1128, 0.1091, 0.1121, 0.1119, 0.1092, 0.1092, 0.1128]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 26 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 26 - Decode Output:\n",
      "tensor([0.0840, 0.0663, 0.4999, 0.3498], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0992, 0.0999, 0.0987, 0.1004, 0.0993, 0.1000, 0.1007, 0.1011, 0.1010,\n",
      "         0.0997]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 27 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 27 - Decode Output:\n",
      "tensor([0.0484, 0.4001, 0.4350, 0.1166], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3366, 0.3317, 0.3317]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 28 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 28 - Decode Output:\n",
      "tensor([0.0901, 0.0996, 0.4045, 0.4058], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0530, 0.0523, 0.0488, 0.0532, 0.0536, 0.0499, 0.0476, 0.0523, 0.0525,\n",
      "         0.0533, 0.0537, 0.0520, 0.0567, 0.0533, 0.0539, 0.0564, 0.0540, 0.0518,\n",
      "         0.0517]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 29 - Output:\n",
      "torch.Size([6, 4])\n",
      "torch.Size([6, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([6, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 6])\n",
      "masked_attn_\n",
      "torch.Size([6, 6])\n",
      "masked_attn_1\n",
      "torch.Size([1, 6])\n",
      "attn_weights.size\n",
      "torch.Size([1, 6])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 29 - Decode Output:\n",
      "tensor([0.4707, 0.3576, 0.0722, 0.0995], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1693, 0.1645, 0.1659, 0.1653, 0.1666, 0.1685]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 30 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 30 - Decode Output:\n",
      "tensor([0.2849, 0.3732, 0.2718, 0.0701], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0768, 0.0800, 0.0778, 0.0744, 0.0750, 0.0775, 0.0784, 0.0747, 0.0746,\n",
      "         0.0795, 0.0769, 0.0768, 0.0778]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 31 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 31 - Decode Output:\n",
      "tensor([0.3525, 0.4291, 0.1583, 0.0601], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0841, 0.0836, 0.0810, 0.0837, 0.0832, 0.0844, 0.0826, 0.0841, 0.0829,\n",
      "         0.0836, 0.0826, 0.0842]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 32 - Output:\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 32 - Decode Output:\n",
      "tensor([0.2461, 0.2747, 0.2073, 0.2719], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0661, 0.0673, 0.0660, 0.0666, 0.0666, 0.0671, 0.0667, 0.0667, 0.0665,\n",
      "         0.0666, 0.0665, 0.0670, 0.0668, 0.0669, 0.0667]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 33 - Output:\n",
      "torch.Size([14, 4])\n",
      "torch.Size([14, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 33 - Decode Output:\n",
      "tensor([0.2538, 0.4777, 0.1558, 0.1127], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0684, 0.0727, 0.0741, 0.0728, 0.0720, 0.0691, 0.0721, 0.0710, 0.0705,\n",
      "         0.0725, 0.0731, 0.0690, 0.0713, 0.0715]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 34 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 34 - Decode Output:\n",
      "tensor([0.0796, 0.0510, 0.3777, 0.4917], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0831, 0.0846, 0.0845, 0.0858, 0.0844, 0.0828, 0.0832, 0.0823, 0.0822,\n",
      "         0.0825, 0.0819, 0.0825]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 35 - Output:\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 35 - Decode Output:\n",
      "tensor([0.1145, 0.0707, 0.3926, 0.4222], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1255, 0.1247, 0.1251, 0.1254, 0.1262, 0.1247, 0.1246, 0.1239]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 36 - Output:\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 36 - Decode Output:\n",
      "tensor([0.4931, 0.3798, 0.0586, 0.0685], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0667, 0.0667, 0.0662, 0.0678, 0.0663, 0.0661, 0.0664, 0.0658, 0.0672,\n",
      "         0.0671, 0.0655, 0.0655, 0.0674, 0.0678, 0.0675]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 37 - Output:\n",
      "torch.Size([17, 4])\n",
      "torch.Size([17, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 37 - Decode Output:\n",
      "tensor([0.4425, 0.2569, 0.1717, 0.1290], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0602, 0.0595, 0.0605, 0.0612, 0.0548, 0.0575, 0.0600, 0.0579, 0.0617,\n",
      "         0.0609, 0.0584, 0.0585, 0.0551, 0.0576, 0.0568, 0.0599, 0.0593]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 38 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 38 - Decode Output:\n",
      "tensor([0.0636, 0.1003, 0.3998, 0.4363], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1088, 0.1192, 0.1107, 0.1094, 0.1077, 0.1152, 0.1151, 0.1060, 0.1079]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 39 - Output:\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 39 - Decode Output:\n",
      "tensor([0.0396, 0.1468, 0.1468, 0.6668], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 40 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 40 - Decode Output:\n",
      "tensor([0.4766, 0.3636, 0.0675, 0.0923], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0560, 0.0556, 0.0552, 0.0555, 0.0553, 0.0555, 0.0557, 0.0556, 0.0556,\n",
      "         0.0557, 0.0557, 0.0559, 0.0556, 0.0560, 0.0556, 0.0554, 0.0553, 0.0548]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 41 - Output:\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 41 - Decode Output:\n",
      "tensor([0.1053, 0.2941, 0.3360, 0.2646], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2037, 0.1992, 0.2023, 0.1961, 0.1986]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 42 - Output:\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 42 - Decode Output:\n",
      "tensor([0.1462, 0.3480, 0.2776, 0.2282], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1448, 0.1418, 0.1389, 0.1462, 0.1456, 0.1437, 0.1390]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 43 - Output:\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 43 - Decode Output:\n",
      "tensor([0.0483, 0.0967, 0.4655, 0.3895], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1425, 0.1440, 0.1427, 0.1431, 0.1420, 0.1427, 0.1429]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 44 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 44 - Decode Output:\n",
      "tensor([0.4809, 0.3819, 0.0642, 0.0730], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0840, 0.0825, 0.0828, 0.0843, 0.0822, 0.0818, 0.0828, 0.0822, 0.0830,\n",
      "         0.0848, 0.0845, 0.0851]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 45 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 45 - Decode Output:\n",
      "tensor([0.1317, 0.0565, 0.3220, 0.4898], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0922, 0.0932, 0.0899, 0.0935, 0.0885, 0.0892, 0.0918, 0.0924, 0.0934,\n",
      "         0.0894, 0.0865]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 46 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 46 - Decode Output:\n",
      "tensor([0.1807, 0.1251, 0.3571, 0.3371], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0528, 0.0535, 0.0529, 0.0520, 0.0525, 0.0527, 0.0528, 0.0529, 0.0525,\n",
      "         0.0532, 0.0525, 0.0529, 0.0526, 0.0520, 0.0522, 0.0520, 0.0520, 0.0534,\n",
      "         0.0525]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 47 - Output:\n",
      "torch.Size([8, 4])\n",
      "torch.Size([8, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([8, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 8])\n",
      "masked_attn_\n",
      "torch.Size([8, 8])\n",
      "masked_attn_1\n",
      "torch.Size([1, 8])\n",
      "attn_weights.size\n",
      "torch.Size([1, 8])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 47 - Decode Output:\n",
      "tensor([0.0616, 0.0722, 0.3356, 0.5306], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1245, 0.1277, 0.1269, 0.1266, 0.1226, 0.1240, 0.1266, 0.1212]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 48 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 48 - Decode Output:\n",
      "tensor([0.1569, 0.1196, 0.4111, 0.3124], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0761, 0.0757, 0.0762, 0.0801, 0.0759, 0.0780, 0.0760, 0.0812, 0.0770,\n",
      "         0.0748, 0.0775, 0.0768, 0.0746]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 49 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 49 - Decode Output:\n",
      "tensor([0.4882, 0.2942, 0.1463, 0.0712], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3372, 0.3375, 0.3254]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 50 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 50 - Decode Output:\n",
      "tensor([0.2561, 0.5347, 0.1550, 0.0543], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0782, 0.0770, 0.0766, 0.0768, 0.0748, 0.0776, 0.0794, 0.0797, 0.0731,\n",
      "         0.0766, 0.0757, 0.0770, 0.0773]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 51 - Output:\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 51 - Decode Output:\n",
      "tensor([0.1710, 0.4141, 0.3618, 0.0532], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.4968, 0.5032]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 52 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 52 - Decode Output:\n",
      "tensor([0.0620, 0.0921, 0.3171, 0.5288], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1000, 0.0998, 0.1000, 0.1001, 0.0999, 0.1000, 0.0998, 0.1003, 0.1003,\n",
      "         0.0998]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 53 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 53 - Decode Output:\n",
      "tensor([0.3150, 0.5411, 0.0517, 0.0922], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3284, 0.3312, 0.3404]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 54 - Output:\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 54 - Decode Output:\n",
      "tensor([0.4009, 0.4591, 0.0909, 0.0492], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0626, 0.0627, 0.0625, 0.0623, 0.0623, 0.0624, 0.0624, 0.0626, 0.0628,\n",
      "         0.0623, 0.0624, 0.0625, 0.0623, 0.0623, 0.0627, 0.0629]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 55 - Output:\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 55 - Decode Output:\n",
      "tensor([0.3983, 0.4714, 0.0769, 0.0534], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1999, 0.1996, 0.2002, 0.2004, 0.1999]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 56 - Output:\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 56 - Decode Output:\n",
      "tensor([0.4281, 0.4388, 0.0640, 0.0691], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0624, 0.0624, 0.0625, 0.0625, 0.0624, 0.0624, 0.0626, 0.0626, 0.0624,\n",
      "         0.0627, 0.0626, 0.0622, 0.0627, 0.0627, 0.0623, 0.0626]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 57 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 57 - Decode Output:\n",
      "tensor([0.2277, 0.1416, 0.4443, 0.1864], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3122, 0.3234, 0.3644]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 58 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 58 - Decode Output:\n",
      "tensor([0.0503, 0.1770, 0.4701, 0.3026], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0905, 0.0907, 0.0906, 0.0905, 0.0914, 0.0920, 0.0901, 0.0908, 0.0916,\n",
      "         0.0907, 0.0911]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 59 - Output:\n",
      "torch.Size([6, 4])\n",
      "torch.Size([6, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([6, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 6])\n",
      "masked_attn_\n",
      "torch.Size([6, 6])\n",
      "masked_attn_1\n",
      "torch.Size([1, 6])\n",
      "attn_weights.size\n",
      "torch.Size([1, 6])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 59 - Decode Output:\n",
      "tensor([0.3769, 0.1714, 0.3432, 0.1084], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1669, 0.1664, 0.1670, 0.1667, 0.1666, 0.1664]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 60 - Output:\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 60 - Decode Output:\n",
      "tensor([0.2253, 0.0354, 0.3007, 0.4386], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.5032, 0.4968]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 61 - Output:\n",
      "torch.Size([14, 4])\n",
      "torch.Size([14, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 61 - Decode Output:\n",
      "tensor([0.2574, 0.4596, 0.2375, 0.0456], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0721, 0.0718, 0.0714, 0.0706, 0.0718, 0.0698, 0.0707, 0.0717, 0.0704,\n",
      "         0.0712, 0.0727, 0.0721, 0.0707, 0.0730]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 62 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 62 - Decode Output:\n",
      "tensor([0.0440, 0.1140, 0.3515, 0.4905], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1001, 0.0993, 0.0986, 0.0988, 0.1004, 0.1002, 0.0999, 0.0996, 0.1019,\n",
      "         0.1011]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 63 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 63 - Decode Output:\n",
      "tensor([0.0642, 0.0621, 0.4280, 0.4457], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0563, 0.0554, 0.0563, 0.0554, 0.0551, 0.0552, 0.0555, 0.0557, 0.0561,\n",
      "         0.0555, 0.0544, 0.0562, 0.0549, 0.0553, 0.0551, 0.0554, 0.0562, 0.0558]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 64 - Output:\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 64 - Decode Output:\n",
      "tensor([0.3193, 0.5245, 0.0850, 0.0712], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0659, 0.0663, 0.0673, 0.0668, 0.0674, 0.0666, 0.0665, 0.0665, 0.0669,\n",
      "         0.0663, 0.0667, 0.0668, 0.0665, 0.0669, 0.0667]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 65 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 65 - Decode Output:\n",
      "tensor([0.3428, 0.4976, 0.1098, 0.0498], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0524, 0.0525, 0.0527, 0.0528, 0.0525, 0.0526, 0.0527, 0.0527, 0.0526,\n",
      "         0.0526, 0.0526, 0.0527, 0.0527, 0.0530, 0.0527, 0.0527, 0.0526, 0.0524,\n",
      "         0.0526]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 66 - Output:\n",
      "torch.Size([17, 4])\n",
      "torch.Size([17, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 66 - Decode Output:\n",
      "tensor([0.2981, 0.4624, 0.1721, 0.0674], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0589, 0.0614, 0.0617, 0.0592, 0.0561, 0.0589, 0.0575, 0.0569, 0.0611,\n",
      "         0.0591, 0.0584, 0.0581, 0.0570, 0.0599, 0.0577, 0.0578, 0.0603]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 67 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 67 - Decode Output:\n",
      "tensor([0.0731, 0.0648, 0.4447, 0.4174], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0993, 0.1029, 0.1011, 0.0956, 0.0983, 0.1028, 0.0985, 0.1028, 0.1042,\n",
      "         0.0944]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 68 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 68 - Decode Output:\n",
      "tensor([0.1976, 0.1493, 0.2848, 0.3683], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0833, 0.0830, 0.0831, 0.0831, 0.0832, 0.0836, 0.0831, 0.0834, 0.0834,\n",
      "         0.0834, 0.0839, 0.0835]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 69 - Output:\n",
      "torch.Size([14, 4])\n",
      "torch.Size([14, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 69 - Decode Output:\n",
      "tensor([0.0585, 0.0692, 0.4807, 0.3916], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0738, 0.0713, 0.0706, 0.0700, 0.0711, 0.0712, 0.0709, 0.0718, 0.0717,\n",
      "         0.0689, 0.0709, 0.0726, 0.0723, 0.0729]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 70 - Output:\n",
      "torch.Size([7, 4])\n",
      "torch.Size([7, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([7, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 7])\n",
      "masked_attn_\n",
      "torch.Size([7, 7])\n",
      "masked_attn_1\n",
      "torch.Size([1, 7])\n",
      "attn_weights.size\n",
      "torch.Size([1, 7])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 70 - Decode Output:\n",
      "tensor([0.3602, 0.2910, 0.2486, 0.1002], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1465, 0.1440, 0.1411, 0.1426, 0.1403, 0.1427, 0.1429]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 71 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 71 - Decode Output:\n",
      "tensor([0.0750, 0.0661, 0.2331, 0.6259], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0901, 0.0899, 0.0902, 0.0900, 0.0888, 0.0901, 0.0928, 0.0961, 0.0883,\n",
      "         0.0918, 0.0916]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 72 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 72 - Decode Output:\n",
      "tensor([0.3328, 0.5122, 0.1083, 0.0467], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0525, 0.0517, 0.0531, 0.0506, 0.0529, 0.0521, 0.0509, 0.0535, 0.0520,\n",
      "         0.0512, 0.0569, 0.0572, 0.0527, 0.0553, 0.0494, 0.0513, 0.0524, 0.0514,\n",
      "         0.0530]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 73 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 73 - Decode Output:\n",
      "tensor([0.0794, 0.0598, 0.3468, 0.5140], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1111, 0.1111, 0.1111, 0.1112, 0.1115, 0.1109, 0.1109, 0.1112, 0.1110]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 74 - Output:\n",
      "torch.Size([13, 4])\n",
      "torch.Size([13, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([13, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 13])\n",
      "masked_attn_\n",
      "torch.Size([13, 13])\n",
      "masked_attn_1\n",
      "torch.Size([1, 13])\n",
      "attn_weights.size\n",
      "torch.Size([1, 13])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 74 - Decode Output:\n",
      "tensor([0.5461, 0.2807, 0.0615, 0.1117], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0781, 0.0778, 0.0785, 0.0742, 0.0754, 0.0781, 0.0781, 0.0768, 0.0778,\n",
      "         0.0742, 0.0781, 0.0768, 0.0761]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 75 - Output:\n",
      "torch.Size([2, 4])\n",
      "torch.Size([2, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([2, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 2])\n",
      "masked_attn_\n",
      "torch.Size([2, 2])\n",
      "masked_attn_1\n",
      "torch.Size([1, 2])\n",
      "attn_weights.size\n",
      "torch.Size([1, 2])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 75 - Decode Output:\n",
      "tensor([0.4338, 0.2360, 0.2835, 0.0467], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.4990, 0.5010]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 76 - Output:\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 76 - Decode Output:\n",
      "tensor([0.0725, 0.0605, 0.3963, 0.4707], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0668, 0.0640, 0.0680, 0.0692, 0.0636, 0.0704, 0.0660, 0.0676, 0.0626,\n",
      "         0.0693, 0.0649, 0.0676, 0.0696, 0.0690, 0.0613]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 77 - Output:\n",
      "torch.Size([15, 4])\n",
      "torch.Size([15, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([15, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 15])\n",
      "masked_attn_\n",
      "torch.Size([15, 15])\n",
      "masked_attn_1\n",
      "torch.Size([1, 15])\n",
      "attn_weights.size\n",
      "torch.Size([1, 15])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 77 - Decode Output:\n",
      "tensor([0.0872, 0.0716, 0.2776, 0.5636], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0668, 0.0672, 0.0663, 0.0674, 0.0665, 0.0664, 0.0664, 0.0668, 0.0671,\n",
      "         0.0667, 0.0662, 0.0662, 0.0668, 0.0663, 0.0669]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 78 - Output:\n",
      "torch.Size([10, 4])\n",
      "torch.Size([10, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([10, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 10])\n",
      "masked_attn_\n",
      "torch.Size([10, 10])\n",
      "masked_attn_1\n",
      "torch.Size([1, 10])\n",
      "attn_weights.size\n",
      "torch.Size([1, 10])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 78 - Decode Output:\n",
      "tensor([0.0582, 0.0726, 0.3960, 0.4732], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1024, 0.1018, 0.0989, 0.0997, 0.0978, 0.1003, 0.0979, 0.0996, 0.1016,\n",
      "         0.1000]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 79 - Output:\n",
      "torch.Size([14, 4])\n",
      "torch.Size([14, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([14, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 14])\n",
      "masked_attn_\n",
      "torch.Size([14, 14])\n",
      "masked_attn_1\n",
      "torch.Size([1, 14])\n",
      "attn_weights.size\n",
      "torch.Size([1, 14])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 79 - Decode Output:\n",
      "tensor([0.2033, 0.3142, 0.3391, 0.1434], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0714, 0.0708, 0.0713, 0.0708, 0.0711, 0.0719, 0.0718, 0.0713, 0.0719,\n",
      "         0.0718, 0.0707, 0.0721, 0.0716, 0.0716]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 80 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 80 - Decode Output:\n",
      "tensor([0.4019, 0.2650, 0.1987, 0.1343], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0897, 0.0919, 0.0909, 0.0908, 0.0903, 0.0918, 0.0910, 0.0918, 0.0913,\n",
      "         0.0914, 0.0890]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 81 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 81 - Decode Output:\n",
      "tensor([0.0740, 0.0679, 0.3750, 0.4831], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0525, 0.0518, 0.0535, 0.0526, 0.0521, 0.0525, 0.0524, 0.0537, 0.0529,\n",
      "         0.0532, 0.0522, 0.0531, 0.0528, 0.0526, 0.0524, 0.0523, 0.0536, 0.0519,\n",
      "         0.0518]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 82 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 82 - Decode Output:\n",
      "tensor([0.0658, 0.0781, 0.4077, 0.4485], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1112, 0.1112, 0.1105, 0.1112, 0.1110, 0.1115, 0.1109, 0.1112, 0.1113]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 83 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 83 - Decode Output:\n",
      "tensor([0.3908, 0.2608, 0.1268, 0.2216], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1118, 0.1101, 0.1125, 0.1094, 0.1105, 0.1126, 0.1106, 0.1111, 0.1115]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 84 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 84 - Decode Output:\n",
      "tensor([0.2345, 0.0458, 0.3032, 0.4164], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111, 0.1111]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 85 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 85 - Decode Output:\n",
      "tensor([0.4436, 0.4246, 0.0726, 0.0592], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0914, 0.0902, 0.0910, 0.0915, 0.0899, 0.0913, 0.0901, 0.0924, 0.0898,\n",
      "         0.0909, 0.0917]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 86 - Output:\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 86 - Decode Output:\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 87 - Output:\n",
      "torch.Size([18, 4])\n",
      "torch.Size([18, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([18, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 18])\n",
      "masked_attn_\n",
      "torch.Size([18, 18])\n",
      "masked_attn_1\n",
      "torch.Size([1, 18])\n",
      "attn_weights.size\n",
      "torch.Size([1, 18])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 87 - Decode Output:\n",
      "tensor([0.4529, 0.4170, 0.0594, 0.0707], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0545, 0.0545, 0.0552, 0.0553, 0.0554, 0.0555, 0.0564, 0.0565, 0.0560,\n",
      "         0.0557, 0.0552, 0.0561, 0.0555, 0.0559, 0.0548, 0.0562, 0.0564, 0.0550]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 88 - Output:\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 88 - Decode Output:\n",
      "tensor([0.0508, 0.1029, 0.4802, 0.3661], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0635, 0.0622, 0.0630, 0.0630, 0.0604, 0.0601, 0.0646, 0.0630, 0.0628,\n",
      "         0.0623, 0.0621, 0.0622, 0.0619, 0.0636, 0.0617, 0.0635]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 89 - Output:\n",
      "torch.Size([16, 4])\n",
      "torch.Size([16, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([16, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 16])\n",
      "masked_attn_\n",
      "torch.Size([16, 16])\n",
      "masked_attn_1\n",
      "torch.Size([1, 16])\n",
      "attn_weights.size\n",
      "torch.Size([1, 16])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 89 - Decode Output:\n",
      "tensor([0.4601, 0.4043, 0.0602, 0.0754], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625,\n",
      "         0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 90 - Output:\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 90 - Decode Output:\n",
      "tensor([0.2782, 0.5885, 0.0689, 0.0644], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2004, 0.2000, 0.2002, 0.2002, 0.1993]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 91 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 91 - Decode Output:\n",
      "tensor([0.0463, 0.1608, 0.3295, 0.4634], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3308, 0.3330, 0.3362]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 92 - Output:\n",
      "torch.Size([5, 4])\n",
      "torch.Size([5, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([5, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 5])\n",
      "masked_attn_\n",
      "torch.Size([5, 5])\n",
      "masked_attn_1\n",
      "torch.Size([1, 5])\n",
      "attn_weights.size\n",
      "torch.Size([1, 5])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 92 - Decode Output:\n",
      "tensor([0.0650, 0.1190, 0.3162, 0.4997], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.2055, 0.2000, 0.2051, 0.1947, 0.1946]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Graph 93 - Output:\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([3, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 3])\n",
      "masked_attn_\n",
      "torch.Size([3, 3])\n",
      "masked_attn_1\n",
      "torch.Size([1, 3])\n",
      "attn_weights.size\n",
      "torch.Size([1, 3])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 93 - Decode Output:\n",
      "tensor([0.4194, 0.4251, 0.0975, 0.0580], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.3333, 0.3333, 0.3334]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 94 - Output:\n",
      "torch.Size([17, 4])\n",
      "torch.Size([17, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([17, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 17])\n",
      "masked_attn_\n",
      "torch.Size([17, 17])\n",
      "masked_attn_1\n",
      "torch.Size([1, 17])\n",
      "attn_weights.size\n",
      "torch.Size([1, 17])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 94 - Decode Output:\n",
      "tensor([0.3200, 0.5409, 0.0763, 0.0627], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0587, 0.0589, 0.0587, 0.0587, 0.0583, 0.0591, 0.0593, 0.0589, 0.0592,\n",
      "         0.0590, 0.0582, 0.0590, 0.0593, 0.0582, 0.0591, 0.0587, 0.0585]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 95 - Output:\n",
      "torch.Size([11, 4])\n",
      "torch.Size([11, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([11, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 11])\n",
      "masked_attn_\n",
      "torch.Size([11, 11])\n",
      "masked_attn_1\n",
      "torch.Size([1, 11])\n",
      "attn_weights.size\n",
      "torch.Size([1, 11])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 95 - Decode Output:\n",
      "tensor([0.0686, 0.0844, 0.4653, 0.3817], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0879, 0.0887, 0.0837, 0.0930, 0.0852, 0.0974, 0.0905, 0.0932, 0.0964,\n",
      "         0.0963, 0.0877]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 96 - Output:\n",
      "torch.Size([1, 4])\n",
      "torch.Size([1, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([1, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 1])\n",
      "masked_attn_\n",
      "torch.Size([1, 1])\n",
      "masked_attn_1\n",
      "torch.Size([1, 1])\n",
      "attn_weights.size\n",
      "torch.Size([1, 1])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 96 - Decode Output:\n",
      "tensor([0.2833, 0.4011, 0.2833, 0.0323], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[1.]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 97 - Output:\n",
      "torch.Size([19, 4])\n",
      "torch.Size([19, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([19, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 19])\n",
      "masked_attn_\n",
      "torch.Size([19, 19])\n",
      "masked_attn_1\n",
      "torch.Size([1, 19])\n",
      "attn_weights.size\n",
      "torch.Size([1, 19])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 97 - Decode Output:\n",
      "tensor([0.4581, 0.3315, 0.0988, 0.1116], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0509, 0.0522, 0.0538, 0.0521, 0.0527, 0.0536, 0.0512, 0.0542, 0.0528,\n",
      "         0.0536, 0.0522, 0.0510, 0.0531, 0.0532, 0.0534, 0.0537, 0.0532, 0.0518,\n",
      "         0.0514]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 98 - Output:\n",
      "torch.Size([12, 4])\n",
      "torch.Size([12, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([12, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 12])\n",
      "masked_attn_\n",
      "torch.Size([12, 12])\n",
      "masked_attn_1\n",
      "torch.Size([1, 12])\n",
      "attn_weights.size\n",
      "torch.Size([1, 12])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 98 - Decode Output:\n",
      "tensor([0.1014, 0.0551, 0.2637, 0.5798], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.0834, 0.0831, 0.0832, 0.0827, 0.0833, 0.0825, 0.0826, 0.0838, 0.0830,\n",
      "         0.0824, 0.0853, 0.0848]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 99 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 99 - Decode Output:\n",
      "tensor([0.0655, 0.0653, 0.5266, 0.3425], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1105, 0.1121, 0.1104, 0.1115, 0.1120, 0.1089, 0.1124, 0.1124, 0.1098]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "Graph 100 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "phi_v_i.size()\n",
      "torch.Size([1, 16])\n",
      "phi_v_j.size()\n",
      "torch.Size([9, 16])\n",
      "attn_output.size\n",
      "torch.Size([1, 9])\n",
      "masked_attn_\n",
      "torch.Size([9, 9])\n",
      "masked_attn_1\n",
      "torch.Size([1, 9])\n",
      "attn_weights.size\n",
      "torch.Size([1, 9])\n",
      "output_size\n",
      "torch.Size([1, 4])\n",
      "output_size1\n",
      "torch.Size([4])\n",
      "Graph 100 - Decode Output:\n",
      "tensor([0.0924, 0.1013, 0.3813, 0.4251], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "Attention Weights:\n",
      "tensor([[0.1099, 0.1110, 0.1114, 0.1101, 0.1087, 0.1115, 0.1116, 0.1139, 0.1120]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "in_features = x.shape[1]\n",
    "n_heads = adj_tensor.shape[2]\n",
    "hidden_features = 4 * n_heads\n",
    "out_features = n_heads\n",
    "d_h = 4 * n_heads\n",
    "gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "gat_models = []\n",
    "for graph_idx, (x, adj_tensor) in enumerate(graphs):\n",
    "    gat_models.append(gat_model)\n",
    "    x = x.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(x, adj_tensor)\n",
    "    print(f\"Graph {graph_idx+1} - Output:\")\n",
    "    print(output.shape)\n",
    "    print(x.shape)\n",
    "\n",
    "    # Generate v_prev tensor\n",
    "    v_i = torch.randn(1).cuda()\n",
    "    # Generate neighbors tensor\n",
    "    v_j = torch.randn(x.shape[0],1).cuda()\n",
    "\n",
    "\n",
    "    x, decode_output, attn_weights = gat_model.decode(output, v_i, v_j)\n",
    "    print(f\"Graph {graph_idx+1} - Decode Output:\")\n",
    "    print(decode_output)\n",
    "    print(\"Attention Weights:\")\n",
    "    print(attn_weights)\n",
    "    # print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "https://chioni.github.io/posts/gat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 데이터로 모델 학습\n",
    "for graph_idx, (graph, x, adj_tensor) in enumerate(graphs):\n",
    "    # Initialize the GAT model for the current graph\n",
    "    in_features = x.shape[1]\n",
    "    n_heads = adj_tensor.shape[1]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = 2 * n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).to(device)\n",
    "\n",
    "    # Set the optimizer and loss function\n",
    "    optimizer = optim.Adam(gat_model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.NLLLoss().to(device)\n",
    "\n",
    "    # Move the feature matrix and adjacency tensor to the GPU\n",
    "    x = x.to(device)\n",
    "    adj_tensor = adj_tensor.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = gat_model(x, adj_tensor)\n",
    "\n",
    "        # Generate random labels for the current graph\n",
    "        num_nodes = x.shape[0]\n",
    "        labels = torch.tensor([random.randint(0, 1) for _ in range(num_nodes)]).to(device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Graph {}: Epoch: {:03d}, Loss: {:.4f}\".format(graph_idx+1, epoch+1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
