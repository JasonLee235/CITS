{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수정할 사항\n",
    "\n",
    "- 다음주 월요일 시험 후에 다시 한 번 검토해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from scipy import sparse as sp\n",
    "import random\n",
    "from graphviz import Graph\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0201],\n",
      "        [0.7718],\n",
      "        [1.5436]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(3,1)\n",
    "print(a)\n",
    "b = a.squeeze(1)\n",
    "c = torch.randn(8,1)\n",
    "output = c * b\n",
    "output.size()\n",
    "result = torch.matmul(c.tanspose(0,1),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1,3)\n",
    "a = a.transpose(0,1)\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_features = hidden_features\n",
    "        self.d_h = d_h\n",
    "\n",
    "        self.phi = torch.nn.Linear(hidden_features, d_h * hidden_features)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.C = torch.nn.Parameter(torch.randn(1)) # constant C\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x, v_i, v_j):\n",
    "        phi1 = torch.randn(d_h,1).cuda() #linear weight\n",
    "        phi2 = torch.randn(d_h,1).cuda()\n",
    "\n",
    "        phi1_v_i = phi1 * v_i # phi1_v_prev 의 사이즈 (d* d_h)\n",
    "        v_j = v_j.squeeze()\n",
    "        phi2_v_j = phi2 * v_j # phi2_neighbors 의 사이즈 (n,d* d_h)\n",
    "        phi2_v_j = phi2_v_j\n",
    "\n",
    "        attn_input = torch.matmul(phi1_v_i.transpose(0,1), phi2_v_j) / (self.d_h ** 0.5) # (1,n) 의 크기를 갖는 attn_input\n",
    "        # attn_input = attn_input.squeeze(0)  # Remove the extra dimension\n",
    "        attn_input = attn_input.squeeze()\n",
    "\n",
    "        attn_output = self.C * self.activation(attn_input)\n",
    "        print(attn_output.size())\n",
    "        masked_attn_output = attn_output.masked_fill(v_j == 0, float('-inf'))\n",
    "        attn_weights = self.softmax(masked_attn_output)\n",
    "        print(attn_weights.size())\n",
    "        # attn_weights = self.softmax(attn_output)        \n",
    "\n",
    "        output = attn_weights.squeeze() * x\n",
    "        output = output.squeeze(0)\n",
    "\n",
    "        return x, output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class GraphAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, n_heads, is_concat = True, dropout = 0.6, leacky_relu_negative_slope = 0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = torch.nn.Parameter(torch.randn(in_features, out_features))\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias = False)\n",
    "\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias = False)\n",
    "        self.activation = nn.LeakyReLU(negative_slope = leacky_relu_negative_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        n_nodes = x.shape[0]\n",
    "        g=self.linear(x).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1,1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim = -1)\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        e = e.squeeze(-1)\n",
    "        assert adj.shape[0] == 1 or adj.shape[0] == n_nodes\n",
    "        assert adj.shape[1] == 1 or adj.shape[1] == n_nodes\n",
    "        assert adj.shape[2] == 1 or adj.shape[2] == self.n_heads\n",
    "        e=e.masked_fill(adj == 0, 1)\n",
    "        a = self.softmax(e)\n",
    "        a = self.dropout(a)\n",
    "        attn_res = torch.einsum('ijh,jhf->ihf', a, g)\n",
    "        if self.is_concat:\n",
    "            return attn_res.reshape(n_nodes, self.n_heads * self.n_hidden)\n",
    "        else:\n",
    "            return attn_res.mean(dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_features, hidden_features, out_features, n_heads, d_h):\n",
    "        super(GAT, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.attention1 = GraphAttentionLayer(in_features, hidden_features, n_heads)\n",
    "        self.attention2 = GraphAttentionLayer(hidden_features, out_features, n_heads)\n",
    "        self.norm= nn.LayerNorm(out_features)\n",
    "        self.decoder = Decoder(out_features, hidden_features, out_features, n_heads, d_h)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.attention1(x, adj)\n",
    "        x = self.attention2(x, adj)\n",
    "        x = self.norm(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, v_i, v_j):\n",
    "        return self.decoder(x, v_i, v_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_weighted_graph(num_nodes, num_edges, max_weight=10):\n",
    "    # 방향 그래프 생성\n",
    "    graph = nx.Graph()\n",
    "    \n",
    "    # 노드 추가\n",
    "    nodes = range(num_nodes)\n",
    "    graph.add_nodes_from(nodes)\n",
    "    \n",
    "    # 간선 추가\n",
    "    edges = []\n",
    "    for i in range(num_edges):\n",
    "        # 임의의 출발 노드와 도착 노드 선택\n",
    "        source = random.choice(nodes)\n",
    "        target = random.choice(nodes)\n",
    "        \n",
    "        # 출발 노드와 도착 노드가 같은 경우 건너뜀\n",
    "        if source == target:\n",
    "            continue\n",
    "        \n",
    "        # 가중치 랜덤 생성\n",
    "        weight = random.randint(1, max_weight)\n",
    "        \n",
    "        # 간선 추가\n",
    "        edges.append((source, target, weight))\n",
    "\n",
    "    adj_matrix = nx.adjacency_matrix(graph)\n",
    "    adj_matrix = adj_matrix + sp.eye(adj_matrix.shape[0]) # Add self-loop\n",
    "    adj_tensor = torch.Tensor(adj_matrix.todense())\n",
    "    \n",
    "    in_features =  1\n",
    "    x = torch.randn(num_nodes, in_features)\n",
    "    n_heads = 4\n",
    "\n",
    "    adj_tensor = adj_tensor.unsqueeze(2) # adj_tensor (num_nodes, num_nodes, n_heads)\n",
    "    adj_tensor = adj_tensor.repeat(1, 1, n_heads) #\n",
    "    \n",
    "    graph.add_weighted_edges_from(edges)\n",
    "    \n",
    "    return graph, x, adj_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_graphs = 100\n",
    "output_file = 'random_undirected_graphs.pkl'\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for _ in range(num_graphs):\n",
    "    num_nodes, num_edges, max_weight = np.random.randint(1,20), np.random.randint(1,30), np.random.randint(1,30)\n",
    "    graph, x, adj_tensor = generate_random_weighted_graph(num_nodes, num_edges, max_weight)\n",
    "    graphs.append((x, adj_tensor))\n",
    "\n",
    "\n",
    "# 그래프를 pickle 파일로 저장\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(graphs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일에서 그래프 데이터 로드\n",
    "with open('random_undirected_graphs.pkl', 'rb') as f:\n",
    "    graphs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph 1 - Output:\n",
      "torch.Size([9, 4])\n",
      "torch.Size([9, 1])\n",
      "torch.Size([9])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[110], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Generate neighbors tensor\u001b[39;00m\n\u001b[0;32m     21\u001b[0m v_j \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m---> 24\u001b[0m x, decode_output, attn_weights \u001b[39m=\u001b[39m gat_model\u001b[39m.\u001b[39;49mdecode(output, v_i, v_j)\n\u001b[0;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGraph \u001b[39m\u001b[39m{\u001b[39;00mgraph_idx\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m - Decode Output:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(decode_output)\n",
      "Cell \u001b[1;32mIn[106], line 18\u001b[0m, in \u001b[0;36mGAT.decode\u001b[1;34m(self, x, v_i, v_j)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, x, v_i, v_j):\n\u001b[1;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(x, v_i, v_j)\n",
      "File \u001b[1;32mc:\\Users\\j\\anaconda3\\envs\\39py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[104], line 30\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, v_i, v_j)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mprint\u001b[39m(attn_output\u001b[39m.\u001b[39msize())\n\u001b[0;32m     29\u001b[0m masked_attn_output \u001b[39m=\u001b[39m attn_output\u001b[39m.\u001b[39mmasked_fill(v_j \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 30\u001b[0m attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(masked_attn_output)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(attn_weights\u001b[39m.\u001b[39msize())\n\u001b[0;32m     32\u001b[0m \u001b[39m# attn_weights = self.softmax(attn_output)        \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\j\\anaconda3\\envs\\39py\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\j\\anaconda3\\envs\\39py\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1376\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1376\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49msoftmax(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim, _stacklevel\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\j\\anaconda3\\envs\\39py\\lib\\site-packages\\torch\\nn\\functional.py:1834\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1832\u001b[0m     dim \u001b[39m=\u001b[39m _get_softmax_dim(\u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1833\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1834\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49msoftmax(dim)\n\u001b[0;32m   1835\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1836\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msoftmax(dim, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "in_features = x.shape[1]\n",
    "n_heads = adj_tensor.shape[2]\n",
    "hidden_features = 4 * n_heads\n",
    "out_features = n_heads\n",
    "d_h = 4 * n_heads\n",
    "gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).cuda()\n",
    "gat_models = []\n",
    "for graph_idx, (x, adj_tensor) in enumerate(graphs):    \n",
    "    gat_models.append(gat_model)\n",
    "    x = x.cuda()\n",
    "    adj_tensor = adj_tensor.cuda()\n",
    "    output = gat_model(x, adj_tensor)\n",
    "    print(f\"Graph {graph_idx+1} - Output:\")\n",
    "    print(output.shape)\n",
    "    print(x.shape)\n",
    "    #output : 각 노드에 대한 클래스 라벨 예측 값\n",
    "\n",
    "    # Generate v_prev tensor\n",
    "    v_i = torch.randn(1).cuda()\n",
    "    # Generate neighbors tensor\n",
    "    v_j = torch.randn(x.shape[0],1).cuda()\n",
    "\n",
    "\n",
    "    x, decode_output, attn_weights = gat_model.decode(output, v_i, v_j)\n",
    "    print(f\"Graph {graph_idx+1} - Decode Output:\")\n",
    "    print(decode_output)\n",
    "    print(\"Attention Weights:\")\n",
    "    print(attn_weights)\n",
    "    # print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고\n",
    "https://chioni.github.io/posts/gat/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그래프 데이터로 모델 학습\n",
    "for graph_idx, (graph, x, adj_tensor) in enumerate(graphs):\n",
    "    # Initialize the GAT model for the current graph\n",
    "    in_features = x.shape[1]\n",
    "    n_heads = adj_tensor.shape[1]\n",
    "    hidden_features = 4 * n_heads\n",
    "    out_features = 2 * n_heads\n",
    "    d_h = 4 * n_heads\n",
    "    gat_model = GAT(in_features, hidden_features, out_features, n_heads, d_h).to(device)\n",
    "\n",
    "    # Set the optimizer and loss function\n",
    "    optimizer = optim.Adam(gat_model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.NLLLoss().to(device)\n",
    "\n",
    "    # Move the feature matrix and adjacency tensor to the GPU\n",
    "    x = x.to(device)\n",
    "    adj_tensor = adj_tensor.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = gat_model(x, adj_tensor)\n",
    "\n",
    "        # Generate random labels for the current graph\n",
    "        num_nodes = x.shape[0]\n",
    "        labels = torch.tensor([random.randint(0, 1) for _ in range(num_nodes)]).to(device)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(\"Graph {}: Epoch: {:03d}, Loss: {:.4f}\".format(graph_idx+1, epoch+1, loss.item()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
